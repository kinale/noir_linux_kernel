
From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 0AA4CC28B2C
	for <linux-kernel@archiver.kernel.org>; Mon, 15 Aug 2022 07:14:21 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S231816AbiHOHOT (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 15 Aug 2022 03:14:19 -0400
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:45348 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S231422AbiHOHOQ (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 15 Aug 2022 03:14:16 -0400
Received: from mail-yb1-xb4a.google.com (mail-yb1-xb4a.google.com [IPv6:2607:f8b0:4864:20::b4a])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id 41D6F167E3
        for <linux-kernel@vger.kernel.org>; Mon, 15 Aug 2022 00:14:15 -0700 (PDT)
Received: by mail-yb1-xb4a.google.com with SMTP id bu13-20020a056902090d00b00671743601f1so5402644ybb.0
        for <linux-kernel@vger.kernel.org>; Mon, 15 Aug 2022 00:14:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20210112;
        h=cc:to:from:subject:mime-version:message-id:date:from:to:cc;
        bh=z4p9UT4lxLVkyWfKqF3x+tnpC+mV1b5bxrrrI+NPdWs=;
        b=piDg2UyaMVdWsfVtY25uudKOrYW03PyH9HrdlHivzZFPMlnxqC1T4rDovBtNchQzPK
         /dCTIu+n6enx4rqddgWYUnUNWhqY9rzNmCR37vdMExqEglmq8sVlfKTbsUARorDQx9jq
         MSwfAAYO4z97B2S3Ns0C3hRwt5slpIYHdK9FqsSEwVGRTg08rIlU1F9i0EuEfDh778qc
         Tx1rNs2phJ6r+8M00dESf7IYZoeIEnoeiuV3QEZijq5i+Bb4XTMIXx++Fr84jPi83NlN
         RUFAf1xtTF5QiM/X2fZUjX+9xCa3CcVqPihEZf3XE9YavkyrE+ARMnXFvS4HMky9+Tn5
         gGmA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=cc:to:from:subject:mime-version:message-id:date:x-gm-message-state
         :from:to:cc;
        bh=z4p9UT4lxLVkyWfKqF3x+tnpC+mV1b5bxrrrI+NPdWs=;
        b=4Xl7OUL6Vpyj4xl1SSAW8FHh+exYaO3LtnDR8FPBzOvJF/wtsyttg1LGslosGOML2Y
         UYSQKOe94NIl2WnoRjqiZIxhvZRee2ztr4IvqhO5uB4rOLek9JdUwUxHErIlB/4YfHuR
         5mhi7o5Nz8AtPAhASb7T82YNV6xAibTmW6z2gYirharFd4ELV4Yvc1RDDJhv0+gjuzep
         s1R7pqavBbQFYx5xRIHQtn4mBFNtkWaif47UnwAsJpvNwIr7MRwyJ4LY2lutsgP1iz0a
         7lLFWUZdpu9htdKD3btEOs7e6e7SlZbVyvoZUpTnV1bhBJHdYexJDYUsEULljDXygyKo
         kl3A==
X-Gm-Message-State: ACgBeo1K9GOnH4k4pQsYAg3OghVkdlXTy+9Yn0tlMZr8u29BlW2341zE
        MWiIo9ns3E7ZFSZuzYDYrA/JmgAuZEs=
X-Google-Smtp-Source: AA6agR7jqVqjTZkigPGCSY0GIoM7sD+LPLSni6yKoPIw0wwgzVaPuQIan99ay0ZJTc2bF1K9/gM+GdBzKjc=
X-Received: from yuzhao.bld.corp.google.com ([2620:15c:183:200:d91:5887:ac93:ddf0])
 (user=yuzhao job=sendgmr) by 2002:a0d:c184:0:b0:324:d917:78ac with SMTP id
 c126-20020a0dc184000000b00324d91778acmr11907364ywd.468.1660547654045; Mon, 15
 Aug 2022 00:14:14 -0700 (PDT)
Date:   Mon, 15 Aug 2022 01:13:19 -0600
Message-Id: <20220815071332.627393-1-yuzhao@google.com>
Mime-Version: 1.0
X-Mailer: git-send-email 2.37.1.595.g718a3a8f04-goog
Subject: [PATCH v14 00/14] Multi-Gen LRU Framework
From:   Yu Zhao <yuzhao@google.com>
To:     Andrew Morton <akpm@linux-foundation.org>
Cc:     Andi Kleen <ak@linux.intel.com>,
        Aneesh Kumar <aneesh.kumar@linux.ibm.com>,
        Catalin Marinas <catalin.marinas@arm.com>,
        Dave Hansen <dave.hansen@linux.intel.com>,
        Hillf Danton <hdanton@sina.com>, Jens Axboe <axboe@kernel.dk>,
        Johannes Weiner <hannes@cmpxchg.org>,
        Jonathan Corbet <corbet@lwn.net>,
        Linus Torvalds <torvalds@linux-foundation.org>,
        Matthew Wilcox <willy@infradead.org>,
        Mel Gorman <mgorman@suse.de>,
        Michael Larabel <Michael@michaellarabel.com>,
        Michal Hocko <mhocko@kernel.org>,
        Mike Rapoport <rppt@kernel.org>,
        Peter Zijlstra <peterz@infradead.org>,
        Tejun Heo <tj@kernel.org>, Vlastimil Babka <vbabka@suse.cz>,
        Will Deacon <will@kernel.org>,
        linux-arm-kernel@lists.infradead.org, linux-doc@vger.kernel.org,
        linux-kernel@vger.kernel.org, linux-mm@kvack.org, x86@kernel.org,
        page-reclaim@google.com, Yu Zhao <yuzhao@google.com>
Content-Type: text/plain; charset="UTF-8"
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

What's new
==========
Retested on v6.0-rc1; rebased to the latest mm-unstable.

TLDR
====
The current page reclaim is too expensive in terms of CPU usage and it
often makes poor choices about what to evict. This patchset offers an
alternative solution that is performant, versatile and
straightforward.

Patchset overview
=================
The design and implementation overview is in patch 14:
https://lore.kernel.org/r/20220815071332.627393-15-yuzhao@google.com/

01. mm: x86, arm64: add arch_has_hw_pte_young()
02. mm: x86: add CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG
Take advantage of hardware features when trying to clear the accessed
bit in many PTEs.

03. mm/vmscan.c: refactor shrink_node()
04. Revert "include/linux/mm_inline.h: fold __update_lru_size() into
    its sole caller"
Minor refactors to improve readability for the following patches.

05. mm: multi-gen LRU: groundwork
Adds the basic data structure and the functions that insert pages to
and remove pages from the multi-gen LRU (MGLRU) lists.

06. mm: multi-gen LRU: minimal implementation
A minimal implementation without optimizations.

07. mm: multi-gen LRU: exploit locality in rmap
Exploits spatial locality to improve efficiency when using the rmap.

08. mm: multi-gen LRU: support page table walks
Further exploits spatial locality by optionally scanning page tables.

09. mm: multi-gen LRU: optimize multiple memcgs
Optimizes the overall performance for multiple memcgs running mixed
types of workloads.

10. mm: multi-gen LRU: kill switch
Adds a kill switch to enable or disable MGLRU at runtime.

11. mm: multi-gen LRU: thrashing prevention
12. mm: multi-gen LRU: debugfs interface
Provide userspace with features like thrashing prevention, working set
estimation and proactive reclaim.

13. mm: multi-gen LRU: admin guide
14. mm: multi-gen LRU: design doc
Add an admin guide and a design doc.

Benchmark results
=================
Independent lab results
-----------------------
Based on the popularity of searches [01] and the memory usage in
Google's public cloud, the most popular open-source memory-hungry
applications, in alphabetical order, are:
      Apache Cassandra      Memcached
      Apache Hadoop         MongoDB
      Apache Spark          PostgreSQL
      MariaDB (MySQL)       Redis

An independent lab evaluated MGLRU with the most widely used benchmark
suites for the above applications. They posted 960 data points along
with kernel metrics and perf profiles collected over more than 500
hours of total benchmark time. Their final reports show that, with 95%
confidence intervals (CIs), the above applications all performed
significantly better for at least part of their benchmark matrices.

On 5.14:
1. Apache Spark [02] took 95% CIs [9.28, 11.19]% and [12.20, 14.93]%
   less wall time to sort three billion random integers, respectively,
   under the medium- and the high-concurrency conditions, when
   overcommitting memory. There were no statistically significant
   changes in wall time for the rest of the benchmark matrix.
2. MariaDB [03] achieved 95% CIs [5.24, 10.71]% and [20.22, 25.97]%
   more transactions per minute (TPM), respectively, under the medium-
   and the high-concurrency conditions, when overcommitting memory.
   There were no statistically significant changes in TPM for the rest
   of the benchmark matrix.
3. Memcached [04] achieved 95% CIs [23.54, 32.25]%, [20.76, 41.61]%
   and [21.59, 30.02]% more operations per second (OPS), respectively,
   for sequential access, random access and Gaussian (distribution)
   access, when THP=always; 95% CIs [13.85, 15.97]% and
   [23.94, 29.92]% more OPS, respectively, for random access and
   Gaussian access, when THP=never. There were no statistically
   significant changes in OPS for the rest of the benchmark matrix.
4. MongoDB [05] achieved 95% CIs [2.23, 3.44]%, [6.97, 9.73]% and
   [2.16, 3.55]% more operations per second (OPS), respectively, for
   exponential (distribution) access, random access and Zipfian
   (distribution) access, when underutilizing memory; 95% CIs
   [8.83, 10.03]%, [21.12, 23.14]% and [5.53, 6.46]% more OPS,
   respectively, for exponential access, random access and Zipfian
   access, when overcommitting memory.

On 5.15:
5. Apache Cassandra [06] achieved 95% CIs [1.06, 4.10]%, [1.94, 5.43]%
   and [4.11, 7.50]% more operations per second (OPS), respectively,
   for exponential (distribution) access, random access and Zipfian
   (distribution) access, when swap was off; 95% CIs [0.50, 2.60]%,
   [6.51, 8.77]% and [3.29, 6.75]% more OPS, respectively, for
   exponential access, random access and Zipfian access, when swap was
   on.
6. Apache Hadoop [07] took 95% CIs [5.31, 9.69]% and [2.02, 7.86]%
   less average wall time to finish twelve parallel TeraSort jobs,
   respectively, under the medium- and the high-concurrency
   conditions, when swap was on. There were no statistically
   significant changes in average wall time for the rest of the
   benchmark matrix.
7. PostgreSQL [08] achieved 95% CI [1.75, 6.42]% more transactions per
   minute (TPM) under the high-concurrency condition, when swap was
   off; 95% CIs [12.82, 18.69]% and [22.70, 46.86]% more TPM,
   respectively, under the medium- and the high-concurrency
   conditions, when swap was on. There were no statistically
   significant changes in TPM for the rest of the benchmark matrix.
8. Redis [09] achieved 95% CIs [0.58, 5.94]%, [6.55, 14.58]% and
   [11.47, 19.36]% more total operations per second (OPS),
   respectively, for sequential access, random access and Gaussian
   (distribution) access, when THP=always; 95% CIs [1.27, 3.54]%,
   [10.11, 14.81]% and [8.75, 13.64]% more total OPS, respectively,
   for sequential access, random access and Gaussian access, when
   THP=never.

Our lab results
---------------
To supplement the above results, we ran the following benchmark suites
on 5.16-rc7 and found no regressions [10].
      fs_fio_bench_hdd_mq      pft
      fs_lmbench               pgsql-hammerdb
      fs_parallelio            redis
      fs_postmark              stream
      hackbench                sysbenchthread
      kernbench                tpcc_spark
      memcached                unixbench
      multichase               vm-scalability
      mutilate                 will-it-scale
      nginx

[01] https://trends.google.com
[02] https://lore.kernel.org/r/20211102002002.92051-1-bot@edi.works/
[03] https://lore.kernel.org/r/20211009054315.47073-1-bot@edi.works/
[04] https://lore.kernel.org/r/20211021194103.65648-1-bot@edi.works/
[05] https://lore.kernel.org/r/20211109021346.50266-1-bot@edi.works/
[06] https://lore.kernel.org/r/20211202062806.80365-1-bot@edi.works/
[07] https://lore.kernel.org/r/20211209072416.33606-1-bot@edi.works/
[08] https://lore.kernel.org/r/20211218071041.24077-1-bot@edi.works/
[09] https://lore.kernel.org/r/20211122053248.57311-1-bot@edi.works/
[10] https://lore.kernel.org/r/20220104202247.2903702-1-yuzhao@google.com/

Read-world applications
=======================
Third-party testimonials
------------------------
Konstantin reported [11]:
   I have Archlinux with 8G RAM + zswap + swap. While developing, I
   have lots of apps opened such as multiple LSP-servers for different
   langs, chats, two browsers, etc... Usually, my system gets quickly
   to a point of SWAP-storms, where I have to kill LSP-servers,
   restart browsers to free memory, etc, otherwise the system lags
   heavily and is barely usable.
   
   1.5 day ago I migrated from 5.11.15 kernel to 5.12 + the LRU
   patchset, and I started up by opening lots of apps to create memory
   pressure, and worked for a day like this. Till now I had not a
   single SWAP-storm, and mind you I got 3.4G in SWAP. I was never
   getting to the point of 3G in SWAP before without a single
   SWAP-storm.

Vaibhav from IBM reported [12]:
   In a synthetic MongoDB Benchmark, seeing an average of ~19%
   throughput improvement on POWER10(Radix MMU + 64K Page Size) with
   MGLRU patches on top of 5.16 kernel for MongoDB + YCSB across
   three different request distributions, namely, Exponential, Uniform
   and Zipfan.

Shuang from U of Rochester reported [13]:
   With the MGLRU, fio achieved 95% CIs [38.95, 40.26]%, [4.12, 6.64]%
   and [9.26, 10.36]% higher throughput, respectively, for random
   access, Zipfian (distribution) access and Gaussian (distribution)
   access, when the average number of jobs per CPU is 1; 95% CIs
   [42.32, 49.15]%, [9.44, 9.89]% and [20.99, 22.86]% higher
   throughput, respectively, for random access, Zipfian access and
   Gaussian access, when the average number of jobs per CPU is 2.

Daniel from Michigan Tech reported [14]:
   With Memcached allocating ~100GB of byte-addressable Optante,
   performance improvement in terms of throughput (measured as queries
   per second) was about 10% for a series of workloads.

Large-scale deployments
-----------------------
We've rolled out MGLRU to tens of millions of Chrome OS users and
about a million Android users. Google's fleetwide profiling [15] shows
an overall 40% decrease in kswapd CPU usage, in addition to
improvements in other UX metrics, e.g., an 85% decrease in the number
of low-memory kills at the 75th percentile and an 18% decrease in
app launch time at the 50th percentile.

The downstream kernels that have been using MGLRU include:
1. Android [16]
2. Arch Linux Zen [17]
3. Armbian [18]
4. Chrome OS [19]
5. Liquorix [20]
6. post-factum [21]
7. XanMod [22]

[11] https://lore.kernel.org/r/140226722f2032c86301fbd326d91baefe3d7d23.camel@yandex.ru/
[12] https://lore.kernel.org/r/87czj3mux0.fsf@vajain21.in.ibm.com/
[13] https://lore.kernel.org/r/20220105024423.26409-1-szhai2@cs.rochester.edu/
[14] https://lore.kernel.org/r/CA+4-3vksGvKd18FgRinxhqHetBS1hQekJE2gwco8Ja-bJWKtFw@mail.gmail.com/
[15] https://dl.acm.org/doi/10.1145/2749469.2750392
[16] https://android.com
[17] https://archlinux.org
[18] https://armbian.com
[19] https://chromium.org
[20] https://liquorix.net
[21] https://codeberg.org/pf-kernel
[22] https://xanmod.org

Summery
=======
The facts are:
1. The independent lab results and the real-world applications
   indicate substantial improvements; there are no known regressions.
2. Thrashing prevention, working set estimation and proactive reclaim
   work out of the box; there are no equivalent solutions.
3. There is a lot of new code; no smaller changes have been
   demonstrated similar effects.

Our options, accordingly, are:
1. Given the amount of evidence, the reported improvements will likely
   materialize for a wide range of workloads.
2. Gauging the interest from the past discussions, the new features
   will likely be put to use for both personal computers and data
   centers.
3. Based on Google's track record, the new code will likely be well
   maintained in the long term. It'd be more difficult if not
   impossible to achieve similar effects with other approaches.

Yu Zhao (14):
  mm: x86, arm64: add arch_has_hw_pte_young()
  mm: x86: add CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG
  mm/vmscan.c: refactor shrink_node()
  Revert "include/linux/mm_inline.h: fold __update_lru_size() into its
    sole caller"
  mm: multi-gen LRU: groundwork
  mm: multi-gen LRU: minimal implementation
  mm: multi-gen LRU: exploit locality in rmap
  mm: multi-gen LRU: support page table walks
  mm: multi-gen LRU: optimize multiple memcgs
  mm: multi-gen LRU: kill switch
  mm: multi-gen LRU: thrashing prevention
  mm: multi-gen LRU: debugfs interface
  mm: multi-gen LRU: admin guide
  mm: multi-gen LRU: design doc

 Documentation/admin-guide/mm/index.rst        |    1 +
 Documentation/admin-guide/mm/multigen_lru.rst |  156 +
 Documentation/mm/index.rst                    |    1 +
 Documentation/mm/multigen_lru.rst             |  159 +
 arch/Kconfig                                  |    8 +
 arch/arm64/include/asm/pgtable.h              |   15 +-
 arch/x86/Kconfig                              |    1 +
 arch/x86/include/asm/pgtable.h                |    9 +-
 arch/x86/mm/pgtable.c                         |    5 +-
 fs/exec.c                                     |    2 +
 fs/fuse/dev.c                                 |    3 +-
 include/linux/cgroup.h                        |   15 +-
 include/linux/memcontrol.h                    |   36 +
 include/linux/mm.h                            |    5 +
 include/linux/mm_inline.h                     |  231 +-
 include/linux/mm_types.h                      |   77 +
 include/linux/mmzone.h                        |  214 ++
 include/linux/nodemask.h                      |    1 +
 include/linux/page-flags-layout.h             |   16 +-
 include/linux/page-flags.h                    |    4 +-
 include/linux/pgtable.h                       |   17 +-
 include/linux/sched.h                         |    4 +
 include/linux/swap.h                          |    4 +
 kernel/bounds.c                               |    7 +
 kernel/cgroup/cgroup-internal.h               |    1 -
 kernel/exit.c                                 |    1 +
 kernel/fork.c                                 |    9 +
 kernel/sched/core.c                           |    1 +
 mm/Kconfig                                    |   26 +
 mm/huge_memory.c                              |    3 +-
 mm/internal.h                                 |    1 +
 mm/memcontrol.c                               |   28 +
 mm/memory.c                                   |   39 +-
 mm/mm_init.c                                  |    6 +-
 mm/mmzone.c                                   |    2 +
 mm/rmap.c                                     |    6 +
 mm/swap.c                                     |   54 +-
 mm/vmscan.c                                   | 2972 ++++++++++++++++-
 mm/workingset.c                               |  110 +-
 39 files changed, 4095 insertions(+), 155 deletions(-)
 create mode 100644 Documentation/admin-guide/mm/multigen_lru.rst
 create mode 100644 Documentation/mm/multigen_lru.rst


base-commit: d2af7b221349ff6241e25fa8c67bcfae2b360700
-- 
2.37.1.595.g718a3a8f04-goog



From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id EBBECC433EF
	for <linux-kernel@archiver.kernel.org>; Tue,  2 Nov 2021 00:20:12 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id CC6DE60EBC
	for <linux-kernel@archiver.kernel.org>; Tue,  2 Nov 2021 00:20:12 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S229670AbhKBAWp (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 1 Nov 2021 20:22:45 -0400
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:49146 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S229479AbhKBAWo (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 1 Nov 2021 20:22:44 -0400
Received: from mail-qt1-x842.google.com (mail-qt1-x842.google.com [IPv6:2607:f8b0:4864:20::842])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id B6FDAC061714
        for <linux-kernel@vger.kernel.org>; Mon,  1 Nov 2021 17:20:09 -0700 (PDT)
Received: by mail-qt1-x842.google.com with SMTP id o12so6733067qtv.4
        for <linux-kernel@vger.kernel.org>; Mon, 01 Nov 2021 17:20:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=edi.works; s=google;
        h=from:to:cc:subject:date:message-id:in-reply-to:references;
        bh=tR1/30cvDfOtIR34b4k4ClDZHww95hp+zKlZOFEq2JQ=;
        b=DAcQ5/Xsi5s9DlvrYj12WbVhrq/fkIqQnNXRqltHDf2qKXvmRN6tj2WHEaPafFZVOq
         +f+aTRXWdoaCr0Z1szNaGryTAe+1g0PxiiCul9B6H343d0PZ/6PevDlSsFJ3lBm7TCwz
         irKBuA2dmEQehC937OvatwelwEFYAmKmXONM5/dB3YvYXE4sQ7F/kEJHzL4yIyNrccWm
         iJMvUqqhkvpop4e76Og3mFHUcxrXzOwo+jiVC6c86g0pLH3mc0rrwu6wpEJe7RgM74J5
         ZqQP4ejz6C/BDkYUvepjmXYZKTilcETlfK8qHbWA6O6eSx/0bcY9WMvVsLsNj8mqR9Ge
         OaQA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references;
        bh=tR1/30cvDfOtIR34b4k4ClDZHww95hp+zKlZOFEq2JQ=;
        b=OOlKSKY90pTDplY8OnqXhHsO+O6MbO7pmFoe+hZVB39D4KRfuf1wAp/AKuUYZezFX4
         IWP2e3Eh/V65LKHM/2QpDod9yqDmBGMhAs358sSEZY31RhGRn/+Lcat6DdqpKQEg1bgH
         +suyMFmzJcQBXeYSJs0yX37sSVh84cZVweaQYC3D6XF8CXUtNk7A3C0PdG8S4/cg7TcD
         tEZaOW4TXatg0ybRi9TF1fvMwj2F87XWzUCjn3HdLz4v4O3p98ep6LvogYAb2FKX8aE/
         03ujFTpQsd5ZjCLDpBO0FpG0PUASfErnQxaQEk9YYn+dQLtBiSNFQDIYr/CZGDz6Dl5g
         BAGQ==
X-Gm-Message-State: AOAM531dzzGmCHqzkJnCrzmj/8UPDeIBH/iT+nG+ZkxVeqKKagYpxCxo
        NgaWCMxhmqO1G4YAG7qwXMt3Zg==
X-Google-Smtp-Source: ABdhPJzmSPqeNNqfdNL7ckbCOVwM6H1394wvZSJnJil/u9nRo5BjZ9FQgwSef8OFJzdw3cbormxb2Q==
X-Received: by 2002:ac8:7f81:: with SMTP id z1mr10507176qtj.40.1635812408626;
        Mon, 01 Nov 2021 17:20:08 -0700 (PDT)
Received: from localhost.localdomain (c-67-169-44-201.hsd1.ca.comcast.net. [67.169.44.201])
        by smtp.gmail.com with ESMTPSA id g1sm10793909qkd.89.2021.11.01.17.20.07
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 01 Nov 2021 17:20:08 -0700 (PDT)
From:   bot@edi.works
To:     yuzhao@google.com
Cc:     hdanton@sina.com, linux-kernel@vger.kernel.org, linux-mm@kvack.org,
        page-reclaim@google.com, corbet@lwn.net,
        michael@michaellarabel.com, sofia.trinh@edi.works
Subject: Re: [PATCH v4 00/11] Multigenerational LRU Framework
Date:   Mon,  1 Nov 2021 17:20:02 -0700
Message-Id: <20211102002002.92051-1-bot@edi.works>
X-Mailer: git-send-email 2.18.0
In-Reply-To: <20210818063107.2696454-1-yuzhao@google.com>
References: <20210818063107.2696454-1-yuzhao@google.com>
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Kernel / Apache Spark benchmark with MGLRU

TLDR
====
With the MGLRU, Apache Spark took 95% CIs [9.28, 11.19]% and [12.20,
14.93]% less wall time to sort 3 billion random integers,
respectively, under the medium- and high-concurrency conditions when
slightly overcommitting memory. There were no statistically
significant changes in wall time when sorting the same dataset under
other conditions.

Background
==========
Memory overcommit can increase utilization and, if carried out
properly, can also increase throughput. The challenges are to improve
working set estimation and to optimize page reclaim. The risks are
performance degradations and OOM kills. Short of overcoming the
challenges, the only way to reduce the risks is to underutilize
memory.

Apache Spark is one of the most popular open-source big-data
frameworks. Dataset sorting is the most widely used benchmark for
such frameworks.

Matrix
======
Kernels: version [+ patchset]
* Baseline: 5.14
* Patched: 5.14 + MGLRU

Memory conditions: % of memory size
* Underutilizing: ~10% on inactive file list
* Overcommitting: ~10% swapped out

Concurrency conditions: average # of workers per CPU
* Low: 1
* Medium: 2
* High: 3

Cluster mode: local
Dataset size: 3 billion random integers (57GB text)

Total configurations: 12
Data points per configuration: 10
Total run duration (minutes) per data point: ~20

Procedure
=========
The latest MGLRU patchset for the 5.14 kernel is available at
git fetch https://linux-mm.googlesource.com/page-reclaim \
    refs/changes/30/1430/1

Baseline and patched 5.14 kernel images are available at
https://drive.google.com/drive/folders/1eMkQleAFGkP2vzM_JyRA21oKE0ESHBqp

<install and configure OS>
spark-shell < gen.scala

<for each kernel>
    grub2-set-default <baseline, patched>
    <for each memory condition>
        <update run_spark.sh>
        <for each concurrency condition>
            <update run_spark.sh>
            <for each data point>
                reboot
                run_spark.sh
                <collect wall time>

Hardware
========
Memory (GB): 64
CPU (total #): 32
NVMe SSD (GB): 1024

OS
==
$ cat /etc/redhat-release
Red Hat Enterprise Linux release 8.4 (Ootpa)

$ cat /proc/swaps
Filename          Type          Size          Used     Priority
/dev/nvme0n1p3    partition     32970748      0        -2

$ cat /proc/cmdline
<existing parameters> systemd.unified_cgroup_hierarchy=1

$ cat /sys/fs/cgroup/user.slice/memory.min
4294967296

$ cat /proc/sys/vm/overcommit_memory
1

$ cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]

Apache Spark
============
$ spark-shell --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.2
      /_/

Using Scala version 2.12.10, OpenJDK 64-Bit Server VM, 11.0.12
Branch HEAD
Compiled by user centos on 2021-05-24T04:27:48Z
Revision de351e30a90dd988b133b3d00fa6218bfcaba8b8
Url https://github.com/apache/spark
Type --help for more information.

$ cat gen.scala
import java.io._
import scala.collection.mutable.ArrayBuffer

object GenData {
    def main(args: Array[String]): Unit = {
        val file = new File("dataset.txt")
        val writer = new BufferedWriter(new FileWriter(file))
        val buf = ArrayBuffer(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L)
        for(_ <- 0 until 300000000) {
            for (i <- 0 until 10) {
                buf.update(i, scala.util.Random.nextLong())
            }
            writer.write(s"${buf.mkString(",")}\n")
        }
        writer.close()
    }
}
GenData.main(Array())

$ cat sort.scala
import java.time.temporal.ChronoUnit
import org.apache.spark.sql.SparkSession

object SparkSort {
    def main(args: Array[String]): Unit = {
        val spark = SparkSession.builder().getOrCreate()
        val file = sc.textFile("dataset.txt", 32)
        val start = java.time.Instant.now()
        val results = file.flatMap(_.split(",")).map(x => (x, 1)).sortByKey().takeOrdered(10)
        val finish = java.time.Instant.now()
        println(s"wall time: ${ChronoUnit.SECONDS.between(start, finish)}")
        results.foreach(println)
        spark.stop()
    }
}
SparkSort.main(Array())

$ cat run_spark.sh
spark-shell --master local\[<32, 64, 96>\] --driver-memory <52G, 62G> < sort.scala

Results
=======
Comparing the patched with the baseline kernel, Apache Spark took 95%
CIs [9.28, 11.19]% and [12.20, 14.93]% less wall time to sort the
dataset, respectively, under the medium- and high-concurrency
conditions when slightly overcommitting memory. There were no
statistically significant changes in wall time under other conditions.

+--------------------+-----------------------+-----------------------+
| Mean wall time (s) | Underutilizing memory | Overcommitting memory |
| [95% CI]           |                       |                       |
+--------------------+-----------------------+-----------------------+
| Low concurrency    | 1037.1 / 1037.0       | 1038.2 / 1036.6       |
|                    | [-1.41, 1.21]         | [-3.67, 0.47]         |
+--------------------+-----------------------+-----------------------+
| Medium concurrency | 1141.8 / 1142.6       | 1297.9 / 1165.1       |
|                    | [-1.35, 2.95]         | [-145.21, -120.38]    |
+--------------------+-----------------------+-----------------------+
| High concurrency   | 1239.3 / 1236.4       | 1456.8 / 1259.2       |
|                    | [-7.81, 2.01]         | [-217.53, -177.66]    |
+--------------------+-----------------------+-----------------------+
Table 1. Comparison between the baseline and patched kernels

Comparing overcommitting with underutilizing memory, Apache Spark
took 95% CIs [12.58, 14.76]% and [15.95, 19.15]% more wall time to
sort the dataset, respectively, under the low- and medium-concurrency
conditions when using the baseline kernel; 95% CIs [1.78, 2.16]% and
[1.42, 2.27]% more wall time, respectively, under the medium- and
high-concurrency conditions when using the patched kernel. There were
no statistically significant changes in wall time under other
conditions.

+--------------------+------------------------+----------------------+
| Mean wall time (s) | Baseline kernel        | Patched kernel       |
| [95% CI]           |                        |                      |
+--------------------+------------------------+----------------------+
| Low concurrency    | 1037.1 / 1038.2        | 1037.0 / 1036.6      |
|                    | [-0.31, 2.51]          | [-2.43, 1.63]        |
+--------------------+------------------------+----------------------+
| Medium concurrency | 1141.8 / 1297.9        | 1142.6 / 1165.1      |
|                    | [143.68, 168.51]       | [20.33, 24.66]       |
+--------------------+------------------------+----------------------+
| High concurrency   | 1239.3 / 1456.8        | 1236.4 / 1259.2      |
|                    | [197.62, 237.37]       | [17.55, 28.04]       |
+--------------------+------------------------+----------------------+
Table 2. Comparison between underutilizing and overcommitting memory

Metrics collected during each run are available at
https://github.com/ediworks/KernelPerf/tree/master/mglru/spark/5.14

Appendix
========
$ cat raw_data_spark.r
v <- c(
    # baseline 52g 32t
    1034, 1036, 1036, 1037, 1037, 1037, 1038, 1038, 1038, 1040,
    # baseline 52g 64t
    1139, 1139, 1140, 1140, 1142, 1143, 1143, 1144, 1144, 1144,
    # baseline 52g 96t
    1236, 1237, 1238, 1238, 1238, 1239, 1240, 1241, 1243, 1243,
    # baseline 62g 32t
    1036, 1036, 1038, 1038, 1038, 1038, 1039, 1039, 1040, 1040,
    # baseline 62g 64t
    1266, 1277, 1284, 1296, 1299, 1302, 1311, 1313, 1314, 1317,
    # baseline 62g 96t
    1403, 1431, 1440, 1447, 1460, 1461, 1467, 1475, 1487, 1497,
    # patched 52g 32t
    1035, 1036, 1036, 1037, 1037, 1037, 1037, 1038, 1038, 1039,
    # patched 52g 64t
    1138, 1140, 1140, 1143, 1143, 1143, 1144, 1145, 1145, 1145,
    # patched 52g 96t
    1228, 1228, 1233, 1234, 1235, 1236, 1236, 1240, 1246, 1248,
    # patched 62g 32t
    1032, 1035, 1035, 1035, 1036, 1036, 1037, 1039, 1040, 1041,
    # patched 62g 64t
    1162, 1164, 1164, 1164, 1164, 1164, 1166, 1166, 1168, 1169,
    # patched 62g 96t
    1252, 1256, 1256, 1258, 1260, 1260, 1260, 1260, 1265, 1265
)

a <- array(v, dim = c(10, 3, 2, 2))

# baseline vs patched
for (mem in 1:2) {
    for (con in 1:3) {
        r <- t.test(a[, con, mem, 1], a[, con, mem, 2])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("mem%d con%d: no significance", mem, con)
        } else {
            s <- sprintf("mem%d con%d: [%.2f, %.2f]%%", mem, con, -p[2], -p[1])
        }
        print(s)
    }
}

# 52g vs 62g
for (ker in 1:2) {
    for (con in 1:3) {
        r <- t.test(a[, con, 1, ker], a[, con, 2, ker])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("ker%d con%d: no significance", ker, con)
        } else {
            s <- sprintf("ker%d con%d: [%.2f, %.2f]%%", ker, con, -p[2], -p[1])
        }
        print(s)
    }
}

$ R -q -s -f raw_data_spark.r

        Welch Two Sample t-test

data:  a[, con, mem, 1] and a[, con, mem, 2]
t = 0.16059, df = 16.4, p-value = 0.8744
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -1.21749  1.41749
sample estimates:
mean of x mean of y
   1037.1    1037.0

[1] "mem1 con1: no significance"

        Welch Two Sample t-test

data:  a[, con, mem, 1] and a[, con, mem, 2]
t = -0.78279, df = 17.565, p-value = 0.4442
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2.950923  1.350923
sample estimates:
mean of x mean of y
   1141.8    1142.6

[1] "mem1 con2: no significance"

        Welch Two Sample t-test

data:  a[, con, mem, 1] and a[, con, mem, 2]
t = 1.2933, df = 11.303, p-value = 0.2217
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2.019103  7.819103
sample estimates:
mean of x mean of y
   1239.3    1236.4

[1] "mem1 con3: no significance"

        Welch Two Sample t-test

data:  a[, con, mem, 1] and a[, con, mem, 2]
t = 1.6562, df = 13.458, p-value = 0.1208
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.4799188  3.6799188
sample estimates:
mean of x mean of y
   1038.2    1036.6

[1] "mem2 con1: no significance"

        Welch Two Sample t-test

data:  a[, con, mem, 1] and a[, con, mem, 2]
t = 24.096, df = 9.2733, p-value = 1.115e-09
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 120.3881 145.2119
sample estimates:
mean of x mean of y
   1297.9    1165.1

[1] "mem2 con2: [-11.19, -9.28]%"

        Welch Two Sample t-test

data:  a[, con, mem, 1] and a[, con, mem, 2]
t = 22.289, df = 9.3728, p-value = 1.944e-09
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 177.6666 217.5334
sample estimates:
mean of x mean of y
   1456.8    1259.2

[1] "mem2 con3: [-14.93, -12.20]%"

        Welch Two Sample t-test

data:  a[, con, 1, ker] and a[, con, 2, ker]
t = -1.6398, df = 17.697, p-value = 0.1187
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2.5110734  0.3110734
sample estimates:
mean of x mean of y
   1037.1    1038.2

[1] "ker1 con1: no significance"

        Welch Two Sample t-test

data:  a[, con, 1, ker] and a[, con, 2, ker]
t = -28.33, df = 9.2646, p-value = 2.57e-10
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -168.5106 -143.6894
sample estimates:
mean of x mean of y
   1141.8    1297.9

[1] "ker1 con2: [12.58, 14.76]%"

        Welch Two Sample t-test

data:  a[, con, 1, ker] and a[, con, 2, ker]
t = -24.694, df = 9.1353, p-value = 1.12e-09
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -237.3794 -197.6206
sample estimates:
mean of x mean of y
   1239.3    1456.8

[1] "ker1 con3: [15.95, 19.15]%"

        Welch Two Sample t-test

data:  a[, con, 1, ker] and a[, con, 2, ker]
t = 0.42857, df = 12.15, p-value = 0.6757
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -1.630775  2.430775
sample estimates:
mean of x mean of y
   1037.0    1036.6

[1] "ker2 con1: no significance"

        Welch Two Sample t-test

data:  a[, con, 1, ker] and a[, con, 2, ker]
t = -21.865, df = 17.646, p-value = 3.151e-14
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -24.66501 -20.33499
sample estimates:
mean of x mean of y
   1142.6    1165.1

[1] "ker2 con2: [1.78, 2.16]%"

        Welch Two Sample t-test

data:  a[, con, 1, ker] and a[, con, 2, ker]
t = -9.2738, df = 14.72, p-value = 1.561e-07
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -28.04897 -17.55103
sample estimates:
mean of x mean of y
   1236.4    1259.2

[1] "ker2 con3: [1.42, 2.27]%"


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 34CABC433F5
	for <linux-kernel@archiver.kernel.org>; Sat,  9 Oct 2021 05:43:43 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 11E9960F41
	for <linux-kernel@archiver.kernel.org>; Sat,  9 Oct 2021 05:43:43 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S244275AbhJIFpV (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Sat, 9 Oct 2021 01:45:21 -0400
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:56306 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S232529AbhJIFpS (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Sat, 9 Oct 2021 01:45:18 -0400
Received: from mail-qt1-x844.google.com (mail-qt1-x844.google.com [IPv6:2607:f8b0:4864:20::844])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id 7E56BC061570
        for <linux-kernel@vger.kernel.org>; Fri,  8 Oct 2021 22:43:19 -0700 (PDT)
Received: by mail-qt1-x844.google.com with SMTP id b12so3523124qtq.3
        for <linux-kernel@vger.kernel.org>; Fri, 08 Oct 2021 22:43:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=edi.works; s=google;
        h=from:to:cc:subject:date:message-id:in-reply-to:references;
        bh=R1b6LRnDgw4MXgnqL8ApMyFBNf+HWB+MOQp9ynyp86Q=;
        b=Xwz0/FiEnkJdoNRxntzjBQc0Gm3a1pMvmW4rJ/VSan9bUkANBa8mnhQrPpTuGc0Hae
         8CKOaacVHDcYNUMJQFgQD534oCqNmai1pLLiTVPnZ4cWEVJwArNfkY73kb0XyZo41D+F
         Re7wlKk2DZd5JyjQS9qbCV0Zv0ZJs2U1OpAOJEX+Tpy1Qep/sHjRtrnhK+TAM2WxAPui
         f2/ZMviteTrkP6MkVvrBa2n9dgRQl9+kEC7v4zjfh253YkytvYNWFvblp3AzR5xxFxc1
         vOxYItPXxuJe9RKIuMpMtPUX47N9auwSnFl1gCSrqIrTBrKHKiQpUxBZZRpsLKmKXoNT
         Ra1w==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references;
        bh=R1b6LRnDgw4MXgnqL8ApMyFBNf+HWB+MOQp9ynyp86Q=;
        b=Owm8wGCPpsOm0XiQP7IzQP6t5a6FlVsWc9IooTmSZZbwqzzqw6JNeK3dO/6n0d46pN
         JO0+iJfBYaDsY0I+MB6Outg+by4g7CzdVaLV3M3qZ4d1+I71g36hfrhls5z52fESOrIl
         OgiPZv6i1o9E/1ZVbFVOzr0rshKl4R7mjT9h8Gq0qpgGm613OKQ1vqUcc9HhazLl5omP
         GvmXB3VGYhZwQbcEyYfILca3bX5f0v9y0uHCOT1BXzDSEL8xZbcirHx7V2lwxVk3P/Zc
         ffb03C3ergfJx8zgmPwOIhF4KO8qF0CQTucj3WunSsh4/md+U804t2EpKfBdmPhDK7jH
         i4Uw==
X-Gm-Message-State: AOAM533KkikHfuRAMUyLmtf7jaz9hLsnsWxd/kn02wZXaPO/+L6UvdLB
        57Jj1qoR8wL/JeuxP2+G4iPejg==
X-Google-Smtp-Source: ABdhPJyhJjm+JmkYPHf2Z7h4TM+aA7Ak9oB7tciu7UAheWRLg4EpDJMCE5h16KIxP85tql1QpPeCdA==
X-Received: by 2002:ac8:4e92:: with SMTP id 18mr2318782qtp.323.1633758198507;
        Fri, 08 Oct 2021 22:43:18 -0700 (PDT)
Received: from localhost.localdomain (c-67-169-44-201.hsd1.ca.comcast.net. [67.169.44.201])
        by smtp.gmail.com with ESMTPSA id t64sm1063783qkd.71.2021.10.08.22.43.16
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Fri, 08 Oct 2021 22:43:17 -0700 (PDT)
From:   bot@edi.works
To:     yuzhao@google.com
Cc:     hdanton@sina.com, linux-kernel@vger.kernel.org, linux-mm@kvack.org,
        page-reclaim@google.com, corbet@lwn.net,
        michael@michaellarabel.com, sofia.trinh@edi.works
Subject: Re: [PATCH v4 00/11] Multigenerational LRU Framework
Date:   Fri,  8 Oct 2021 22:43:15 -0700
Message-Id: <20211009054315.47073-1-bot@edi.works>
X-Mailer: git-send-email 2.18.0
In-Reply-To: <20210818063107.2696454-1-yuzhao@google.com>
References: <20210818063107.2696454-1-yuzhao@google.com>
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Kernel / MariaDB benchmark with MGLRU

TLDR
====
With the MGLRU, MariaDB achieved 95% CIs [5.24, 10.71]% and [20.22,
25.97]% more transactions per minute (TPM), respectively, under the
medium- and high-concurrency conditions when slightly overcommitting
memory. There were no statistically significant changes in TPM under
other conditions.

Rationale
=========
Memory overcommit can improve utilization and, if not overdone, can
also increase throughput. The challenges are estimating working sets
and optimizing page reclaim. The risks are performance degradations
and OOM kills. Unless overcoming the challenges, the only way to
reduce the risks is to overprovision memory.

MariaDB is one of the most popular open-source RDBMSs. HammerDB is
the leading open-source benchmarking software derived from the TPC
specifications. OLTP is the most important use case for RDBMSs.

Matrix
======
Kernels: version [+ patchset]
* Baseline: 5.14
* Patched: 5.14 + MGLRU

Memory conditions: % of memory size
* Underutilizing: ~10% on inactive file list
* Overcommitting: ~10% swapped out

Concurrency conditions: average # of users per CPU
* Low: ~3
* Medium: ~13
* High: ~19

Total configurations: 12
Data points per configuration: 10
Total run duration (minutes) per data point: ~45

Procedure
=========
The latest MGLRU patchset for the 5.14 kernel is available at
git fetch https://linux-mm.googlesource.com/page-reclaim \
  refs/changes/30/1430/1

Baseline and patched 5.14 kernel images are available at
https://drive.google.com/drive/folders/1eMkQleAFGkP2vzM_JyRA21oKE0ESHBqp

<install and configure OS>
hammerdbcli auto prep_tpcc.tcl
systemctl stop mariadb
e2image <backup /mnt/data>

<for each kernel>
    grub2-set-default <baseline / patched>
    <for each memory condition>
        <update /etc/my.cnf>
        <for each concurrency condition>
            <update run_tpcc.tcl>
            <for each data point>
                systemctl stop mariadb
                e2image <restore /mnt/data>
                reboot
                hammerdbcli auto run_tpcc.tcl
                <collect TPM>

Hardware
========
Memory (GB): 64
CPU (total #): 32
NVMe SSD (GB): 1024

OS
==
$ cat /etc/redhat-release
Red Hat Enterprise Linux release 8.4 (Ootpa)

$ cat /proc/swaps
Filename          Type          Size          Used     Priority
/dev/nvme0n1p3    partition     32970748      0          -2

$ mount | grep data
/dev/nvme0n1p4 on /mnt/data type ext4 (rw,relatime,seclabel)

$ cat /proc/cmdline
<existing parameters> systemd.unified_cgroup_hierarchy=1

$ cat /sys/fs/cgroup/user.slice/memory.min
4294967296

$ cat /proc/sys/vm/overcommit_memory
1

MariaDB
=======
$ mysql --version
mysql  Ver 15.1 Distrib 10.3.28-MariaDB, for Linux (x86_64) using
readline 5.1

$ cat /etc/my.cnf
<existing parameters>

[mysqld]
innodb_buffer_pool_size=<50G, 60G>
innodb_doublewrite=0
innodb_flush_log_at_trx_commit=0
innodb_flush_method=O_DIRECT_NO_FSYNC
innodb_flush_neighbors=0
innodb_io_capacity=4000
innodb_io_capacity_max=20000
innodb_log_buffer_size=1G
innodb_log_file_size=20G
innodb_max_dirty_pages_pct=90
innodb_max_dirty_pages_pct_lwm=10
max_connections=1000
datadir=/mnt/data

HammerDB
========
$ hammerdbcli -h
HammerDB CLI v4.2
Copyright (C) 2003-2021 Steve Shaw
Type "help" for a list of commands
Usage: hammerdbcli [ auto [ script_to_autoload.tcl  ] ]

$ cat prep_tpcc.tcl
dbset db maria
diset connection maria_socket /var/lib/mysql/mysql.sock
diset tpcc maria_count_ware 1200
diset tpcc maria_num_vu 32
diset tpcc maria_partition true
buildschema
waittocomplete
quit

$ cat run_tpcc.tcl
dbset db maria
diset connection maria_socket /var/lib/mysql/mysql.sock
diset tpcc maria_total_iterations 20000000
diset tpcc maria_driver timed
diset tpcc maria_rampup 10
diset tpcc maria_duration 30
diset tpcc maria_allwarehouse true
vuset logtotemp 1
vuset unique 1
loadscript
vuset vu <100, 400, 600>
vucreate
vurun
runtimer 3000
Vudestroy

Results
=======
Comparing the patched with the baseline kernel, MariaDB achieved 95%
CIs [5.24, 10.71]% and [20.22, 25.97]% more TPM, respectively, under
the medium- and high-concurrency conditions when slightly
overcommitting memory. There were no statistically significant
changes in TPM under other conditions.

+--------------------+-----------------------+-----------------------+
| Mean TPM [95% CI]  | Underutilizing memory | Overcommitting memory |
+--------------------+-----------------------+-----------------------+
| Low concurrency    | 270811.6 / 271522.7   | 447933.4 / 447283.3   |
|                    | [-40.97, 1463.17]     | [-1330.61, 30.41]     |
+--------------------+-----------------------+-----------------------+
| Medium concurrency | 240212.9 / 242846.7   | 327276.6 / 353372.7   |
|                    | [-2611.38, 7878.98]   | [17149.01, 35043.19]  |
+--------------------+-----------------------+-----------------------+
| High concurrency   | 283897.8 / 283668.1   | 274069.7 / 337366.8   |
|                    | [-11538.08, 11078.68] | [55417.42, 71176.78]  |
+--------------------+-----------------------+-----------------------+
Table 1. Comparison between the baseline and patched kernels

Comparing overcommitting with underutilizing memory, MariaDB achieved
95% CIs [65.12, 65.68]% and [32.45, 40.04]% more TPM, respectively,
under the low- and medium-concurrency conditions when using the
baseline kernel; 95% CIs [64.48, 64.98]%, [43.53, 47.50]% and [16.48,
21.38]% more TPM, respectively, under the low-, medium- and
high-concurrency conditions when using the patched kernel. There were
no statistically significant changes in TPM under other conditions.

+--------------------+------------------------+----------------------+
| Mean TPM [95% CI]  | Baseline kernel        | Patched kernel       |
+--------------------+------------------------+----------------------+
| Low concurrency    | 270811.6 / 447933.4    | 271522.7 / 447283.3  |
|                    | [176362.0, 177881.6]   | [175089.3, 176431.9] |
+--------------------+------------------------+----------------------+
| Medium concurrency | 240212.9 / 327276.6    | 242846.7 / 353372.7  |
|                    | [77946.4, 96181.0]     | [105707.7, 115344.3] |
+--------------------+------------------------+----------------------+
| High concurrency   | 283897.8 / 274069.7    | 283668.1 / 337366.8  |
|                    | [-21605.703, 1949.503] | [46758.85, 60638.55] |
+--------------------+------------------------+----------------------+
Table 2. Comparison between underutilizing and overcommitting memory

Metrics collected during each run are available at
https://github.com/ediworks/KernelPerf/tree/master/mglru/mariadb/5.14

References
==========
HammerDB v4.2 New Features:
https://www.hammerdb.com/blog/uncategorized/hammerdb-v4-2-new-features
-pt1-mariadb-build-and-test-example-with-the-cli/

Appendix
========
$ cat raw_data.r
v <- c(
# baseline 50g 100vu
269531,270113,270256,270367,270393,270630,270707,271373,272291,272455,
# baseline 50g 400vu
231856,234985,235144,235552,238551,239994,244413,245255,247997,248382,
# baseline 50g 600vu
256365,271733,275966,280623,281014,283764,293327,296750,298728,300708,
# baseline 60g 100vu
446973,447383,447412,447489,447874,448046,448123,448531,448739,448764,
# baseline 60g 400vu
312427,312936,313780,321503,329554,330551,332377,333584,337105,348949,
# baseline 60g 600vu
262338,262971,266242,266489,268036,272494,279045,281472,289942,291668,
# patched 50g 100vu
270621,270913,271026,271137,271517,271616,271699,272117,272218,272363,
# patched 50g 400vu
233314,238265,238722,240540,241676,245204,245688,247440,248417,249201,
# patched 50g 600vu
271114,271928,277562,279455,282074,285515,287836,288508,289451,303238,
# patched 60g 100vu
445923,446178,446837,446889,447331,447480,447823,447999,448145,448228,
# patched 60g 400vu
345705,349373,350832,351229,351758,352520,355130,355247,357762,364171,
# patched 60g 600vu
330860,334705,336001,337291,338326,338361,338970,339163,339784,340207
)

a <- array(v, dim = c(10, 3, 2, 2))

# baseline vs patched
for (m in 1:2) {
    for (c in 1:3) {
        r <- t.test(a[, c, m, 1], a[, c, m, 2])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("m%d c%d: no significance", m, c)
        } else {
            s <- sprintf("m%d c%d: [%.2f, %.2f]%%", m, c, -p[2],
-p[1])
        }
        print(s)
    }
}

# 50g vs 60g
for (k in 1:2) {
    for (c in 1:3) {
        r <- t.test(a[, c, 1, k], a[, c, 2, k])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("k%d c%d: no significance", k, c)
        } else {
            s <- sprintf("k%d c%d: [%.2f, %.2f]%%", k, c, -p[2], -p[1])
        }
        print(s)
    }
}

$ R -q -s -f raw_data.r

        Welch Two Sample t-test

data:  a[, c, m, 1] and a[, c, m, 2]
t = -2.0139, df = 15.122, p-value = 0.06217
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -1463.17673    40.97673
sample estimates:
mean of x mean of y
 270811.6  271522.7

[1] "50g 100vu: no significance"

        Welch Two Sample t-test

data:  a[, c, m, 1] and a[, c, m, 2]
t = -1.0564, df = 17.673, p-value = 0.305
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -7878.98  2611.38
sample estimates:
mean of x mean of y
 240212.9  242846.7

[1] "50g 400vu: no significance"

        Welch Two Sample t-test

data:  a[, c, m, 1] and a[, c, m, 2]
t = 0.043083, df = 15.895, p-value = 0.9662
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -11078.68  11538.08
sample estimates:
mean of x mean of y
 283897.8  283668.1

[1] "50g 600vu: no significance"

        Welch Two Sample t-test

data:  a[, c, m, 1] and a[, c, m, 2]
t = 2.0171, df = 16.831, p-value = 0.05993
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  -30.41577 1330.61577
sample estimates:
mean of x mean of y
 447933.4  447283.3

[1] "60g 100vu: no significance"

        Welch Two Sample t-test

data:  a[, c, m, 1] and a[, c, m, 2]
t = -6.3473, df = 12.132, p-value = 3.499e-05
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -35043.19 -17149.01
sample estimates:
mean of x mean of y
 327276.6  353372.7

[1] "60g 400vu: [5.24, 10.71]%"

        Welch Two Sample t-test

data:  a[, c, m, 1] and a[, c, m, 2]
t = -17.844, df = 10.233, p-value = 4.822e-09
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -71176.78 -55417.42
sample estimates:
mean of x mean of y
 274069.7  337366.8

[1] "60g 600vu: [20.22, 25.97]%"

        Welch Two Sample t-test

data:  a[, c, 1, k] and a[, c, 2, k]
t = -495.48, df = 15.503, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -177881.6 -176362.0
sample estimates:
mean of x mean of y
 270811.6  447933.4

[1] "baseline 100vu: [65.12, 65.68]%"

        Welch Two Sample t-test

data:  a[, c, 1, k] and a[, c, 2, k]
t = -20.601, df = 13.182, p-value = 2.062e-11
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -96181.0 -77946.4
sample estimates:
mean of x mean of y
 240212.9  327276.6

[1] "baseline 400vu: [32.45, 40.04]%"

        Welch Two Sample t-test

data:  a[, c, 1, k] and a[, c, 2, k]
t = 1.7607, df = 16.986, p-value = 0.09628
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -1949.503 21605.703
sample estimates:
mean of x mean of y
 283897.8  274069.7

[1] "baseline 600vu: no significance"

        Welch Two Sample t-test

data:  a[, c, 1, k] and a[, c, 2, k]
t = -553.68, df = 16.491, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -176431.9 -175089.3
sample estimates:
mean of x mean of y
 271522.7  447283.3

[1] "patched 100vu: [64.48, 64.98]%"

        Welch Two Sample t-test

data:  a[, c, 1, k] and a[, c, 2, k]
t = -48.194, df = 17.992, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -115344.3 -105707.7
sample estimates:
mean of x mean of y
 242846.7  353372.7

[1] "patched 400vu: [43.53, 47.50]%"

        Welch Two Sample t-test

data:  a[, c, 1, k] and a[, c, 2, k]
t = -17.109, df = 10.6, p-value = 4.629e-09
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -60638.55 -46758.85
sample estimates:
mean of x mean of y
 283668.1  337366.8

[1] "patched 600vu: [16.48, 21.38]%"


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 73E7EC433F5
	for <linux-kernel@archiver.kernel.org>; Thu, 21 Oct 2021 19:41:10 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id 53F906121F
	for <linux-kernel@archiver.kernel.org>; Thu, 21 Oct 2021 19:41:10 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S231702AbhJUTnZ (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Thu, 21 Oct 2021 15:43:25 -0400
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:55596 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S230020AbhJUTnY (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 21 Oct 2021 15:43:24 -0400
Received: from mail-qt1-x843.google.com (mail-qt1-x843.google.com [IPv6:2607:f8b0:4864:20::843])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id B37D5C061764
        for <linux-kernel@vger.kernel.org>; Thu, 21 Oct 2021 12:41:07 -0700 (PDT)
Received: by mail-qt1-x843.google.com with SMTP id n2so1558713qta.2
        for <linux-kernel@vger.kernel.org>; Thu, 21 Oct 2021 12:41:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=edi.works; s=google;
        h=from:to:cc:subject:date:message-id:in-reply-to:references;
        bh=ya+ey+/s1GWZFIFDEVYnkh73PpmflCvvRgfCjgGVd2s=;
        b=Xfi1Dn95lqsvmCPIc9GnNDru6HiU0weozjV/Unthgfpg2bqsLQ47oUJ/Z2lrDYgv4+
         Q2LZe6wFOpSHL5FTPcC+NLlyyg5PaLJOn9+QILPOQ+gkP1JzYkLFjtZf1dGhpEnb/Bhm
         XJGy8O7SW3jp1HVtKIdU/I+jKwwIRqePw4pxnwKb5zaNcFXYVu8EWC2mGJdKvpLvA5rR
         sYzUqBIgoEDLa55RAcIa6+8cyL5lkFhwlC0viulj63CT885EzIyO32ApeqVMUGQiMDeS
         FTqSIInkVM6/agBSGZVYHLYx4jdJcN59OqOpMPFMfkZbfG49GTX0Yy50UXMcCMGK1mFJ
         52Aw==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references;
        bh=ya+ey+/s1GWZFIFDEVYnkh73PpmflCvvRgfCjgGVd2s=;
        b=aZ9OFvlLKwGL7t3LJ8xF47sbkoH55IEGL8fldBp5MGfVRBAe7+YCX/9hdpzKWhuQj8
         jjxFakOyohNkNjEQoCel8Xicffodw3nJ+FhOyjcLWi/Qq34ddTTPH2grQMaEfxfeaobR
         t/d0gIWWjApz24+ZoDjH/QIoJ96xLGBYlzFXyuMXzB/EpDBTttZN0Xd2kb0smLxnxZTT
         c3o+IdBt6p1ugXcxsd20Yk6z0kZnRkvYLyEx5yeCgKbznBmFgFyQae+bjk+JJXKcDmfP
         BuTweW5F9xgj7dL4Z+lGTUaVyWKXOmWV8UUZ7Px7MukwxL7/ojI4+XONouCipfL2w1WG
         mG8g==
X-Gm-Message-State: AOAM533be2cRCB8tP5yW/o33yfq7agRAb+rDtPEo8u7uaLyFEOuIBDcx
        rgEko7OjJO9DgHXeFevLvKH/Ow==
X-Google-Smtp-Source: ABdhPJyzSsn7y+qPAs8E1dehOi51zLVJJCYFsl8n/25ZtWFBz+j/k5Vz5EpwQdRSMgQb7oILZ3MQsg==
X-Received: by 2002:a05:622a:1194:: with SMTP id m20mr8330851qtk.175.1634845266822;
        Thu, 21 Oct 2021 12:41:06 -0700 (PDT)
Received: from localhost.localdomain (c-67-169-44-201.hsd1.ca.comcast.net. [67.169.44.201])
        by smtp.gmail.com with ESMTPSA id c26sm2904609qtm.21.2021.10.21.12.41.05
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Thu, 21 Oct 2021 12:41:06 -0700 (PDT)
From:   bot@edi.works
To:     yuzhao@google.com
Cc:     hdanton@sina.com, linux-kernel@vger.kernel.org, linux-mm@kvack.org,
        page-reclaim@google.com, corbet@lwn.net,
        michael@michaellarabel.com, sofia.trinh@edi.works
Subject: Re: [PATCH v4 00/11] Multigenerational LRU Framework
Date:   Thu, 21 Oct 2021 12:41:03 -0700
Message-Id: <20211021194103.65648-1-bot@edi.works>
X-Mailer: git-send-email 2.18.0
In-Reply-To: <20210818063107.2696454-1-yuzhao@google.com>
References: <20210818063107.2696454-1-yuzhao@google.com>
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Kernel / Memcached benchmark with MGLRU

TLDR
====
With the MGLRU, Memcached achieved 95% CIs [23.54, 32.25]%, [20.76,
41.61]%, [13.85, 15.97]%, [21.59, 30.02]% and [23.94, 29.92]% more
operations per second (OPS), respectively, for sequential access w/
THP=always, random access w/ THP=always, random access w/ THP=never,
Gaussian access w/ THP=always and Gaussian access w/ THP=never. There
were no statistically significant changes in OPS for sequential
access w/ THP=never.

Background
==========
Memory overcommit can increase utilization and, if carried out
properly, can also increase throughput. The challenges are to improve
working set estimation and to optimize page reclaim. The risks are
performance degradations and OOM kills. Short of overcoming the
challenges, the only way to reduce the risks is to underutilize
memory.

Memcached is one of the most popular open-source in-memory KV stores.
memtier_benchmark is the leading open-source KV store benchmarking
software that supports multiple access patterns. THP can have a
negative effect under memory pressure, due to internal and/or
external fragmentations.

Matrix
======
Kernels: version [+ patchset]
* Baseline: 5.14
* Patched: 5.14 + MGLRU

Memory conditions: % of memory size
* Underutilizing: N/A
* Overcommitting: ~10% swapped out (zram)

THP (2MB Transparent Huge Pages):
* Always
* Never

Read patterns (2kB objects):
* Parallel sequential
* Uniform random
* Gaussian (SD = 1/6 of key range)

Total configurations: 12
Data points per configuration: 10
Total run duration (minutes) per data point: ~20

Note that the goal of this benchmark is to compare the performance
for the same key range, object size, and hit ratio. Since Memcached
does not support backing storage, it requires fewer in-memory objects
to underutilize memory, which reduces the hit ratio and therefore is
not applicable in this case.

Procedure
=========
The latest MGLRU patchset for the 5.14 kernel is available at
git fetch https://linux-mm.googlesource.com/page-reclaim \
    refs/changes/30/1430/1

Baseline and patched 5.14 kernel images are available at
https://drive.google.com/drive/folders/1eMkQleAFGkP2vzM_JyRA21oKE0ESHBqp

<install and configure OS>

<for each kernel>
    grub2-set-default <baseline, patched>
    <for each THP setting>
        echo <always, never> > \
            /sys/kernel/mm/transparent_hugepage/enabled
        <update /etc/sysconfig/memcached>
        <for each access pattern>
            <update run_memtier.sh>
            <for each data point>
                reboot
                run_memtier.sh
                <collect OPS>

Hardware
========
Memory (GB): 64
CPU (total #): 32
NVMe SSD (GB): 1024

OS
==
$ cat /etc/redhat-release
Red Hat Enterprise Linux release 8.4 (Ootpa)

$ cat /proc/swaps
Filename          Type          Size          Used     Priority
/dev/zram0        partition     8388604       0        -2

$ cat /proc/cmdline
<existing parameters> systemd.unified_cgroup_hierarchy=1

$ cat /sys/fs/cgroup/user.slice/memory.min
4294967296

$ cat /proc/sys/vm/overcommit_memory
1

Memcached
=========
$ memcached -V
memcached 1.5.22

$ cat /etc/sysconfig/memcached
USER="memcached"
MAXCONN="10000"
CACHESIZE="65536"
OPTIONS="-s /tmp/memcached.sock -a 0766 -t 16 -b 10000 -B binary <-L>"
memtier_benchmark
$ memtier_benchmark -v
memtier_benchmark 1.3.0
Copyright (C) 2011-2020 Redis Labs Ltd.
This is free software.  You may redistribute copies of it under the
terms of
the GNU General Public License <http://www.gnu.org/licenses/gpl.html>.
There is NO WARRANTY, to the extent permitted by law.

$ cat run_memtier.sh
# load objects
memtier_benchmark -S /tmp/memcached.sock -P memcache_binary -n
allkeys -c 1 -t 16 --ratio 1:0 --pipeline 1 -d 2000 --key-minimum=1
--key-maximum=30000000 --key-pattern=P:P

# run benchmark
memtier_benchmark -S /tmp/memcached.sock -P memcache_binary -n
30000000 -c 1 -t 16 --ratio 0:1 --pipeline 1 --randomize
--distinct-client-seed --key-minimum=1 --key-maximum=30000000
--key-pattern=<P:P, R:R, G:G>

Results
=======
Comparing the patched with the baseline kernel, Memcached achieved
95% CIs [23.54, 32.25]%, [20.76, 41.61]%, [13.85, 15.97]%, [21.59,
30.02]% and [23.94, 29.92]% more OPS, respectively, for sequential
access w/ THP=always, random access w/ THP=always, random access w/
THP=never, Gaussian access w/ THP=always and Gaussian access w/
THP=never. There were no statistically significant changes in OPS for
sequential access w/ THP=never.

+-------------------+-----------------------+------------------------+
| Mean OPS [95% CI] | THP=always            | THP=never              |
+-------------------+-----------------------+------------------------+
| Sequential access | 519599.7 / 664543.2   | 525394.8 / 527170.6    |
|                   | [122297.9, 167589.0]  | [-15138.63, 18690.31]  |
+-------------------+-----------------------+------------------------+
| Random access     | 450033.2 / 590360.7   | 509237.3 / 585142.4    |
|                   | [93415.59, 187239.37] | [70504.51, 81305.60]   |
+-------------------+-----------------------+------------------------+
| Gaussian access   | 481182.4 / 605358.7   | 531270.8 / 674341.4    |
|                   | [103892.6, 144460.0]] | [127199.8, 158941.2]   |
+-------------------+-----------------------+------------------------+
Table 1. Comparison between the baseline and patched kernels

Comparing THP=never with THP=always, Memcached achieved 95% CIs
[2.73, 23.58]% and [5.45, 15.37]% more OPS, respectively, for random
access and Gaussian access when using the baseline kernel; 95% CIs
[-22.65, -18.69]% and [10.67, 12.12]% more OPS, respectively, for
sequential access and Gaussian access when using the patched kernel.
There were no statistically significant changes in OPS under other
conditions.

+-------------------+-----------------------+------------------------+
| Mean OPS [95% CI] | Baseline kernel       |  Patched kernel        |
+-------------------+-----------------------+------------------------+
| Sequential access | 519599.7 / 525394.8   | 664543.2 / 527170.6    |
|                   | [-18739.71, 30329.80] | [-150551.0, -124194.1] |
+-------------------+-----------------------+------------------------+
| Random access     | 450033.2 / 509237.3   | 590360.7 / 585142.4    |
|                   | [12303.49, 106104.69] | [-10816.1516, 379.475] |
+-------------------+-----------------------+------------------------+
| Gaussian access   | 481182.4 / 531270.8   | 605358.7 / 674341.4    |
|                   | [26229.02, 73947.84]  | [64570.58, 73394.70]   |
+-------------------+-----------------------+------------------------+
Table 2. Comparison between THP=always and THP=never

Metrics collected during each run are available at
https://github.com/ediworks/KernelPerf/tree/master/mglru/memcached/5.14

References
==========
memtier_benchmark: A High-Throughput Benchmarking Tool for Redis &
Memcached
https://redis.com/blog/memtier_benchmark-a-high-throughput-benchmarking-tool-for-redis-memcached/

Appendix
========
$ cat raw_data.r
v <- c(
    # baseline THP=always sequential
    460266.29, 466497.70, 516145.38, 523474.39, 528507.72, 529481.86, 533867.92, 537028.56, 546027.45, 554699.89,
    # baseline THP=always random
    371470.66, 378967.63, 381137.01, 385205.60, 449100.72, 474670.76, 490470.46, 513341.53, 525159.49, 530808.55,
    # baseline THP=always Gaussian
    455674.14, 457089.50, 460001.46, 463269.94, 468283.00, 474169.61, 477684.67, 506331.96, 507875.30, 541444.54,
    # baseline THP=never sequential
    501887.04, 507303.10, 509573.54, 515222.79, 517429.04, 530805.74, 536490.44, 538088.45, 540459.92, 556687.57,
    # baseline THP=never random
    496489.97, 506444.42, 508002.80, 508707.39, 509746.28, 511157.58, 511897.57, 511926.06, 512652.28, 515348.95,
    # baseline THP=never Gaussian
    493199.15, 504207.48, 518781.40, 520536.21, 528619.45, 540677.91, 544365.57, 551698.32, 554046.80, 556576.14,
    # patched THP=always sequential
    660711.43, 660936.88, 661275.57, 662540.65, 663417.25, 665546.99, 665680.49, 667564.03, 668555.96, 669202.36,
    # patched THP=always random
    582574.69, 583714.04, 587102.54, 587375.85, 588997.85, 589052.96, 593922.17, 594722.98, 596178.28, 599965.83,
    # patched THP=always Gaussian
    601707.98, 602055.03, 603020.28, 603335.93, 604519.55, 605086.48, 607405.59, 607570.79, 609009.54, 609875.98,
    # patched THP=never sequential
    507753.56, 509462.65, 509964.30, 510369.66, 515001.36, 531685.00, 543709.22, 545142.98, 548392.56, 550224.74,
    # patched THP=never random
    571017.21, 579705.57, 582801.51, 584475.82, 586247.73, 587209.97, 587354.87, 588661.14, 591237.23, 592712.76,
    # patched THP=never Gaussian
    666403.77, 669691.68, 670248.43, 672190.97, 672466.43, 674320.42, 674897.72, 677282.76, 678886.51, 687024.85
)

a <- array(v, dim = c(10, 3, 2, 2))

# baseline vs patched
for (thp in 1:2) {
    for (pattern in 1:3) {
        r <- t.test(a[, pattern, thp, 1], a[, pattern, thp, 2])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("thp%d pattern%d: no significance", thp, pattern)
        } else {
            s <- sprintf("thp%d pattern%d: [%.2f, %.2f]%%", thp, pattern, -p[2], -p[1])
        }
        print(s)
    }
}

# THP=always vs THP=never
for (kernel in 1:2) {
    for (pattern in 1:3) {
        r <- t.test(a[, pattern, 1, kernel], a[, pattern, 2, kernel])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("kernel%d pattern%d: no significance", kernel, pattern)
        } else {
            s <- sprintf("kernel%d pattern%d: [%.2f, %.2f]%%", kernel, pattern, -p[2], -p[1])
        }
        print(s)
    }
}

$ R -q -s -f raw_data.r

        Welch Two Sample t-test

data:  a[, pattern, thp, 1] and a[, pattern, thp, 2]
t = -14.434, df = 9.1861, p-value = 1.269e-07
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -167589.0 -122297.9
sample estimates:
mean of x mean of y
 519599.7  664543.2

[1] "thp1 pattern1: [23.54, 32.25]%"

        Welch Two Sample t-test

data:  a[, pattern, thp, 1] and a[, pattern, thp, 2]
t = -6.7518, df = 9.1333, p-value = 7.785e-05
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -187239.37  -93415.59
sample estimates:
mean of x mean of y
 450033.2  590360.7

[1] "thp1 pattern2: [20.76, 41.61]%"

        Welch Two Sample t-test

data:  a[, pattern, thp, 1] and a[, pattern, thp, 2]
t = -13.805, df = 9.1933, p-value = 1.866e-07
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -144460.0 -103892.6
sample estimates:
mean of x mean of y
 481182.4  605358.7

[1] "thp1 pattern3: [21.59, 30.02]%"

        Welch Two Sample t-test

data:  a[, pattern, thp, 1] and a[, pattern, thp, 2]
t = -0.22059, df = 17.979, p-value = 0.8279
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -18690.31  15138.63
sample estimates:
mean of x mean of y
 525394.8  527170.6

[1] "thp2 pattern1: no significance"

        Welch Two Sample t-test

data:  a[, pattern, thp, 1] and a[, pattern, thp, 2]
t = -29.606, df = 17.368, p-value = 2.611e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -81305.60 -70504.51
sample estimates:
mean of x mean of y
 509237.3  585142.4

[1] "thp2 pattern2: [13.85, 15.97]%"

        Welch Two Sample t-test

data:  a[, pattern, thp, 1] and a[, pattern, thp, 2]
t = -20.02, df = 10.251, p-value = 1.492e-09
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -158941.2 -127199.8
sample estimates:
mean of x mean of y
 531270.8  674341.4

[1] "thp2 pattern3: [23.94, 29.92]%"

        Welch Two Sample t-test

data:  a[, pattern, 1, kernel] and a[, pattern, 2, kernel]
t = -0.50612, df = 14.14, p-value = 0.6206
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -30329.80  18739.71
sample estimates:
mean of x mean of y
 519599.7  525394.8

[1] "kernel1 pattern1: no significance"

        Welch Two Sample t-test

data:  a[, pattern, 1, kernel] and a[, pattern, 2, kernel]
t = -2.8503, df = 9.1116, p-value = 0.01885
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -106104.69  -12303.49
sample estimates:
mean of x mean of y
 450033.2  509237.3

[1] "kernel1 pattern2: [2.73, 23.58]%"

        Welch Two Sample t-test

data:  a[, pattern, 1, kernel] and a[, pattern, 2, kernel]
t = -4.4308, df = 16.918, p-value = 0.0003701
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -73947.84 -26229.02
sample estimates:
mean of x mean of y
 481182.4  531270.8

[1] "kernel1 pattern3: [5.45, 15.37]%"

        Welch Two Sample t-test

data:  a[, pattern, 1, kernel] and a[, pattern, 2, kernel]
t = 23.374, df = 9.5538, p-value = 9.402e-10
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 124194.1 150551.0
sample estimates:
mean of x mean of y
 664543.2  527170.6

[1] "kernel2 pattern1: [-22.65, -18.69]%"

        Welch Two Sample t-test

data:  a[, pattern, 1, kernel] and a[, pattern, 2, kernel]
t = 1.96, df = 17.806, p-value = 0.06583
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  -379.4756 10816.1516
sample estimates:
mean of x mean of y
 590360.7  585142.4

[1] "kernel2 pattern2: no significance"

        Welch Two Sample t-test

data:  a[, pattern, 1, kernel] and a[, pattern, 2, kernel]
t = -33.687, df = 13.354, p-value = 2.614e-14
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -73394.70 -64570.58
sample estimates:
mean of x mean of y
 605358.7  674341.4

[1] "kernel2 pattern3: [10.67, 12.12]%"


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id D4274C433EF
	for <linux-kernel@archiver.kernel.org>; Tue,  9 Nov 2021 02:13:52 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id BB80161207
	for <linux-kernel@archiver.kernel.org>; Tue,  9 Nov 2021 02:13:52 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S238704AbhKICQg (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 8 Nov 2021 21:16:36 -0500
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:60270 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S234245AbhKICQf (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 8 Nov 2021 21:16:35 -0500
Received: from mail-qt1-x835.google.com (mail-qt1-x835.google.com [IPv6:2607:f8b0:4864:20::835])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id E81A7C061570
        for <linux-kernel@vger.kernel.org>; Mon,  8 Nov 2021 18:13:49 -0800 (PST)
Received: by mail-qt1-x835.google.com with SMTP id j17so3530316qtx.2
        for <linux-kernel@vger.kernel.org>; Mon, 08 Nov 2021 18:13:49 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=edi.works; s=google;
        h=from:to:cc:subject:date:message-id:in-reply-to:references;
        bh=+atqJCfHU621dS9WyrBXBvV6FFQ5mGU2jRsU3bUDRbI=;
        b=ZCnZFR/0yNZQJDtqVx+GvPJoZWER4UJ/1s82yeY1haNcXXcxJCo3AdNBMpNPyHv/Om
         gmomaUUDytulhbnpdPrdPVQZWDBn3ZQU8CblDSq4Oay+oWlXfY427cIu4eK6FrJeNKU6
         lnQUnAe+zSnbjcj4V5zoLXhiy69gYJRA3+uzgh1L39m3VZe9MEEjz07HiS3YyZdke/ea
         FQCH9bN0KdwM0wADDg/82XQvtI/FmCscjlsymZxXdoHaWZYW8uUCMRZM60qrE7/PusYU
         eCHRMETOaQ9VgB6Q+0AVFR7mqFWw0d4R5u5X0nX2yQIUxTEwZyxlj/geI49djRhN4aYj
         KywA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references;
        bh=+atqJCfHU621dS9WyrBXBvV6FFQ5mGU2jRsU3bUDRbI=;
        b=ZCqc+0DyBKtsWjxIgCwDAmaxV14oJX9l9QPsVLjgIwyuwJ7saRPC8UAevjq6uNnyWm
         metT7Td3q9dYoXldv0hKeFEoO8COEeP96kgt8qzPPGeQ/xDM6qERsnVcXzkKcsYxGJhy
         od/aWVuYb5hWJmmtctYZrd1ELNkt9sfL72MQi0wS/LpYrEQLvT1mA0ApKEF3lWMN8xHw
         AdiwaJPD2So9oBsf9k2y/YunOAdo+kePs6A38oZ/lWIVsv3hBfmiZcTqidOgJXxqxGKs
         qo1hE4H5OUYjq9HXImm5A/aTfcYWDc3TTrNFP7Y+8DDqChx2kS5s2xAgtnwdSzOuCED3
         4kVg==
X-Gm-Message-State: AOAM5328ovDvM+tKBEDnniabK+h00klvbQqs9Z43Glkz1BIpXmXk8u8g
        yyw3uWXRz2k5CKL5vt/fbIZ+ZA==
X-Google-Smtp-Source: ABdhPJzGqHA3jMlRw3rtoISwQKkM23f9iFna14Bp2MItm/4vyiNHY++SkmdW2C5c/kQwANNnx7QXPA==
X-Received: by 2002:ac8:7f03:: with SMTP id f3mr4556893qtk.320.1636424029022;
        Mon, 08 Nov 2021 18:13:49 -0800 (PST)
Received: from localhost.localdomain (c-67-169-44-201.hsd1.ca.comcast.net. [67.169.44.201])
        by smtp.gmail.com with ESMTPSA id az14sm10602470qkb.125.2021.11.08.18.13.47
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 08 Nov 2021 18:13:48 -0800 (PST)
From:   bot@edi.works
To:     yuzhao@google.com
Cc:     hdanton@sina.com, linux-kernel@vger.kernel.org, linux-mm@kvack.org,
        page-reclaim@google.com, corbet@lwn.net,
        michael@michaellarabel.com, sofia.trinh@edi.works
Subject: Re: [PATCH v4 00/11] Multigenerational LRU Framework
Date:   Mon,  8 Nov 2021 18:13:46 -0800
Message-Id: <20211109021346.50266-1-bot@edi.works>
X-Mailer: git-send-email 2.18.0
In-Reply-To: <20210818063107.2696454-1-yuzhao@google.com>
References: <20210818063107.2696454-1-yuzhao@google.com>
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Kernel / MongoDB benchmark with MGLRU

TLDR
====
With the MGLRU, MongoDB achieved 95% CIs [2.23, 3.44]%, [6.97, 9.73]%
and [2.16, 3.55]% more operations per second (OPS) respectively for
exponential (distribution) access, random access and Zipfian access,
when underutizling memory; 95% CIs [8.83, 10.03]%, [21.12, 23.14]%
and [5.53, 6.46]% more OPS respectively for exponential access,
random access and Zipfian access, when slightly overcommitting memory.

Background
==========
Memory overcommit can increase utilization and, if carried out
properly, can also increase throughput. The challenges are to improve
working set estimation and to optimize page reclaim. The risks are
performance degradation and OOM kills. Short of overcoming the
challenges, the only way to reduce the risks is to underutilize
memory.

MongoDB is one of the most popular open-source NoSQL databases. YCSB
is the leading open-source NoSQL database benchmarking software that
supports multiple access distributions.

Matrix
======
Kernels: version [+ patchset]
* Baseline: 5.14
* Patched: 5.14 + MGLRU

Memory utilization: % of memory size
* Underutilizing: ~15% on inactive file list
* Overcommitting: ~5% swapped out

Concurrency: average # of users per CPU
* Medium: 2

Access distributions (1kB objects, 20% update):
* Exponential
* Uniform random
* Zipfian

Total configurations: 12
Data points per configuration: 10
Total run duration (minutes) per data point: ~20

Note that MongoDB reached the peak performance with the concurrency
for this benchmark, i.e., its performance degraded with fewer or more
users for this benchmark.

Procedure
=========
The latest MGLRU patchset for the 5.14 kernel is available at
git fetch https://linux-mm.googlesource.com/page-reclaim \
    refs/changes/30/1430/1

Baseline and patched 5.14 kernel images are available at
https://drive.google.com/drive/folders/1eMkQleAFGkP2vzM_JyRA21oKE0ESHBqp

<install and configure OS>
ycsb_load.sh
systemctl stop mongod
e2image <backup /mnt/data>

<for each kernel>
    grub2-set-default <baseline, patched>
    <for each memory utilization>
        <update /etc/mongod.conf>
        <for each access distribution>
            <update ycsb_run.sh>
            <for each data point>
                systemctl stop mongod
                e2image <restore /mnt/data>
                reboot
                ycsb_run.sh
                <collect OPS>

Hardware
========
Memory (GB): 64
CPU (total #): 32
NVMe SSD (GB): 1024

OS
==
$ cat /etc/redhat-release
Red Hat Enterprise Linux release 8.4 (Ootpa)

$ cat /proc/swaps
Filename          Type          Size          Used     Priority
/dev/nvme0n1p3    partition     32970748      0        -2

$ cat /proc/cmdline
<existing parameters> systemd.unified_cgroup_hierarchy=1

$ cat /sys/fs/cgroup/user.slice/memory.min
4294967296

$ cat /proc/sys/vm/overcommit_memory
1

$ cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]

MongoDB
=======
$ mongod --version
db version v5.0.3
Build Info: {
    "version": "5.0.3",
    "gitVersion": "657fea5a61a74d7a79df7aff8e4bcf0bc742b748",
    "openSSLVersion": "OpenSSL 1.1.1g FIPS  21 Apr 2020",
    "modules": [],
    "allocator": "tcmalloc",
    "environment": {
        "distmod": "rhel80",
        "distarch": "x86_64",
        "target_arch": "x86_64"
    }
}

$ cat /etc/mongod.conf
# mongod.conf
<existing parameters>

# Where and how to store data.
storage:
  dbPath: /mnt/data
  journal:
    enabled: true
  wiredTiger:
    engineConfig:
      cacheSizeGB: <50, 60>

<existing parameters>

YCSB
====
$ git log
commit ce3eb9ce51c84ee9e236998cdd2cefaeb96798a8 (HEAD -> master,
origin/master, origin/HEAD)
Author: Ivan <john.koepi@gmail.com>
Date:   Tue Feb 16 17:38:00 2021 +0200

    [scylla] enable token aware LB by default, improve the docs (#1507)

$ cat ycsb_load.sh
# load objects
ycsb load mongodb -s -threads 16 \
    -p mongodb.url=mongodb://%2Ftmp%2Fmongodb-27017.sock \
    -p workload=site.ycsb.workloads.CoreWorkload \
    -p recordcount=80000000

$ cat ycsb_run.sh
# run benchmark
ycsb run mongodb -s -threads 64 \
    -p mongodb.url=mongodb://%2Ftmp%2Fmongodb-27017.sock \
    -p workload=site.ycsb.workloads.CoreWorkload \
    -p recordcount=80000000 -p operationcount=80000000 \
    -p readproportion=0.8 -p updateproportion=0.2 \
    -p requestdistribution=<exponential, uniform, zipfian>

Results
=======
Comparing the patched with the baseline kernel, MongoDB achieved 95%
CIs [2.23, 3.44]%, [6.97, 9.73]% and [2.16, 3.55]% more OPS
respectively for exponential access, random access and Zipfian
access, when underutizling memory; 95% CIs [8.83, 10.03]%, [21.12,
23.14]% and [5.53, 6.46]% more OPS respectively for exponential
access, random access and Zipfian access, when slightly
overcommitting memory.

+--------------------+-----------------------+-----------------------+
| Mean OPS [95% CI]  | Underutilizing memory | Overcommitting memory |
+--------------------+-----------------------+-----------------------+
| Exponential access | 76615.56 / 78788.76   | 73984.90 / 80961.66   |
|                    | [1708.76, 2637.62]    | [6533.94, 7419.58]    |
+--------------------+-----------------------+-----------------------+
| Random access      | 62093.40 / 67276.01   | 55990.56 / 68379.91   |
|                    | [4324.96, 6040.25]    | [11824.09, 12954.62]  |
+--------------------+-----------------------+-----------------------+
| Zipfian access     | 92532.25 / 95174.43   | 93545.62 / 99151.12   |
|                    | [1997.20, 3287.17]    | [5171.27, 6039.72]    |
+--------------------+-----------------------+-----------------------+
Table 1. Comparison between the baseline and patched kernels

Comparing overcommitting with underutilizing memory, MongoDB achieved
95% CIs [-4.10, -2.77]%, [-11.20, -8.46]% and [0.36, 1.83]% more OPS
respectively for exponential access, random access and Zipfian
access, when using the baseline kernel; 95% CIs [2.27, 3.25]%, [0.78,
2.50]% and [3.81, 4.54]% more OPS respectively for exponential
access, random access and Zipfian access, when using the patched
kernel.

+--------------------+-----------------------+-----------------------+
| Mean OPS [95% CI]  | Baseline kernel       |  Patched kernel       |
+--------------------+-----------------------+-----------------------+
| Exponential access | 76615.56 / 73984.90   | 78788.76 / 80961.66   |
|                    | [-3139.12, -2122.20]  | [1786.70, 2559.09]    |
+--------------------+-----------------------+-----------------------+
| Random access      | 62093.40 / 55990.56   | 67276.01 / 68379.91   |
|                    | [-6953.44, -5252.23]  | [525.42, 1682.38]     |
+--------------------+-----------------------+-----------------------+
| Zipfian access     | 92532.25 / 93545.62   | 95174.43 / 99151.12   |
|                    | [330.99, 1695.75]     | [3628.31, 4325.06]    |
+--------------------+-----------------------+-----------------------+
Table 2. Comparison between underutilizing and overcommitting memory

Metrics collected during each run are available at
https://github.com/ediworks/KernelPerf/tree/master/mglru/mongodb/5.14

Appendix
========
$ cat raw_data_mongodb.r
v <- c(
    # baseline 50g exp
    75814.86, 75884.91, 76052.71, 76621.01, 76641.19, 76661.24, 76870.15, 77017.79, 77289.08, 77302.67,
    # baseline 50g uni
    60638.17, 60968.91, 61128.61, 61548.40, 61779.30, 61917.58, 62152.28, 63440.15, 63625.47, 63735.11,
    # baseline 50g zip
    91271.16, 91482.41, 91524.17, 92467.16, 92585.62, 92843.29, 92885.65, 93229.98, 93408.94, 93624.08,
    # baseline 60g exp
    73183.67, 73191.30, 73527.58, 73831.79, 74047.95, 74056.24, 74401.23, 74418.53, 74547.58, 74643.08,
    # baseline 60g uni
    55175.76, 55477.42, 55605.52, 55680.21, 55903.39, 56171.05, 56375.06, 56380.43, 56509.94, 56626.78,
    # baseline 60g zip
    92653.82, 92775.02, 93100.44, 93290.21, 93593.74, 93775.64, 93868.72, 93915.12, 94194.77, 94288.69,
    # patched 50g exp
    78349.95, 78385.64, 78392.33, 78419.91, 78726.59, 78738.68, 78930.72, 78948.25, 79404.38, 79591.14,
    # patched 50g uni
    66622.91, 66667.33, 66951.43, 67104.80, 67117.30, 67196.90, 67389.75, 67406.62, 68131.43, 68171.61,
    # patched 50g zip
    94261.14, 94822.34, 94914.70, 95114.89, 95156.75, 95205.90, 95383.78, 95612.00, 95624.00, 95648.81,
    # patched 60g exp
    80272.04, 80612.33, 80679.23, 80717.74, 81011.18, 81029.64, 81146.68, 81371.84, 81379.13, 81396.76,
    # patched 60g uni
    67559.52, 67600.11, 67718.90, 68062.57, 68278.78, 68446.56, 68452.82, 68853.86, 69278.34, 69547.67,
    # patched 60g zip
    98706.81, 98864.41, 98903.77, 99044.10, 99155.68, 99162.94, 99165.64, 99482.31, 99484.91, 99540.62
)

a <- array(v, dim = c(10, 3, 2, 2))

# baseline vs patched
for (mem in 1:2) {
    for (dist in 1:3) {
        r <- t.test(a[, dist, mem, 1], a[, dist, mem, 2])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("mem%d dist%d: no significance", mem, dist)
        } else {
            s <- sprintf("mem%d dist%d: [%.2f, %.2f]%%", mem, dist, -p[2], -p[1])
        }
        print(s)
    }
}

# 50g vs 60g
for (kern in 1:2) {
    for (dist in 1:3) {
        r <- t.test(a[, dist, 1, kern], a[, dist, 2, kern])
        print(r)

p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("kern%d dist%d: no significance", kern, dist)
        } else {
            s <- sprintf("kern%d dist%d: [%.2f, %.2f]%%", kern, dist, -p[2], -p[1])
        }
        print(s)
    }
}

$ R -q -s -f raw_data_mongodb.r

        Welch Two Sample t-test

data:  a[, dist, mem, 1] and a[, dist, mem, 2]
t = -9.8624, df = 17.23, p-value = 1.671e-08
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2637.627 -1708.769
sample estimates:
mean of x mean of y
 76615.56  78788.76

[1] "mem1 dist1: [2.23, 3.44]%"

        Welch Two Sample t-test

data:  a[, dist, mem, 1] and a[, dist, mem, 2]
t = -13.081, df = 12.744, p-value = 9.287e-09
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -6040.256 -4324.964
sample estimates:
mean of x mean of y
 62093.40  67276.01

[1] "mem1 dist2: [6.97, 9.73]%"

        Welch Two Sample t-test

data:  a[, dist, mem, 1] and a[, dist, mem, 2]
t = -8.8194, df = 13.459, p-value = 5.833e-07
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -3287.17 -1997.20
sample estimates:
mean of x mean of y
 92532.25  95174.43

[1] "mem1 dist3: [2.16, 3.55]%"

        Welch Two Sample t-test

data:  a[, dist, mem, 1] and a[, dist, mem, 2]
t = -33.368, df = 16.192, p-value = 2.329e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -7419.582 -6533.942
sample estimates:
mean of x mean of y
 73984.90  80961.66

[1] "mem2 dist1: [8.83, 10.03]%"

        Welch Two Sample t-test

data:  a[, dist, mem, 1] and a[, dist, mem, 2]
t = -46.386, df = 16.338, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -12954.62 -11824.09
sample estimates:
mean of x mean of y
 55990.56  68379.91

[1] "mem2 dist2: [21.12, 23.14]%"

        Welch Two Sample t-test

data:  a[, dist, mem, 1] and a[, dist, mem, 2]
t = -27.844, df = 13.209, p-value = 4.049e-13
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -6039.729 -5171.275
sample estimates:
mean of x mean of y
 93545.62  99151.12

[1] "mem2 dist3: [5.53, 6.46]%"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 10.87, df = 18, p-value = 2.439e-09
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 2122.207 3139.125
sample estimates:
mean of x mean of y
 76615.56  73984.90

[1] "kern1 dist1: [-4.10, -2.77]%"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 15.593, df = 12.276, p-value = 1.847e-09
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 5252.237 6953.447
sample estimates:
mean of x mean of y
 62093.40  55990.56

[1] "kern1 dist2: [-11.20, -8.46]%"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = -3.1512, df = 15.811, p-value = 0.006252
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -1695.7509  -330.9911
sample estimates:
mean of x mean of y
 92532.25  93545.62

[1] "kern1 dist3: [0.36, 1.83]%"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = -11.836, df = 17.672, p-value = 7.84e-10
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2559.092 -1786.704
sample estimates:
mean of x mean of y
 78788.76  80961.66

[1] "kern2 dist1: [2.27, 3.25]%"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = -4.0276, df = 16.921, p-value = 0.0008807
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -1682.3864  -525.4236
sample estimates:
mean of x mean of y
 67276.01  68379.91

[1] "kern2 dist2: [0.78, 2.50]%"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = -24.26, df = 15.517, p-value = 9.257e-14
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -4325.062 -3628.314
sample estimates:
mean of x mean of y
 95174.43  99151.12

[1] "kern2 dist3: [3.81, 4.54]%"


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 78B3AC433FE
	for <linux-kernel@archiver.kernel.org>; Thu,  2 Dec 2021 06:28:22 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1345093AbhLBGbk (ORCPT
        <rfc822;linux-kernel@archiver.kernel.org>);
        Thu, 2 Dec 2021 01:31:40 -0500
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:39338 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1355811AbhLBGbc (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 2 Dec 2021 01:31:32 -0500
Received: from mail-qt1-x832.google.com (mail-qt1-x832.google.com [IPv6:2607:f8b0:4864:20::832])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id 6ED70C061574
        for <linux-kernel@vger.kernel.org>; Wed,  1 Dec 2021 22:28:10 -0800 (PST)
Received: by mail-qt1-x832.google.com with SMTP id p19so26527416qtw.12
        for <linux-kernel@vger.kernel.org>; Wed, 01 Dec 2021 22:28:10 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=edi.works; s=google;
        h=from:to:cc:subject:date:message-id:in-reply-to:references;
        bh=tEGocQ62hfwK6Gu+tfXjRHtRf5LF3zuLTHi8XMt04nI=;
        b=egTpWtfp0SlDeNQfYXNIF/sz/hm9yIgvOq3ak4DqC9hQjt2e0KVpzWOumLM64pgFkb
         ynToEFexId5xQ+YbHls1LlszRALQxnpntAM2Chs1J9PGx8mtJtWtQpIx+qI933VdtjFu
         c3AUkr75E+ZunYLd4DJhfI87LiIMBwO33L/BiWjaN0IYdRJfMjpD+LRfu5R6qCounkBr
         waUsZ14ac6RFO0dChtHu4r64IVglwygEJDv96gH+YK11OUpqVEi8dpKnJEKZrlDZkxMm
         YKfkasjp0I+6goy/tyBNV9PaVflu21JIN4Tc3CNL1j4Il3con+iu6M/7vZV58O5qt4/V
         Dh2w==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references;
        bh=tEGocQ62hfwK6Gu+tfXjRHtRf5LF3zuLTHi8XMt04nI=;
        b=024Mo2HoqFlsZo4Xg06p3EYkDKm/ADCK21id47yWmLbmNnGQemjWbBW0ZVYonJMT40
         MJ/63i2ZKtfQjSFuD9fEUiKrUYYHi0uCqYDmHjhEzh4+uoZGds2GjRMrYbTkj9x6/1/g
         /h5xbT6ZGKHQoRLKDuDD9S+gEcfjBSu54YIaErgvzaUFRMYt12IFEO7wf63wO28UOfIF
         aCcy+jueO5gBZxJQdB4g9WdnVbBnRsQ8QPGQL7C1XZLGXzlT6INeCIZDTUrDU/pQ7Our
         hqo8acMspkg4vX1WiGvGD1AQhhteI7hGHQJktPu4UjhcssnnMA1Ryy4xqaJfVOGP5fl2
         u3oA==
X-Gm-Message-State: AOAM5314Cw8my3n/XVRCw/S26pdOTHhU4E7YV4/R+puYSYNqAJOsHePu
        KO7Bo97oblGo/MDrjCKOZoIuMw==
X-Google-Smtp-Source: ABdhPJzkHOvKxuieF3eK/80llO2ki9JIw/SCWjKwybaECkUsS1OnWMfBcN2OeB78J7LDJKkQLTNW3w==
X-Received: by 2002:a05:622a:388:: with SMTP id j8mr11965266qtx.366.1638426489474;
        Wed, 01 Dec 2021 22:28:09 -0800 (PST)
Received: from localhost.localdomain (c-67-169-44-201.hsd1.ca.comcast.net. [67.169.44.201])
        by smtp.gmail.com with ESMTPSA id 15sm1102145qtp.55.2021.12.01.22.28.08
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 01 Dec 2021 22:28:09 -0800 (PST)
From:   bot@edi.works
To:     yuzhao@google.com
Cc:     linux-kernel@vger.kernel.org, linux-mm@kvack.org,
        page-reclaim@google.com, corbet@lwn.net,
        michael@michaellarabel.com, sofia.trinh@edi.works
Subject: Re: [PATCH v5 00/10] Multigenerational LRU Framework
Date:   Wed,  1 Dec 2021 22:28:06 -0800
Message-Id: <20211202062806.80365-1-bot@edi.works>
X-Mailer: git-send-email 2.18.0
In-Reply-To: <20211111041510.402534-1-yuzhao@google.com>
References: <20211111041510.402534-1-yuzhao@google.com>
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Kernel / Apache Cassandra benchmark with MGLRU

TLDR
====
With the MGLRU, Apache Cassandra achieved 95% CIs [1.06, 4.10]%,
[1.94, 5.43]% and [4.11, 7.50]% more operations per second (OPS),
respectively, for exponential (distribution) access, random access
and Zipfian access, when swap was off; 95% CIs [0.50, 2.60]%, [6.51,
8.77]% and [3.29, 6.75]% more OPS, respectively, for exponential
access, random access and Zipfian access, when swap was set to
minimum (vm.swappiness=1).

Background
==========
Memory overcommit can increase utilization and, if carried out
properly, can also increase throughput. The challenges are to improve
working set estimation and to optimize page reclaim. The risks are
performance degradation and OOM kills. Short of overcoming the
challenges, the only way to reduce the risks is to underutilize
memory.

Apache Cassandra is one of the most popular open-source NoSQL
databases. YCSB is the leading open-source NoSQL database
benchmarking software that supports multiple access distributions.
Swap can have a negative effect, as Apache Cassandra cautions "Do
never allow your system to swap" [1].

[1]: https://github.com/apache/cassandra/blob/trunk/conf/cassandra.yaml#L394

Matrix
======
Kernels: version [+ patchset]
* Baseline: 5.15
* Patched: 5.15 + MGLRU

Swap configurations:
* Off
* Minimum (vm.swappiness=1)

Concurrency: average # of users per CPU
* Medium: 3

Access distributions (2kB objects, 10% update):
* Exponential
* Uniform random
* Zipfian

Total configurations: 12
Data points per configuration: 10
Total run duration (minutes) per data point: ~40

Note that Apache Cassandra reached the peak performance for this
benchmark with 2-3 users per CPU, i.e., its performance started
degrading with fewer or more users.

Procedure
=========
The latest MGLRU patchset for the 5.15 kernel is available at
git fetch https://linux-mm.googlesource.com/page-reclaim \
    refs/changes/30/1430/2

Baseline and patched 5.15 kernel images are available at
https://drive.google.com/drive/folders/1eMkQleAFGkP2vzM_JyRA21oKE0ESHBqp

<install and configure OS>
ycsb_load.sh
systemctl stop cassandra
e2image <backup /mnt/data>

<for each kernel>
    grub-set-default <baseline, patched>
    <for each swap configuration>
        <swapoff, swapon>
        <for each access distribution>
            <update ycsb_run.sh>
            <for each data point>
                systemctl stop cassandra
                e2image <restore /mnt/data>
                reboot
                ycsb_run.sh
                <collect OPS>

Hardware
========
Memory (GB): 256
CPU (total #): 48
NVMe SSD (GB): 1024

OS
==
$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=21.10
DISTRIB_CODENAME=impish
DISTRIB_DESCRIPTION="Ubuntu 21.10"

$ cat /proc/swaps
Filename          Type          Size          Used     Priority
/dev/nvme0n1p3    partition     32970748      0        -2

$ cat /sys/fs/cgroup/user.slice/memory.min
4294967296

$ cat /proc/sys/vm/overcommit_memory
1

$ cat /proc/sys/vm/swappiness
1

$ cat /proc/sys/vm/max_map_count
1048575

Apache Cassandra
================
$ nodetool version
ReleaseVersion: 4.0.1

$ cat jvm8-server.options
<existing parameters>

#-XX:+UseParNewGC
#-XX:+UseConcMarkSweepGC
#-XX:+CMSParallelRemarkEnabled
#-XX:SurvivorRatio=8
#-XX:MaxTenuringThreshold=1
#-XX:CMSInitiatingOccupancyFraction=75
#-XX:+UseCMSInitiatingOccupancyOnly
#-XX:CMSWaitDuration=10000
#-XX:+CMSParallelInitialMarkEnabled
#-XX:+CMSEdenChunksRecordAlways
#-XX:+CMSClassUnloadingEnabled

-XX:+UseG1GC
-XX:+ParallelRefProcEnabled
-XX:MaxGCPauseMillis=400

<existing parameters>

$ cat cassandra.yaml
<existing parameters>

data_file_directories: /mnt/data/
key_cache_size_in_mb: 5000
file_cache_enabled: true
file_cache_size_in_mb: 10000
buffer_pool_use_heap_if_exhausted: false
memtable_offheap_space_in_mb: 10000
memtable_allocation_type: offheap_buffers

<existing parameters>

YCSB
====
$ git log
commit ce3eb9ce51c84ee9e236998cdd2cefaeb96798a8 (HEAD -> master,
origin/master, origin/HEAD)
Author: Ivan <john.koepi@gmail.com>
Date:   Tue Feb 16 17:38:00 2021 +0200

    [scylla] enable token aware LB by default, improve the docs (#1507)

$ cat ycsb_load.sh
# load objects
cqlsh -e "create keyspace ycsb WITH REPLICATION = {'class' : \
    'SimpleStrategy', 'replication_factor': 1};"
cqlsh -k ycsb -e "create table usertable (y_id varchar primary key, \
    field0 varchar, field1 varchar, field2 varchar, field3 varchar, \
    field4 varchar, field5 varchar ,field6 varchar, field7 varchar, \
    field8 varchar, field9 varchar);"
ycsb load cassandra-cql -s -threads 24 -p hosts=localhost \
    -p workload=site.ycsb.workloads.CoreWorkload -p fieldlength=200 \
    -p recordcount=130000000

$ cat ycsb_run.sh
# run benchmark
ycsb run cassandra-cql -s -threads 144 -p hosts=localhost \
    -p workload=site.ycsb.workloads.CoreWorkload \
    -p recordcount=130000000 -p operationcount=130000000 \
    -p readproportion=0.9 -p updateproportion=0.1 \
    -p maxexecutiontime=1800 \
    -p requestdistribution=<exponential, uniform, zipfian>

Results
=======
Comparing the patched with the baseline kernel, Apache Cassandra
achieved 95% CIs [1.06, 4.10]%, [1.94, 5.43]% and [4.11, 7.50]% more
OPS, respectively, for exponential access, random access and Zipfian
access, when swap was off; 95% CIs [0.50, 2.60]%, [6.51, 8.77]% and
[3.29, 6.75]% more OPS, respectively, for exponential access, random
access and Zipfian access, when swap was set to minimum
(vm.swappiness=1).

+--------------------+--------------------+---------------------+
| Mean OPS [95% CI]  | No swap            | Minimum swap        |
+--------------------+--------------------+---------------------+
| Exponential access | 71084.9 / 72917.5  | 71499.6 / 72607.9   |
|                    | [751.42, 2913.77]  | [358.40, 1858.19]   |
+--------------------+--------------------+---------------------+
| Random access      | 47127.2 / 48862.8  | 47585.4 / 51220.1   |
|                    | [912.68, 2558.51]  | [3097.39, 4172.00]  |
+--------------------+--------------------+---------------------+
| Zipfian access     | 70271.5 / 74348.8  | 70698.2 / 74248.3   |
|                    | [2887.20, 5267.39] | [2326.69, 4773.50]  |
+--------------------+--------------------+---------------------+
Table 1. Comparison between the baseline and the patched kernels

Comparing minimum swap with no swap, Apache Cassandra achieved 95%
CIs [4.05, 5.60]% more OPS for random access, when using the patched
kernel. There were no statistically significant changes in OPS under
other conditions.

+--------------------+--------------------+---------------------+
| Mean OPS [95% CI]  | Baseline kernel    |  Patched kernel     |
+--------------------+--------------------+---------------------+
| Exponential access | 71084.9 / 71499.6  | 72917.5 / 72607.9   |
|                    | [-358.97, 1188.37] | [-1376.93, 757.73]  |
+--------------------+--------------------+---------------------+
| Random access      | 47127.2 / 47585.4  | 48862.8 / 51220.1   |
|                    | [-424.55, 1340.95] | [1977.09, 2737.50]  |
+--------------------+--------------------+---------------------+
| Zipfian access     | 70271.5 / 70698.2  | 74348.8 / 74248.3   |
|                    | [-749.39, 1602.79] | [-1337.07, 1136.07] |
+--------------------+--------------------+---------------------+
Table 2. Comparison between no swap and minimum swap

Metrics collected during each run are available at
https://github.com/ediworks/KernelPerf/tree/master/mglru/cassandra/5.15

Appendix
========
$ cat raw_data_cassandra.r
v <- c(
    # baseline swapoff exp
    69952, 70274, 70286, 70818, 70946, 71202, 71244, 71615, 71787, 72725,
    # baseline swapoff uni
    45309, 46056, 46086, 46188, 47275, 47524, 47797, 48243, 48329, 48465,
    # baseline swapoff zip
    69096, 69194, 69386, 69408, 69412, 70795, 70890, 71170, 71232, 72132,
    # baseline swapon exp
    69836, 70783, 70951, 71188, 71521, 71764, 72035, 72166, 72287, 72465,
    # baseline swapon uni
    46089, 46963, 47308, 47599, 47776, 47822, 47952, 48042, 48092, 48211,
    # baseline swapon zip
    68986, 69279, 69290, 69805, 70146, 70913, 71462, 71978, 72370, 72753,
    # patched swapoff exp
    70701, 71328, 71458, 72846, 72885, 73078, 73702, 74077, 74415, 74685,
    # patched swapoff uni
    48275, 48460, 48735, 48813, 48902, 48969, 48996, 49007, 49213, 49258,
    # patched swapoff zip
    71829, 72909, 73259, 73835, 74200, 74544, 75318, 75514, 76031, 76049,
    # patched swapon exp
    71169, 71968, 72208, 72374, 72401, 72755, 72861, 72942, 73469, 73932,
    # patched swapon uni
    50292, 50529, 50981, 51224, 51414, 51420, 51480, 51608, 51625, 51628,
    # patched swapon zip
    72032, 72325, 73834, 74366, 74482, 74573, 74810, 75044, 75371, 75646
)

a <- array(v, dim = c(10, 3, 2, 2))

# baseline vs patched
for (swap in 1:2) {
    for (dist in 1:3) {
        r <- t.test(a[, dist, swap, 1], a[, dist, swap, 2])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("swap%d dist%d: no significance", swap, dist)
        } else {
            s <- sprintf("swap%d dist%d: [%.2f, %.2f]%%", swap, dist, -p[2], -p[1])
        }
        print(s)
    }
}

# swapoff vs swapon
for (kern in 1:2) {
    for (dist in 1:3) {
        r <- t.test(a[, dist, 1, kern], a[, dist, 2, kern])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("kern%d dist%d: no significance", kern, dist)
        } else {
            s <- sprintf("kern%d dist%d: [%.2f, %.2f]%%", kern, dist, -p[2], -p[1])
        }
        print(s)
    }
}

$ R -q -s -f raw_data_cassandra.r

        Welch Two Sample t-test

data:  a[, dist, swap, 1] and a[, dist, swap, 2]
t = -3.6172, df = 14.793, p-value = 0.002585
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2913.7703  -751.4297
sample estimates:
mean of x mean of y
  71084.9   72917.5

[1] "swap1 dist1: [1.06, 4.10]%"

        Welch Two Sample t-test

data:  a[, dist, swap, 1] and a[, dist, swap, 2]
t = -4.679, df = 10.331, p-value = 0.0007961
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2558.5199  -912.6801
sample estimates:
mean of x mean of y
  47127.2   48862.8

[1] "swap1 dist2: [1.94, 5.43]%"

        Welch Two Sample t-test

data:  a[, dist, swap, 1] and a[, dist, swap, 2]
t = -7.2315, df = 16.902, p-value = 1.452e-06
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -5267.396 -2887.204
sample estimates:
mean of x mean of y
  70271.5   74348.8

[1] "swap1 dist3: [4.11, 7.50]%"

        Welch Two Sample t-test

data:  a[, dist, swap, 1] and a[, dist, swap, 2]
t = -3.1057, df = 17.95, p-value = 0.006118
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -1858.191  -358.409
sample estimates:
mean of x mean of y
  71499.6   72607.9

[1] "swap2 dist1: [0.50, 2.60]%"

        Welch Two Sample t-test

data:  a[, dist, swap, 1] and a[, dist, swap, 2]
t = -14.307, df = 16.479, p-value = 1.022e-10
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -4172.006 -3097.394
sample estimates:
mean of x mean of y
  47585.4   51220.1

[1] "swap2 dist2: [6.51, 8.77]%"

        Welch Two Sample t-test

data:  a[, dist, swap, 1] and a[, dist, swap, 2]
t = -6.1048, df = 17.664, p-value = 9.877e-06
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -4773.504 -2326.696
sample estimates:
mean of x mean of y
  70698.2   74248.3

[1] "swap2 dist3: [3.29, 6.75]%"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = -1.1261, df = 17.998, p-value = 0.2749
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -1188.3785   358.9785
sample estimates:
mean of x mean of y
  71084.9   71499.6

[1] "kern1 dist1: no significance"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = -1.1108, df = 14.338, p-value = 0.2849
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -1340.9555   424.5555
sample estimates:
mean of x mean of y
  47127.2   47585.4

[1] "kern1 dist2: no significance"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = -0.76534, df = 17.035, p-value = 0.4545
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -1602.7926   749.3926
sample estimates:
mean of x mean of y
  70271.5   70698.2

[1] "kern1 dist3: no significance"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 0.62117, df = 14.235, p-value = 0.5443
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -757.7355 1376.9355
sample estimates:
mean of x mean of y
  72917.5   72607.9

[1] "kern2 dist1: no significance"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = -13.18, df = 15.466, p-value = 8.07e-10
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2737.509 -1977.091
sample estimates:
mean of x mean of y
  48862.8   51220.1

[1] "kern2 dist2: [4.05, 5.60]%"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 0.17104, df = 17.575, p-value = 0.8661
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -1136.076  1337.076
sample estimates:
mean of x mean of y
  74348.8   74248.3

[1] "kern2 dist3: no significance"


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 0AEC6C433EF
	for <linux-kernel@archiver.kernel.org>; Thu,  9 Dec 2021 07:24:24 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S233803AbhLIH14 (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Thu, 9 Dec 2021 02:27:56 -0500
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:47380 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S229942AbhLIH1y (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 9 Dec 2021 02:27:54 -0500
Received: from mail-qt1-x82a.google.com (mail-qt1-x82a.google.com [IPv6:2607:f8b0:4864:20::82a])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id 1CC59C061746
        for <linux-kernel@vger.kernel.org>; Wed,  8 Dec 2021 23:24:21 -0800 (PST)
Received: by mail-qt1-x82a.google.com with SMTP id f20so4521189qtb.4
        for <linux-kernel@vger.kernel.org>; Wed, 08 Dec 2021 23:24:21 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=edi.works; s=google;
        h=from:to:cc:subject:date:message-id:in-reply-to:references;
        bh=NBh2FtOhN9V4AaYJCGgnULr5RlKifG8K4kl2lNElkqE=;
        b=dIoU0XEyugtg1Fe2bPpH7wzdzKvXTAXc3KL9Z6npueqZiH0qtF/YtLnsyOlTbdZhSu
         F7bt/nVHNrNfB1O5MkYHm0tiquhJAqwe/SGcU8C/kazHaEN2U2llPdmns270QrS7ebBw
         E4VkAqkKJvS6qsVlXjTh20qHdH8zeumBr2/luRpmpzr5BRBKJuVOUPDWa7/jqys2WekI
         R4NdkdeLyhA2DELWgsWRweTv4bAhEcMgC2fqJSxQtS0aDGKJ9V5B9cgcKt0G6b3CLMFd
         zyoexv1850Dn79utmgbFqSwDdD7vZgV39EtphdiB3qAo3pvbk9/eMS9RZQlKsNo35OE7
         wA9Q==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references;
        bh=NBh2FtOhN9V4AaYJCGgnULr5RlKifG8K4kl2lNElkqE=;
        b=FMNK+tMFeYNyctVSAsyorwwl0ACIDil1hRgZv7jRMxqOGweaqvjQNQJNJq6joJcZep
         b2L76ve6sw8PygsSrb24/N8OzjkIJfGauT8SlRUhT4jRDqRk816Zr9RLl4qkk39IE20g
         noAfr8DzZRRiF/bOTPD/LsJNSaHWjcRZbgTfH15Y1JpXXcYjMJw0XY8gBjyi+YDTsWvI
         O9MdmiyCREC26S184DqfNEG0OCMULiDcVB0D9DHqN8t4W3FRZiskrPLKd3tLk2x8qWDU
         RApANk/20+KDFoXQxVr1qYFT2oPj1eTp6mUdL0eg86DDm5E1kjCobgeOZwdRR+CL0Ppt
         wpuA==
X-Gm-Message-State: AOAM531RSn6GclN7ERZu8z+tcrZ4Rp7qkHaE7O2IbVo2uwuQ+5bqYDhF
        oOUP+pVGQkkS5v8VhsLqLdrlaqmF9HzaAA==
X-Google-Smtp-Source: ABdhPJytuoYkwpk9eT43sFCClb+a55nb+d8+FBlZl6072GJvTh07ntBMbC9woi5lutycMfXsCowCNA==
X-Received: by 2002:ac8:58c9:: with SMTP id u9mr14923345qta.583.1639034660203;
        Wed, 08 Dec 2021 23:24:20 -0800 (PST)
Received: from localhost.localdomain (c-67-169-44-201.hsd1.ca.comcast.net. [67.169.44.201])
        by smtp.gmail.com with ESMTPSA id t11sm3432244qtx.48.2021.12.08.23.24.18
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 08 Dec 2021 23:24:19 -0800 (PST)
From:   bot@edi.works
To:     yuzhao@google.com
Cc:     linux-kernel@vger.kernel.org, linux-mm@kvack.org,
        page-reclaim@google.com, corbet@lwn.net,
        michael@michaellarabel.com, sofia.trinh@edi.works
Subject: Re: [PATCH v5 00/10] Multigenerational LRU Framework
Date:   Wed,  8 Dec 2021 23:24:16 -0800
Message-Id: <20211209072416.33606-1-bot@edi.works>
X-Mailer: git-send-email 2.18.0
In-Reply-To: <20211111041510.402534-1-yuzhao@google.com>
References: <20211111041510.402534-1-yuzhao@google.com>
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Kernel / Apache Hadoop benchmark with MGLRU

TLDR
====
With the MGLRU, Apache Hadoop took 95% CIs [5.31, 9.69]% and [2.02,
7.86]% less wall time to finish TeraSort, respectively, under the
medium- and the high-concurrency conditions, when swap was on. There
were no statistically significant changes in wall time for the rest
of the test matrix.

Background
==========
Memory overcommit can increase utilization and, if carried out
properly, can also increase throughput. The challenges are to improve
working set estimation and to optimize page reclaim. The risks are
performance degradation and OOM kills. Short of overcoming the
challenges, the only way to reduce the risks is to underutilize
memory.

Apache Hadoop is one of the most popular open-source big-data
frameworks. TeraSort is the most widely used benchmark for Apache
Hadoop.

Matrix
======
Kernels: version [+ patchset]
* Baseline: 5.15
* Patched: 5.15 + MGLRU

Swap configurations:
* Off
* On

Concurrency conditions: average # of tasks per CPU
* Low: 1/2
* Medium: 1
* High: 2

Cluster mode: local (12 concurrent jobs)
Dataset size: 100 million records from TeraGen

Total configurations: 12
Data points per configuration: 10
Total run duration (minutes) per data point: ~20

Procedure
=========
The latest MGLRU patchset for the 5.15 kernel is available at
git fetch https://linux-mm.googlesource.com/page-reclaim \
    refs/changes/30/1430/2

Baseline and patched 5.15 kernel images are available at
https://drive.google.com/drive/folders/1eMkQleAFGkP2vzM_JyRA21oKE0ESHBqp

<install and configure OS>
teragen 100000000 /mnt/data/raw
e2image <backup /mnt/data>

<for each kernel>
    grub-set-default <baseline, patched>
    <for each swap configuration>
        <swapoff, swapon>
        <update run_terasort.sh>
        <for each concurrency condition>
            <update run_terasort.sh>
            <for each data point>
                e2image <restore /mnt/data>
                reboot
                run_terasort.sh
                <collect wall time>

Hardware
========
Memory (GB): 256
CPU (total #): 48
NVMe SSD (GB): 2048

OS
==
$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=21.10
DISTRIB_CODENAME=impish
DISTRIB_DESCRIPTION="Ubuntu 21.10"

$ cat /proc/swaps
Filename          Type          Size          Used     Priority
/swap.img         partition     67108860      0        -2

$ cat /proc/sys/vm/overcommit_memory
1

$ cat /proc/sys/vm/swappiness
1

Apache Hadoop
=============
$ hadoop version
Hadoop 3.3.1
Source code repository https://github.com/apache/hadoop.git -r
a3b9c37a397ad4188041dd80621bdeefc46885f2
Compiled by ubuntu on 2021-06-15T05:13Z
Compiled with protoc 3.7.1
>From source with checksum 88a4ddb2299aca054416d6b7f81ca55
This command was run using
/root/hadoop-3.3.1/share/hadoop/common/hadoop-common-3.3.1.jar

$ cat run_terasort.sh
export HADOOP_ROOT_LOGGER="WARN,DRFA"
export HADOOP_HEAPSIZE_MAX=<swapoff: 20G, swapon: 22G>

for ((i = 0; i < 12; i++))
do
    /usr/bin/time -f "%e" hadoop jar \
        hadoop-mapreduce-examples-3.3.1.jar terasort \
        -Dfile.stream-buffer-size=8388608 \
        -Dio.file.buffer.size=8388608 \
        -Dmapreduce.job.heap.memory-mb.ratio=1.0 \
        -Dmapreduce.reduce.input.buffer.percent=1.0 \
        -Dmapreduce.reduce.merge.inmem.threshold=0 \
        -Dmapreduce.task.io.sort.factor=100 \
        -Dmapreduce.task.io.sort.mb=1000 \
        -Dmapreduce.terasort.final.sync=false \
        -Dmapreduce.terasort.num.partitions=100 \
        -Dmapreduce.terasort.partitions.sample=1000000 \
        -Dmapreduce.local.map.tasks.maximum=<2, 4, 8> \
        -Dmapreduce.local.reduce.tasks.maximum=<2, 4, 8> \
        -Dmapreduce.reduce.shuffle.parallelcopies=<2, 4, 8> \
        -Dhadoop.tmp.dir=/mnt/data/tmp$i \
        /mnt/data/raw /mnt/data/sorted$i
done

wait

Results
=======
Comparing the patched with the baseline kernel, Apache Hadoop took
95% CIs [5.31, 9.69]% and [2.02, 7.86]% less wall time to finish
TeraSort, respectively, under the medium- and the high-concurrency
conditions, when swap was on. There were no statistically significant
changes in wall time for the rest of the test matrix.

+--------------------+------------------+------------------+
| Mean wall time (s) | Swap off         | Swap on          |
| [95% CI]           |                  |                  |
+--------------------+------------------+------------------+
| Low concurrency    | 758.43 / 746.83  | 740.78 / 733.42  |
|                    | [-26.80, 3.60]   | [-18.07, 3.35]   |
+--------------------+------------------+------------------+
| Medium concurrency | 911.81 / 910.19  | 911.53 / 843.15  |
|                    | [-26.70, 23.46]  | [-88.35, -48.39] |
+--------------------+------------------+------------------+
| High concurrency   | 921.17 / 929.51  | 1042.85 / 991.33 |
|                    | [-25.50, 42.18]  | [-81.94, -21.08] |
+--------------------+------------------+------------------+
Table 1. Comparison between the baseline and the patched kernels

Comparing swap on with swap off, Apache Hadoop took 95% CIs [-3.39,
-1.27]% and [10.69, 15.73]% more wall time to finish TeraSort,
respectively, under the low- and the high-concurrency conditions,
when using the baseline kernel; 95% CIs [-9.34, -5.39]% and [2.52,
10.78]% more wall time, respectively, under the medium- and the
high-concurrency conditions, when using the patched kernel. There
were no statistically significant changes in wall time for the rest
of the test matrix.

+--------------------+------------------+------------------+
| Mean wall time (s) | Baseline kernel  | Patched kernel   |
| [95% CI]           |                  |                  |
+--------------------+------------------+------------------+
| Low concurrency    | 758.43 / 740.78  | 746.83 / 733.42  |
|                    | [-25.67, -9.64]  | [-29.80, 2.97]   |
+--------------------+------------------+------------------+
| Medium concurrency | 911.81 / 911.53  | 910.19 / 843.15  |
|                    | [-26.62, 26.06]  | [-84.98, -49.09] |
+--------------------+------------------+------------------+
| High concurrency   | 921.17 / 1042.85 | 929.51 / 991.33  |
|                    | [98.51, 144.85]  | [23.43, 100.21]  |
+--------------------+------------------+------------------+
Table 2. Comparison between swap off and on

Metrics collected during each run are available at
https://github.com/ediworks/KernelPerf/tree/master/mglru/hadoop/5.15

Appendix
========
$ cat raw_data_hadoop.r
v <- c(
    # baseline swapoff 2mr
    742.83, 751.91, 755.75, 757.50, 757.83, 758.16, 758.25, 763.58, 766.58, 772.00,
    # baseline swapoff 4mr
    863.25, 868.08, 886.58, 894.66, 901.16, 918.25, 940.91, 944.08, 949.66, 951.50,
    # baseline swapoff 8mr
    892.16, 895.75, 909.25, 922.58, 922.91, 922.91, 923.16, 926.00, 935.33, 961.66,
    # baseline swapon 2mr
    731.58, 732.08, 736.66, 737.75, 738.00, 738.08, 740.08, 740.33, 752.58, 760.66,
    # baseline swapon 4mr
    878.83, 886.33, 902.75, 904.83, 907.25, 918.50, 921.33, 925.50, 927.58, 942.41,
    # baseline swapon 8mr
    1016.58, 1017.33, 1019.33, 1019.50, 1026.08, 1030.50, 1065.16, 1070.50, 1075.25, 1088.33,
    # patched swapoff 2mr
    720.41, 724.58, 727.41, 732.00, 745.41, 748.00, 754.50, 767.91, 773.16, 775.00,
    # patched swapoff 4mr
    887.16, 887.50, 906.66, 907.41, 915.00, 915.58, 915.66, 916.91, 925.00, 925.08,
    # patched swapoff 8mr
    857.08, 864.41, 910.25, 918.58, 921.91, 933.75, 949.50, 966.75, 984.00, 988.91,
    # patched swapon 2mr
    719.33, 721.91, 724.41, 724.83, 725.75, 728.75, 737.83, 743.91, 749.41, 758.08,
    # patched swapon 4mr
    813.33, 819.00, 821.91, 829.33, 839.50, 846.75, 850.25, 857.00, 875.83, 878.66,
    # patched swapon 8mr
    929.41, 955.83, 961.16, 974.66, 988.75, 1004.00, 1009.08, 1019.91, 1030.58, 1040.00
)

a <- array(v, dim = c(10, 3, 2, 2))

# baseline vs patched
for (swap in 1:2) {
    for (mr in 1:3) {
        r <- t.test(a[, mr, swap, 1], a[, mr, swap, 2])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("swap%d mr%d: no significance", swap, mr)
        } else {
            s <- sprintf("swap%d mr%d: [%.2f, %.2f]%%", swap, mr, -p[2], -p[1])
        }
        print(s)
    }
}

# swapoff vs swapon
for (kern in 1:2) {
    for (mr in 1:3) {
        r <- t.test(a[, mr, 1, kern], a[, mr, 2, kern])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("kern%d mr%d: no significance", kern, mr)
        } else {
            s <- sprintf("kern%d mr%d: [%.2f, %.2f]%%", kern, mr, -p[2], -p[1])
        }
        print(s)
    }
}

$ R -q -s -f raw_data_hadoop.r

        Welch Two Sample t-test

data:  a[, mr, swap, 1] and a[, mr, swap, 2]
t = 1.6677, df = 11.658, p-value = 0.122
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -3.604753 26.806753
sample estimates:
mean of x mean of y
  758.439   746.838

[1] "swap1 mr1: no significance"

        Welch Two Sample t-test

data:  a[, mr, swap, 1] and a[, mr, swap, 2]
t = 0.14071, df = 11.797, p-value = 0.8905
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -23.4695  26.7035
sample estimates:
mean of x mean of y
  911.813   910.196

[1] "swap1 mr2: no significance"

        Welch Two Sample t-test

data:  a[, mr, swap, 1] and a[, mr, swap, 2]
t = -0.53558, df = 12.32, p-value = 0.6018
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -42.18602  25.50002
sample estimates:
mean of x mean of y
  921.171   929.514

[1] "swap1 mr3: no significance"

        Welch Two Sample t-test

data:  a[, mr, swap, 1] and a[, mr, swap, 2]
t = 1.4568, df = 15.95, p-value = 0.1646
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -3.352318 18.070318
sample estimates:
mean of x mean of y
  740.780   733.421

[1] "swap2 mr1: no significance"

        Welch Two Sample t-test

data:  a[, mr, swap, 1] and a[, mr, swap, 2]
t = 7.204, df = 17.538, p-value = 1.229e-06
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 48.39677 88.35323
sample estimates:
mean of x mean of y
  911.531   843.156

[1] "swap2 mr2: [-9.69, -5.31]%"

        Welch Two Sample t-test

data:  a[, mr, swap, 1] and a[, mr, swap, 2]
t = 3.5698, df = 17.125, p-value = 0.002336
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 21.08655 81.94945
sample estimates:
mean of x mean of y
 1042.856   991.338

[1] "swap2 mr3: [-7.86, -2.02]%"

        Welch Two Sample t-test

data:  a[, mr, 1, kern] and a[, mr, 2, kern]
t = 4.6319, df = 17.718, p-value = 0.0002153
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  9.640197 25.677803
sample estimates:
mean of x mean of y
  758.439   740.780

[1] "kern1 mr1: [-3.39, -1.27]%"

        Welch Two Sample t-test

data:  a[, mr, 1, kern] and a[, mr, 2, kern]
t = 0.0229, df = 14.372, p-value = 0.982
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -26.06533  26.62933
sample estimates:
mean of x mean of y
  911.813   911.531

[1] "kern1 mr2: no significance"

        Welch Two Sample t-test

data:  a[, mr, 1, kern] and a[, mr, 2, kern]
t = -11.129, df = 16.051, p-value = 5.874e-09
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -144.8574  -98.5126
sample estimates:
mean of x mean of y
  921.171  1042.856

[1] "kern1 mr3: [10.69, 15.73]%"

        Welch Two Sample t-test

data:  a[, mr, 1, kern] and a[, mr, 2, kern]
t = 1.7413, df = 15.343, p-value = 0.1016
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2.974529 29.808529
sample estimates:
mean of x mean of y
  746.838   733.421

[1] "kern2 mr1: no significance"

        Welch Two Sample t-test

data:  a[, mr, 1, kern] and a[, mr, 2, kern]
t = 7.9839, df = 14.571, p-value = 1.073e-06
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 49.09637 84.98363
sample estimates:
mean of x mean of y
  910.196   843.156

[1] "kern2 mr2: [-9.34, -5.39]%"

        Welch Two Sample t-test

data:  a[, mr, 1, kern] and a[, mr, 2, kern]
t = -3.3962, df = 17.1, p-value = 0.003413
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -100.21425  -23.43375
sample estimates:
mean of x mean of y
  929.514   991.338

[1] "kern2 mr3: [2.52, 10.78]%"


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by smtp.lore.kernel.org (Postfix) with ESMTP id B7943C433F5
	for <linux-kernel@archiver.kernel.org>; Sat, 18 Dec 2021 07:12:48 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S232226AbhLRHMq (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Sat, 18 Dec 2021 02:12:46 -0500
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:42300 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S232199AbhLRHMq (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Sat, 18 Dec 2021 02:12:46 -0500
Received: from mail-qk1-x734.google.com (mail-qk1-x734.google.com [IPv6:2607:f8b0:4864:20::734])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id 95230C061574
        for <linux-kernel@vger.kernel.org>; Fri, 17 Dec 2021 23:12:45 -0800 (PST)
Received: by mail-qk1-x734.google.com with SMTP id 132so4428707qkj.11
        for <linux-kernel@vger.kernel.org>; Fri, 17 Dec 2021 23:12:45 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=edi.works; s=google;
        h=from:to:cc:subject:date:message-id:in-reply-to:references;
        bh=PwL8wIkTOyjgtNkwFSM8qrD5vBbMoqMOLnchknihk3s=;
        b=Epy4M/J0wTytOQUJ31lQGrSX0OVU/I4MS9mB+9f11skFGJ+BVzl4xtw6edGxzVPH+b
         CgIQxiezHoNqDcV8+ZF0ZFUFhYBmE18X0NP4t4VHIl1G/SebjA4cWQU2vqHmuHIfzZDc
         /ZwBsSBduEjKQy+ZPG9BEsefJBNWa4SJNDuqN+1JP1+Jjh6EKfXAmAFbkSQfJCouqvkv
         gpNGgu4WQSYOrka56NY5n0vSxWAOWl4DJjC+kXxd1uIN5+zIwaL3M4S3erD6KwLVmQ7e
         KBh7eN5hfw/MvvwCAEcpTzLFPT+CmT1aBeR/YssFkXdvQi1Te2Po3UdcNbsa+jvbPrzU
         DPYg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references;
        bh=PwL8wIkTOyjgtNkwFSM8qrD5vBbMoqMOLnchknihk3s=;
        b=ZfJPNWUJFgO4WjAjpfrkQx40qInw2ncu8Jk1UfnF/JCgpHoV0AIuMjfE2m4ohDSgpK
         BTHdHZYG+a6pF2f/eBcBJBCmoTpf2CkxpUYD0aVdePnDK4odbwE6x8tWJgyKT/z+dmjk
         uRjCDlBTbYubwgh37Xt3qWSPHiwppwpn5mv0ioDOO/0PZZUujD7IRgFK7K1trm/H/va8
         nNsy6p2eF53X9Fbb7qjRM0uijAgt2E/gvJDQ++XauHzHfGThPPJMi+KHH+4xJTUNe4k2
         itrdQ1ZrHNrBvmpe9X66Y/e1C2JwcyA9zGPC+XnB/0kXHazZ7xMPuyYN9IX45llLZ3Ot
         u9Sw==
X-Gm-Message-State: AOAM530HWswC5u6xFuaEtKr+/EuXwFk8+e26qV9sJJUWqhF7oqqtAuxu
        x+oD0Lc0jQRE+5Nkr15KpUvvXg==
X-Google-Smtp-Source: ABdhPJyN9fxKJ74zGQh5vpfuZzaqrMa1rtoR2ARC6DljWGqrO4v6kA4L/RaCaKdU1gPeG+rjubRlVw==
X-Received: by 2002:ae9:ef0e:: with SMTP id d14mr4063248qkg.773.1639811564491;
        Fri, 17 Dec 2021 23:12:44 -0800 (PST)
Received: from localhost.localdomain (c-67-169-44-201.hsd1.ca.comcast.net. [67.169.44.201])
        by smtp.gmail.com with ESMTPSA id d6sm8108004qtq.15.2021.12.17.23.12.42
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Fri, 17 Dec 2021 23:12:43 -0800 (PST)
From:   bot@edi.works
To:     yuzhao@google.com
Cc:     linux-kernel@vger.kernel.org, linux-mm@kvack.org,
        page-reclaim@google.com, corbet@lwn.net,
        michael@michaellarabel.com, sofia.trinh@edi.works
Subject: Re: [PATCH v5 00/10] Multigenerational LRU Framework
Date:   Fri, 17 Dec 2021 23:10:41 -0800
Message-Id: <20211218071041.24077-1-bot@edi.works>
X-Mailer: git-send-email 2.18.0
In-Reply-To: <20211111041510.402534-1-yuzhao@google.com>
References: <20211111041510.402534-1-yuzhao@google.com>
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Kernel / PostgreSQL benchmark with MGLRU

TLDR
====
With the MGLRU, PostgreSQL achieved 95% CI [1.75, 6.42]% more
transactions per minute (TPM) under the high-concurrency conditions,
when swap was off; 95% CIs [12.82, 18.69]% and [22.70, 46.86]% more
TPM, respectively, under the medium- and the high-concurrency
conditions, when swap was on. There were no statistically significant
changes in TPM for the rest of the test matrix.

Background
==========
Memory overcommit can increase utilization and, if carried out
properly, can also increase throughput. The challenges are to improve
working set estimation and to optimize page reclaim. The risks are
performance degradation and OOM kills. Short of overcoming the
challenges, the only way to reduce the risks is to underutilize
memory.

PostgreSQL is one of the most popular open-source RDBMSs. HammerDB is
the leading open-source benchmarking software derived from the TPC
specifications. OLTP is the most important use case for RDBMSs.

Matrix
======
Kernels: version [+ patchset]
* Baseline: 5.15
* Patched: 5.15 + MGLRU

Swap configurations:
* Off
* On (vm.swappiness=1)

Concurrency conditions: average # of users per CPU
* Low: ~4
* Medium: ~8
* High: ~12

Total configurations: 12
Data points per configuration: 10
Total run duration (minutes) per data point: ~50

Procedure
=========
The latest MGLRU patchset for the 5.15 kernel is available at
git fetch https://linux-mm.googlesource.com/page-reclaim \
    refs/changes/30/1430/2

The baseline and the patched 5.15 kernel images are available at
https://drive.google.com/drive/folders/1eMkQleAFGkP2vzM_JyRA21oKE0ESHBqp

<install and configure OS>
hammerdbcli auto prep_tpcc.tcl
systemctl stop postgresql
e2image <backup /mnt/data>

<for each kernel>
    grub-set-default <baseline, patched>
    <for each swap configuration>
        <swapoff, swapon>
        <update /etc/postgresql/13/main/postgresql.conf>
        <for each concurrency condition>
            <update run_tpcc.tcl>
            <for each data point>
                systemctl stop postgresql
                e2image <restore /mnt/data>
                reboot
                hammerdbcli auto run_tpcc.tcl
                <collect TPM>

Hardware
========
Memory (GB): 256
CPU (total #): 48
NVMe SSD (GB): 2048

OS
==
$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=21.10
DISTRIB_CODENAME=impish
DISTRIB_DESCRIPTION="Ubuntu 21.10"

$ cat /proc/swaps
Filename          Type          Size          Used     Priority
/swap.img         partition     67108860      0        -2

$ cat /proc/sys/vm/overcommit_memory
1

$ cat /proc/sys/vm/swappiness
1

$ cat /sys/fs/cgroup/user.slice/memory.min
4294967296

PostgreSQL
==========
$ pg_config --version
PostgreSQL 13.5 (Ubuntu 13.5-0ubuntu0.21.10.1)

$ cat /etc/postgresql/13/main/postgresql.conf
<existing parameters>

data_directory = '/mnt/data'
max_connections = 1000
shared_buffers = <swapoff: 120GB, swapon: 150GB>
temp_buffers = 1GB
work_mem = 1GB
maintenance_work_mem = 1GB
logical_decoding_work_mem = 1GB
bgwriter_delay = 1000ms
bgwriter_lru_maxpages = 100000
bgwriter_lru_multiplier = 1.0
bgwriter_flush_after = 0
effective_io_concurrency = 1000
wal_compression = on
wal_writer_delay = 1000ms
wal_writer_flush_after = 128MB
max_wal_size = 100GB
min_wal_size = 10GB
checkpoint_flush_after = 0
effective_cache_size = 150GB

<existing parameters>

HammerDB
========
$ hammerdbcli -h
HammerDB CLI v4.3
Copyright (C) 2003-2021 Steve Shaw
Type "help" for a list of commands
Usage: hammerdbcli [ auto [ script_to_autoload.tcl  ] ]

$ cat prep_tpcc.tcl
dbset db pg
diset connection pg_host /var/run/postgresql
diset tpcc pg_count_ware 2400
diset tpcc pg_num_vu 48
buildschema
waittocomplete
quit

$ cat run_tpcc.tcl
dbset db pg
diset connection pg_host /var/run/postgresql
diset tpcc pg_total_iterations 20000000
diset tpcc pg_driver timed
diset tpcc pg_rampup 30
diset tpcc pg_duration 10
diset tpcc pg_allwarehouse true
vuset logtotemp 1
vuset unique 1
loadscript
vuset vu <200, 400, 600>
vucreate
vurun
runtimer 3000
vudestroy

Results
=======
Comparing the patched with the baseline kernel, PostgreSQL achieved
95% CI [1.75, 6.42]% more TPM under the high-concurrency conditions,
when swap was off; 95% CIs [12.82, 18.69]% and [22.70, 46.86]% more
TPM, respectively, under the medium- and the high-concurrency
conditions, when swap was on. There were no statistically significant
changes in TPM for the rest of the test matrix.

+--------------------+--------------------+--------------------+
| Mean TPM [95% CI]  | Swap off           | Swap on            |
+--------------------+--------------------+--------------------+
| Low concurrency    | 466430 / 467521    | 475060 / 475047    |
|                    | [-6931, 9112]      | [-7431, 7405]      |
+--------------------+--------------------+--------------------+
| Medium concurrency | 453871 / 459592    | 388245 / 449409    |
|                    | [-774, 12216]      | [49755, 72572]     |
+--------------------+--------------------+--------------------+
| High concurrency   | 443014 / 461112    | 157106 / 211752    |
|                    | [7771, 28423]      | [35664, 73627]     |
+--------------------+--------------------+--------------------+
Table 1. Comparison between the baseline and the patched kernels

Comparing swap on with swap off, PostgreSQL achieved 95% CIs [0.46,
3.24]%, [-16.91, -12.01]% and [-68.64, -60.43]% more TPM,
respectively, under the low-, the medium- and the high-concurrency
conditions, when using the baseline kernel; 95% CIs [-3.76, -0.67]%
and [-56.70, -51.46]% more TPM, respectively, under the medium- and
the high-concurrency conditions, when using the patched kernel. There
were no statistically significant changes in TPM for the rest of the
test matrix.

+--------------------+--------------------+--------------------+
| Mean TPM [95% CI]  | Baseline kernel    | Patched kernel     |
+--------------------+--------------------+--------------------+
| Low concurrency    | 466430 / 475060    | 467521 / 475047    |
|                    | [2160, 15100]      | [-1204, 16256]     |
+--------------------+--------------------+--------------------+
| Medium concurrency | 453871 / 388245    | 459592 / 449409    |
|                    | [-76757, -54494]   | [-17292, -3073]    |
+--------------------+--------------------+--------------------+
| High concurrency   | 443014 / 157106    | 461112 / 211752    |
|                    | [-304097, -267718] | [-261442, -237275] |
+--------------------+--------------------+--------------------+
Table 2. Comparison between swap off and swap on

Metrics collected during each run are available at
https://github.com/ediworks/KernelPerf/tree/master/mglru/postgres/5.15

Appendix
========
$ cat raw_data_postgres.r
v <- c(
    # baseline swapoff 200vu
    462379, 462998, 463363, 464949, 465605, 466977, 467290, 468658, 469682, 472404,
    # baseline swapoff 400vu
    446111, 446305, 447339, 448043, 450604, 452160, 453846, 461309, 465101, 467893,
    # baseline swapoff 600vu
    434061, 435645, 435974, 436026, 436581, 439138, 442121, 445990, 454687, 469926,
    # baseline swapon 200vu
    466546, 467298, 467882, 469185, 472114, 473868, 475217, 481319, 483246, 493931,
    # baseline swapon 400vu
    367605, 371855, 373991, 380763, 388456, 389768, 395270, 403536, 404457, 406749,
    # baseline swapon 600vu
    123036, 127174, 131863, 150724, 155572, 158938, 170892, 179302, 183783, 189785,
    # patched swapoff 200vu
    456088, 457197, 457341, 458069, 459630, 472291, 474782, 475727, 478015, 486071,
    # patched swapoff 400vu
    452681, 453758, 455800, 457675, 458812, 459304, 460897, 461252, 465269, 470475,
    # patched swapoff 600vu
    448009, 452465, 453655, 454333, 456111, 456304, 465371, 471431, 475092, 478351,
    # patched swapon 200vu
    465540, 468681, 471682, 473134, 473148, 474015, 475734, 476691, 481974, 489873,
    # patched swapon 400vu
    436300, 440202, 441955, 445214, 445817, 452176, 452379, 456931, 457724, 465393,
    # patched swapon 600vu
    195315, 197186, 199332, 199667, 209630, 211162, 214787, 222783, 230000, 237667
)

a <- array(v, dim = c(10, 3, 2, 2))

# baseline vs patched
for (swap in 1:2) {
    for (vu in 1:3) {
        r <- t.test(a[, vu, swap, 1], a[, vu, swap, 2])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("swap%d vu%d: no significance", swap, vu)
        } else {
            s <- sprintf("swap%d vu%d: [%.2f, %.2f]%%", swap, vu, -p[2], -p[1])
        }
        print(s)
    }
}

# swapoff vs swapon
for (kern in 1:2) {
    for (vu in 1:3) {
        r <- t.test(a[, vu, 1, kern], a[, vu, 2, kern])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("kern%d vu%d: no significance", kern, vu)
        } else {
            s <- sprintf("kern%d vu%d: [%.2f, %.2f]%%", kern, vu, -p[2], -p[1])
        }
        print(s)
    }
}

$ R -q -s -f raw_data_postgres.r

        Welch Two Sample t-test

data:  a[, vu, swap, 1] and a[, vu, swap, 2]
t = -0.3009, df = 10.521, p-value = 0.7694
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -9112.559  6931.359
sample estimates:
mean of x mean of y
 466430.5  467521.1

[1] "swap1 vu1: no significance"

        Welch Two Sample t-test

data:  a[, vu, swap, 1] and a[, vu, swap, 2]
t = -1.8711, df = 15.599, p-value = 0.08021
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -12216.64    774.24
sample estimates:
mean of x mean of y
 453871.1  459592.3

[1] "swap1 vu2: no significance"

        Welch Two Sample t-test

data:  a[, vu, swap, 1] and a[, vu, swap, 2]
t = -3.6832, df = 17.919, p-value = 0.001712
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -28423.515  -7771.085
sample estimates:
mean of x mean of y
 443014.9  461112.2

[1] "swap1 vu3: [1.75, 6.42]%"

        Welch Two Sample t-test

data:  a[, vu, swap, 1] and a[, vu, swap, 2]
t = 0.0038109, df = 17.001, p-value = 0.997
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -7405.094  7431.894
sample estimates:
mean of x mean of y
 475060.6  475047.2

[1] "swap2 vu1: no significance"

        Welch Two Sample t-test

data:  a[, vu, swap, 1] and a[, vu, swap, 2]
t = -11.413, df = 15.222, p-value = 7.301e-09
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -72572.5 -49755.7
sample estimates:
mean of x mean of y
 388245.0  449409.1

[1] "swap2 vu2: [12.82, 18.69]%"

        Welch Two Sample t-test

data:  a[, vu, swap, 1] and a[, vu, swap, 2]
t = -6.1414, df = 14.853, p-value = 1.97e-05
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -73627.83 -35664.17
sample estimates:
mean of x mean of y
 157106.9  211752.9

[1] "swap2 vu3: [22.70, 46.86]%"

        Welch Two Sample t-test

data:  a[, vu, 1, kern] and a[, vu, 2, kern]
t = -2.9241, df = 11.372, p-value = 0.0134
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -15100.107  -2160.093
sample estimates:
mean of x mean of y
 466430.5  475060.6

[1] "kern1 vu1: [0.46, 3.24]%"

        Welch Two Sample t-test

data:  a[, vu, 1, kern] and a[, vu, 2, kern]
t = 12.629, df = 14.192, p-value = 4.129e-09
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 54494.92 76757.28
sample estimates:
mean of x mean of y
 453871.1  388245.0

[1] "kern1 vu2: [-16.91, -12.01]%"

        Welch Two Sample t-test

data:  a[, vu, 1, kern] and a[, vu, 2, kern]
t = 34.005, df = 12.822, p-value = 5.981e-14
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 267718.5 304097.5
sample estimates:
mean of x mean of y
 443014.9  157106.9

[1] "kern1 vu3: [-68.64, -60.43]%"

        Welch Two Sample t-test

data:  a[, vu, 1, kern] and a[, vu, 2, kern]
t = -1.8367, df = 15.057, p-value = 0.08607
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -16256.986   1204.786
sample estimates:
mean of x mean of y
 467521.1  475047.2

[1] "kern2 vu1: no significance"

        Welch Two Sample t-test

data:  a[, vu, 1, kern] and a[, vu, 2, kern]
t = 3.061, df = 14.554, p-value = 0.008153
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  3073.49 17292.91
sample estimates:
mean of x mean of y
 459592.3  449409.1

[1] "kern2 vu2: [-3.76, -0.67]%"

        Welch Two Sample t-test

data:  a[, vu, 1, kern] and a[, vu, 2, kern]
t = 43.656, df = 16.424, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 237275.9 261442.7
sample estimates:
mean of x mean of y
 461112.2  211752.9

[1] "kern2 vu3: [-56.70, -51.46]%"


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by smtp.lore.kernel.org (Postfix) with ESMTP id A7550C433F5
	for <linux-kernel@archiver.kernel.org>; Mon, 22 Nov 2021 05:34:57 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S230390AbhKVFiA (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Mon, 22 Nov 2021 00:38:00 -0500
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:38414 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S229994AbhKVFh7 (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 22 Nov 2021 00:37:59 -0500
Received: from mail-qk1-x733.google.com (mail-qk1-x733.google.com [IPv6:2607:f8b0:4864:20::733])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id D6111C061574
        for <linux-kernel@vger.kernel.org>; Sun, 21 Nov 2021 21:34:52 -0800 (PST)
Received: by mail-qk1-x733.google.com with SMTP id t6so17006456qkg.1
        for <linux-kernel@vger.kernel.org>; Sun, 21 Nov 2021 21:34:52 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=edi.works; s=google;
        h=from:to:cc:subject:date:message-id:in-reply-to:references;
        bh=8/XTeLmVT7yxl+C3YSSHY74ibGGn3peNDbuBolAOdpI=;
        b=6EQn4D1jTRNzLkXvKUZGoLS4o6zJKZnGfExnEjcq0tRIzzTXZQVZaZnDAq8lhZsTJB
         27rUqB/B2yjV8upYIx4g07DqODL9hOQPtYJKDQqHw0+wAptUh9wwxQ3CFXpgJTv14tc3
         A3khCzwZ0k21HxSxzNxxDoCCLtPY4w/QZqcxOr7UYwG2SDe9iQZQItznVwKkYtKnj4Tl
         yo2SYx8r/Dye1wkuENXWXNm923uaZ5Ke+AGRt0nq4MJZfGTfsE65V1fhpCGKEEVk9YeK
         iNBbSKFgiKzE4awfn1Zc44qpmw9mPUoJdzko2tGFo0EWquhnCz2vAk8NA0RKgBfiwgtl
         ML5Q==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references;
        bh=8/XTeLmVT7yxl+C3YSSHY74ibGGn3peNDbuBolAOdpI=;
        b=27jHwVlVz4dMypvVva7mDPe865O0ewWiV9l5UIefbm5FT/XS0Zi6QJg+BdJ6IEtYHs
         nZbDTgF7kRSwVT8FL5li9exZBakCtabb6D1xl7qU26P8dm08kyxQlHmgob8TnHmsx/DH
         tfL1McbAPamtmnrosZfSY/kxPoLKAtXHU2T7F/MQ7fQpsKH261/HaDNWckmPcsITYult
         bLYwNx5hqp+rB8g0CjPSkwZKGnLWPgCbWc6k8qmiPJFed4VRAUpjVNsl6lnidAI0HDoh
         9DeMbsdfAB79U4mD9m1Ch/xUDNoi4kWm3NrGb5TmMCgeUq1DKRld+ZQnOkv0fowKIEWl
         C13g==
X-Gm-Message-State: AOAM533ElBDtem7Sx4WyajIh0Yy93aWmZ3UyybgiAyJP1qDqZddHlZWA
        AoVdNqc7xDIWVsmgkTFsM3oP5x4/jJMPQw==
X-Google-Smtp-Source: ABdhPJxAVNzInwMWJVlrxM5W7riPdgdCuVgiaKYK+64Rkip3HSLZrRqk0PE/d3ULaRYMK98vjaMfJQ==
X-Received: by 2002:a37:62c5:: with SMTP id w188mr46705436qkb.396.1637559291802;
        Sun, 21 Nov 2021 21:34:51 -0800 (PST)
Received: from localhost.localdomain (c-67-169-44-201.hsd1.ca.comcast.net. [67.169.44.201])
        by smtp.gmail.com with ESMTPSA id p10sm4053447qtw.97.2021.11.21.21.34.50
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Sun, 21 Nov 2021 21:34:51 -0800 (PST)
From:   bot@edi.works
To:     yuzhao@google.com
Cc:     linux-kernel@vger.kernel.org, linux-mm@kvack.org,
        page-reclaim@google.com, corbet@lwn.net,
        michael@michaellarabel.com, sofia.trinh@edi.works
Subject: Re: [PATCH v5 00/10] Multigenerational LRU Framework
Date:   Sun, 21 Nov 2021 21:32:48 -0800
Message-Id: <20211122053248.57311-1-bot@edi.works>
X-Mailer: git-send-email 2.18.0
In-Reply-To: <20211111041510.402534-1-yuzhao@google.com>
References: <20211111041510.402534-1-yuzhao@google.com>
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Kernel / Redis benchmark with MGLRU

TLDR
====
With the MGLRU, Redis achieved 95% CIs [0.58, 5.94]%, [6.55, 14.58]%,
[11.47, 19.36]%, [1.27, 3.54]%, [10.11, 14.81]% and [8.75, 13.64]%
more operations per second (OPS), respectively, for sequential access
w/ THP=always, random access w/ THP=always, Gaussian (distribution)
access w/ THP=always, sequential access w/ THP=never, random access
w/ THP=never and Gaussian access w/ THP=never.

Background
==========
Memory overcommit can increase utilization and, if carried out
properly, can also increase throughput. The challenges are to improve
working set estimation and to optimize page reclaim. The risks are
performance degradation and OOM kills. Short of overcoming the
challenges, the only way to reduce the risks is to underutilize
memory.

Redis is one of the most popular open-source in-memory KV stores.
memtier_benchmark is the leading open-source KV store benchmarking
software that supports multiple access patterns. THP can have a
negative effect under memory pressure, due to internal and/or
external fragmentations.

Matrix
======
Kernels: version [+ patchset]
* Baseline: 5.15
* Patched: 5.15 + MGLRU

Memory utilization: % of memory size
* Underutilizing: N/A
* Overcommitting: ~10% swapped out (zram)

THP (2MB Transparent Huge Pages):
* Always
* Never

Access patterns (4kB objects, 100% read):
* Parallel sequential
* Uniform random
* Gaussian (SD = 1/6 of key range)

Concurrency: average # of users per CPU
* Low: 1

Total configurations: 12
Data points per configuration: 10
Total run duration (minutes) per data point: ~25

Note that the goal of this benchmark is to compare the performance
for the same key range, object size, and hit ratio. Since Redis does
not support eviction to backing storage, it would require fewer
in-memory objects to underutilize memory, which reduces the hit ratio
and therefore is not applicable in this case.

Procedure
=========
The latest MGLRU patchset for the 5.15 kernel is available at
git fetch https://linux-mm.googlesource.com/page-reclaim \
    refs/changes/30/1430/2

Baseline and patched 5.15 kernel images are available at
https://drive.google.com/drive/folders/1eMkQleAFGkP2vzM_JyRA21oKE0ESHBqp

<install and configure OS>
<duplicate Redis service>

<for each kernel>
    grub-set-default <baseline, patched>
    <for each THP setting>
        echo <always, never> >/sys/kernel/mm/transparent_hugepage/enabled
        <for each access pattern>
            <update run_memtier.sh>
            <for each data point>
                reboot
                run_memtier.sh
                <collect total OPS>

Note that the OSS version of Redis does not support sharding, i.e.,
one service uses a single thread to serve all connections. Therefore,
on larger machines, multiple Redis services are required to achieve
better throughput.

Hardware
========
Memory (GB): 256
CPU (total #): 48
NVMe SSD (GB): 1024

OS
==
$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=21.10
DISTRIB_CODENAME=impish
DISTRIB_DESCRIPTION="Ubuntu 21.10"

$ cat /proc/swaps
Filename        Type          Size         Used         Priority
/dev/zram0      partition     10485756     0            1
/dev/zram1      partition     10485756     0            1
/dev/zram2      partition     10485756     0            1
/dev/zram3      partition     10485756     0            1

$ cat /sys/fs/cgroup/user.slice/memory.min
4294967296

$ cat /proc/sys/vm/overcommit_memory
1

Redis
=====
$ redis-server -v
Redis server v=6.0.15 sha=00000000:0 malloc=jemalloc-5.2.1 bits=64
build=4610f4c3acf7fb25

$ cat /etc/redis/redis.conf
<existing parameters>
save ""
unixsocket /var/run/redis/redis-server.sock

memtier_benchmark
=================
$ memtier_benchmark -v
memtier_benchmark 1.3.0
Copyright (C) 2011-2020 Redis Labs Ltd.
This is free software.  You may redistribute copies of it under the
terms of the GNU General Public License
<http://www.gnu.org/licenses/gpl.html>.  There is NO WARRANTY, to the
extent permitted by law.

$ cat run_memtier.sh
# load objects
for ((i = 0; i < 12; i++))
do
    memtier_benchmark -S /var/run/redis$i/redis-server.sock -P redis \
        -n allkeys -c 4 -t 4 --ratio 1:0 --pipeline 8 -d 4000 \
        --key-minimum=1 --key-maximum=5300000 --key-pattern=P:P &
done

wait

# run benchmark
for ((i = 0; i < 12; i++))
do
    memtier_benchmark -S /var/run/redis$i/redis-server.sock -P redis \
        --test-time=1200 -c 4 -t 4 --ratio 0:1 --pipeline 8 \
        --randomize --distinct-client-seed --key-minimum=1 \
        --key-maximum=5300000 --key-pattern=<P:P, R:R, G:G> &
done

wait

Results
=======
Comparing the patched with the baseline kernel, Redis achieved 95%
CIs [0.58, 5.94]%, [6.55, 14.58]%, [11.47, 19.36]%, [1.27, 3.54]%,
[10.11, 14.81]% and [8.75, 13.64]% more OPS, respectively, for
sequential access w/ THP=always, random access w/ THP=always,
Gaussian access w/ THP=always, sequential access w/ THP=never, random
access w/ THP=never and Gaussian access w/ THP=never.

+---------------------------+------------------+------------------+
| Mean million OPS [95% CI] | THP=always       | THP=never        |
+---------------------------+------------------+------------------+
| Sequential access         | 1.84 / 1.9       | 1.702 / 1.743    |
|                           | [0.01, 0.109]    | [0.021, 0.06]    |
+---------------------------+------------------+------------------+
| Random access             | 1.742 / 1.926    | 1.493 / 1.679    |
|                           | [0.114, 0.253]   | [0.15, 0.221]    |
+---------------------------+------------------+------------------+
| Gaussian access           | 1.771 / 2.044    | 1.635 / 1.818    |
|                           | [0.203, 0.342]   | [0.143, 0.222]   |
+---------------------------+------------------+------------------+
Table 1. Comparison between the baseline and patched kernels

Comparing THP=never with THP=always, Redis achieved 95% CIs [-8.66,
-6.34]%, [-17.6, -10.98]% and [-10.92, -4.44]% more OPS, respectively,
for sequential access, random access and Gaussian access when using
the baseline kernel; 95% CIs [-10.83, -5.7]%, [-15.72, -9.93]% and
[-13.92, -8.19]% more OPS, respectively, for sequential access, random
access and Gaussian access when using the patched kernel.

+---------------------------+------------------+------------------+
| Mean million OPS [95% CI] | Baseline kernel  | Patched kernel   |
+---------------------------+------------------+------------------+
| Sequential access         | 1.84 / 1.702     | 1.9 / 1.743      |
|                           | [-0.159, -0.116] | [-0.205, -0.108] |
+---------------------------+------------------+------------------+
| Random access             | 1.742 / 1.493    | 1.926 / 1.679    |
|                           | [-0.306, -0.191] | [-0.302, -0.191] |
+---------------------------+------------------+------------------+
| Gaussian access           | 1.771 / 1.635    | 2.044 / 1.818    |
|                           | [-0.193, -0.078] | [-0.284, -0.167] |
+---------------------------+------------------+------------------+
Table 2. Comparison between THP=always and THP=never

Metrics collected during each run are available at
https://github.com/ediworks/KernelPerf/tree/master/mglru/redis/5.15

Appendix
========
$ cat raw_data_redis.r
v <- c(
    # baseline THP=always sequential
    1.81, 1.81, 1.82, 1.84, 1.84, 1.84, 1.84, 1.85, 1.87, 1.88,
    # baseline THP=always random
    1.66, 1.67, 1.69, 1.69, 1.72, 1.75, 1.75, 1.77, 1.84, 1.88,
    # baseline THP=always Gaussian
    1.69, 1.70, 1.72, 1.76, 1.76, 1.76, 1.76, 1.78, 1.84, 1.94,
    # baseline THP=never sequential
    1.68, 1.68, 1.69, 1.69, 1.69, 1.69, 1.71, 1.72, 1.72, 1.75,
    # baseline THP=never random
    1.45, 1.45, 1.46, 1.47, 1.47, 1.47, 1.50, 1.53, 1.55, 1.58,
    # baseline THP=never Gaussian
    1.59, 1.60, 1.60, 1.60, 1.61, 1.63, 1.65, 1.66, 1.70, 1.71,
    # patched THP=always sequential
    1.79, 1.81, 1.85, 1.88, 1.90, 1.91, 1.96, 1.96, 1.96, 1.98,
    # patched THP=always random
    1.81, 1.86, 1.88, 1.89, 1.91, 1.94, 1.95, 1.96, 1.97, 2.09,
    # patched THP=always Gaussian
    1.95, 1.95, 1.98, 2.00, 2.04, 2.05, 2.08, 2.09, 2.12, 2.18,
    # patched THP=never sequential
    1.71, 1.73, 1.73, 1.74, 1.74, 1.74, 1.75, 1.75, 1.77, 1.77,
    # patched THP=never random
    1.65, 1.65, 1.65, 1.67, 1.68, 1.68, 1.69, 1.69, 1.71, 1.72,
    # patched THP=never Gaussian
    1.76, 1.76, 1.78, 1.81, 1.82, 1.83, 1.83, 1.84, 1.87, 1.88
)

a <- array(v, dim = c(10, 3, 2, 2))

# baseline vs patched
for (thp in 1:2) {
    for (dist in 1:3) {
        r <- t.test(a[, dist, thp, 1], a[, dist, thp, 2])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("thp%d dist%d: no significance", thp, dist)
        } else {
            s <- sprintf("thp%d dist%d: [%.2f, %.2f]%%", thp, dist, -p[2], -p[1])
        }
        print(s)
    }
}

# THP=always vs THP=never
for (kern in 1:2) {
    for (dist in 1:3) {
        r <- t.test(a[, dist, 1, kern], a[, dist, 2, kern])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("kern%d dist%d: no significance", kern, dist)
        } else {
            s <- sprintf("kern%d dist%d: [%.2f, %.2f]%%", kern, dist, -p[2], -p[1])
        }
        print(s)
    }
}

$ R -q -s -f raw_data_redis.r

        Welch Two Sample t-test

data:  a[, dist, thp, 1] and a[, dist, thp, 2]
t = -2.6773, df = 11.109, p-value = 0.02135
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.10926587 -0.01073413
sample estimates:
mean of x mean of y
     1.84      1.90

[1] "thp1 dist1: [0.58, 5.94]%"

        Welch Two Sample t-test

data:  a[, dist, thp, 1] and a[, dist, thp, 2]
t = -5.5311, df = 17.957, p-value = 3.011e-05
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.2539026 -0.1140974
sample estimates:
mean of x mean of y
    1.742     1.926

[1] "thp1 dist2: [6.55, 14.58]%"

        Welch Two Sample t-test

data:  a[, dist, thp, 1] and a[, dist, thp, 2]
t = -8.2093, df = 17.98, p-value = 1.707e-07
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.3428716 -0.2031284
sample estimates:
mean of x mean of y
    1.771     2.044

[1] "thp1 dist3: [11.47, 19.36]%"

        Welch Two Sample t-test

data:  a[, dist, thp, 1] and a[, dist, thp, 2]
t = -4.4705, df = 17.276, p-value = 0.0003243
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.06032607 -0.02167393
sample estimates:
mean of x mean of y
    1.702     1.743

[1] "thp2 dist1: [1.27, 3.54]%"

        Welch Two Sample t-test

data:  a[, dist, thp, 1] and a[, dist, thp, 2]
t = -11.366, df = 13.885, p-value = 2.038e-08
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.2211244 -0.1508756
sample estimates:
mean of x mean of y
    1.493     1.679

[1] "thp2 dist2: [10.11, 14.81]%"

        Welch Two Sample t-test

data:  a[, dist, thp, 1] and a[, dist, thp, 2]
t = -9.6138, df = 17.962, p-value = 1.663e-08
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.2229972 -0.1430028
sample estimates:
mean of x mean of y
    1.635     1.818

[1] "thp2 dist3: [8.75, 13.64]%"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 13.532, df = 17.988, p-value = 7.194e-11
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 0.1165737 0.1594263
sample estimates:
mean of x mean of y
    1.840     1.702

[1] "kern1 dist1: [-8.66, -6.34]%"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 9.197, df = 15.127, p-value = 1.386e-07
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 0.1913354 0.3066646
sample estimates:
mean of x mean of y
    1.742     1.493

[1] "kern1 dist2: [-17.60, -10.98]%"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 5.0552, df = 14.669, p-value = 0.0001523
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 0.07854452 0.19345548
sample estimates:
mean of x mean of y
    1.771     1.635

[1] "kern1 dist3: [-10.92, -4.44]%"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 7.1487, df = 10.334, p-value = 2.614e-05
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 0.1082788 0.2057212
sample estimates:
mean of x mean of y
    1.900     1.743

[1] "kern2 dist1: [-10.83, -5.70]%"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 9.7525, df = 10.871, p-value = 1.042e-06
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 0.1911754 0.3028246
sample estimates:
mean of x mean of y
    1.926     1.679

[1] "kern2 dist2: [-15.72, -9.93]%"

        Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 8.2831, df = 13.988, p-value = 9.168e-07
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 0.167476 0.284524
sample estimates:
mean of x mean of y
    2.044     1.818

[1] "kern2 dist3: [-13.92, -8.19]%"


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by smtp.lore.kernel.org (Postfix) with ESMTP id E02FEC433FE
	for <linux-kernel@archiver.kernel.org>; Tue,  4 Jan 2022 20:23:07 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S232437AbiADUXH (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Tue, 4 Jan 2022 15:23:07 -0500
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:51850 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S232134AbiADUXF (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Tue, 4 Jan 2022 15:23:05 -0500
Received: from mail-qt1-x84a.google.com (mail-qt1-x84a.google.com [IPv6:2607:f8b0:4864:20::84a])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id 1FD1CC061761
        for <linux-kernel@vger.kernel.org>; Tue,  4 Jan 2022 12:23:05 -0800 (PST)
Received: by mail-qt1-x84a.google.com with SMTP id h20-20020ac85e14000000b002b2e9555bb1so29122589qtx.3
        for <linux-kernel@vger.kernel.org>; Tue, 04 Jan 2022 12:23:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20210112;
        h=date:in-reply-to:message-id:mime-version:references:subject:from:to
         :cc;
        bh=K4J7Yr0q+JQeQzl29BR8JFO+xMbXu/b7E/RxxERPELU=;
        b=N3/ySZumcxnw7rpBpXCsdIzt3o8HaeNRURMM4QR/Lpt1YZ1j8Wd9xgyMT8IAeG2LVw
         OGA0VSWgZbZPCvTVFZp0PC12tkQMu7SLEEuRNTGrlElbQDDIsYAf4+3cfXUuZkhD1GEG
         fG+/xLWrHmnr/9OMY5M7OntExZEs0+yzYttMFWp3TDEN1cegZMctdfDVcsvRyExKTEks
         +kK31R+DVih4bgpzV9Vud9MhFfj+pLPTjX4YQt9IpfGrgaSZhkHU9GaGteGIAtY6QTeY
         ouHWbu8iyl09npkMCGo4WhkLqvYn28XUQz6gHctE1koUaH3nrshr26SDenoaX0JFXq8t
         qc3w==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:date:in-reply-to:message-id:mime-version
         :references:subject:from:to:cc;
        bh=K4J7Yr0q+JQeQzl29BR8JFO+xMbXu/b7E/RxxERPELU=;
        b=EBjYxr5L37JpploUX+Lv/afe+456+9UVe444tt34B8N+1mxAvDKjmcWzDAnsuVMxBn
         i5RboKGY7kaMlX7reITvirXeFm9SS8mKEyDo1SGa2efISRiePFqB4K/tv1sSB53aOW3R
         9mEFJ65ffuM6EotEGWkU7DE4a2zRtxv5VKLrl81M64p55NnrU7p9TjOwLyEvsKm/JItA
         x4V2OLfPf3JZB5cetOTNHJ4NcG59fCCV7LKbpKpOPM1lNe8nfHpK0VtVX6XyN7an958A
         f/adQj27MiYM3jrEmNbjuBle3ElyAkg47OHCEtaN3fG+DTqL+BDzMxkIgq0zpgJbyxi0
         4DzA==
X-Gm-Message-State: AOAM53231VT+60883cup4UWUzu0WOTpa4jVCZiTP9pGXvk+WTLcJwp2m
        /BXDU5cBfffEOlvC1hsbrFvqPlu63VY=
X-Google-Smtp-Source: ABdhPJxCUzW2TwXi9MjmaRV35vVSpBymkYeRr8p+MmjPSvtd4Rcm+muywSulMTI3BTbWBiOm5sCrTo1TXck=
X-Received: from yuzhao.bld.corp.google.com ([2620:15c:183:200:6c8c:5506:7ca2:9dfd])
 (user=yuzhao job=sendgmr) by 2002:a05:622a:a:: with SMTP id
 x10mr44195569qtw.516.1641327784231; Tue, 04 Jan 2022 12:23:04 -0800 (PST)
Date:   Tue,  4 Jan 2022 13:22:47 -0700
In-Reply-To: <20220104202227.2903605-1-yuzhao@google.com>
Message-Id: <20220104202247.2903702-1-yuzhao@google.com>
Mime-Version: 1.0
References: <20220104202227.2903605-1-yuzhao@google.com>
X-Mailer: git-send-email 2.34.1.448.ga2b2bfdf31-goog
Subject: Re: [PATCH v6 0/9] Multigenerational LRU Framework
From:   Yu Zhao <yuzhao@google.com>
To:     Andrew Morton <akpm@linux-foundation.org>,
        Linus Torvalds <torvalds@linux-foundation.org>
Cc:     Andi Kleen <ak@linux.intel.com>,
        Catalin Marinas <catalin.marinas@arm.com>,
        Dave Hansen <dave.hansen@linux.intel.com>,
        Hillf Danton <hdanton@sina.com>, Jens Axboe <axboe@kernel.dk>,
        Jesse Barnes <jsbarnes@google.com>,
        Johannes Weiner <hannes@cmpxchg.org>,
        Jonathan Corbet <corbet@lwn.net>,
        Matthew Wilcox <willy@infradead.org>,
        Mel Gorman <mgorman@suse.de>,
        Michael Larabel <Michael@michaellarabel.com>,
        Michal Hocko <mhocko@kernel.org>,
        Rik van Riel <riel@surriel.com>,
        Vlastimil Babka <vbabka@suse.cz>,
        Will Deacon <will@kernel.org>,
        Ying Huang <ying.huang@intel.com>,
        linux-arm-kernel@lists.infradead.org, linux-doc@vger.kernel.org,
        linux-kernel@vger.kernel.org, linux-mm@kvack.org,
        page-reclaim@google.com, x86@kernel.org
Content-Type: text/plain; charset="UTF-8"
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

[*] BENCHMARK
Name: fio_bench_hdd_mq
Description: FIO benchmark running against HDD multi-queue

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 10
  machine_total_logical_cores  | 72
  machine_total_physical_cores | 36
  machine_total_ram_gib        | 256

[*] METRICS
                              LABEL                             | COUNT |          MIN           |          MAX           |          MEAN          |         MEDIAN         |         STDDEV         |   DIRECTION
----------------------------------------------------------------+-------+------------------------+------------------------+------------------------+------------------------+------------------------+----------------
  hdd_bfq_randread_sync1_8k_bw_read                             |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 433.3917662682603      | 557.8477202301904      | 489.18242841063557     | 490.6666666666667      | 19.097252390047217     |
  (B) 84f99e060a66                                              | 481   | 426.431164231961       | 536.8888888888889      | 487.8554059336687      | 487.1111111111111      | 19.48371520112119      |
                                                                |       | -1.61%                 | -3.76%                 | -0.27%                 | -0.72%                 | +2.02%                 | + is good
  hdd_bfq_randread_sync1_8k_lat_read                            |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 1.4456787946614986e+07 | 1.9117502479907356e+07 | 1.645379955129552e+07  | 1.6380090804665854e+07 | 679101.4140279738      |
  (B) 84f99e060a66                                              | 481   | 1.4906508581318656e+07 | 1.9300071993749753e+07 | 1.6499377634492567e+07 | 1.6433589051097086e+07 | 689703.6846144461      |
                                                                |       | +3.11%                 | +0.95%                 | +0.28%                 | +0.33%                 | +1.56%                 | <not defined>
  hdd_bfq_randread_sync1_8k_onedisk_bw_read                     |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 446.764                | 768                    | 642.4415851769633      | 640                    | 49.16317155463473      |
  (B) 84f99e060a66                                              | 481   | 480                    | 826.9960159362549      | 641.5146399847603      | 640                    | 49.84433236262648      |
                                                                |       | +7.44%                 | +7.68%                 | -0.14%                 | +0.00%                 | +1.39%                 | + is good
  hdd_bfq_randread_sync1_8k_onedisk_lat_read                    |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 1.0446467379999999e+07 | 1.8519737817843866e+07 | 1.2503646313603505e+07 | 1.2448606008064516e+07 | 955886.7640578166      |
  (B) 84f99e060a66                                              | 481   | 9.912718333333334e+06  | 1.5913304476987448e+07 | 1.252581114876277e+07  | 1.2491253064257028e+07 | 911555.2614030009      |
                                                                |       | -5.11%                 | -14.07%                | +0.18%                 | +0.34%                 | -4.64%                 | <not defined>
  hdd_bfq_randread_sync1_8m_onedisk_bw_read                     |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 65405.593625498004     | 131072                 | 102073.76626549932     | 98304                  | 16325.722371561216     |
  (B) 84f99e060a66                                              | 481   | 65405.593625498004     | 131072                 | 101552.34142018206     | 98304                  | 16719.246285256806     |
                                                                |       | +0.00%                 | +0.00%                 | -0.51%                 | +0.00%                 | +2.41%                 | + is good
  hdd_bfq_randread_sync1_8m_onedisk_lat_read                    |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 6.4083031916230366e+07 | 1.0661207040092166e+08 | 8.072525879536723e+07  | 7.972267310126582e+07  | 7.477349032091799e+06  |
  (B) 84f99e060a66                                              | 481   | 6.2635378e+07          | 1.2953105246037737e+08 | 8.129236612411591e+07  | 8.028132555691057e+07  | 8.401679764088364e+06  |
                                                                |       | -2.26%                 | +21.50%                | +0.70%                 | +0.70%                 | +12.36%                | <not defined>
  hdd_bfq_randread_sync1_256k_bw_read                           |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 480   | 12281.12793271359      | 15132.444444444445     | 13802.442575918547     | 13767.111111111111     | 527.1238640248814      |
  (B) 84f99e060a66                                              | 480   | 11031.917662682603     | 15246.222222222223     | 13792.122426958833     | 13816.67994687915      | 562.7906225959111      |
                                                                |       | -10.17%                | +0.75%                 | -0.07%                 | +0.36%                 | +6.77%                 | + is good
  hdd_bfq_randread_sync1_256k_lat_read                          |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 1.7051231348338768e+07 | 2.150583715581118e+07  | 1.8638179679985933e+07 | 1.859534185701346e+07  | 717361.022579436       |
  (B) 84f99e060a66                                              | 481   | 1.6743855827803267e+07 | 2.2890306188082602e+07 | 1.8663653487930812e+07 | 1.8575175956296563e+07 | 792153.6139743543      |
                                                                |       | -1.80%                 | +6.44%                 | +0.14%                 | -0.11%                 | +10.43%                | <not defined>
  hdd_bfq_randread_sync1_prio_256k_antagonized_bw_read          |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 480   | 1251.4555112881806     | 5798.982735723771      | 3195.510963197429      | 3185.777777777778      | 656.6188770523272      |
  (B) 84f99e060a66                                              | 480   | 1535.4466578131917     | 6026.538291279327      | 3072.9427623541496     | 3068.9012837538735     | 649.9058467436236      |
                                                                |       | +22.69%                | +3.92%                 | -3.84%                 | -3.67%                 | -1.02%                 | <not defined>
  hdd_bfq_randread_sync1_prio_256k_antagonized_lat_read         |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 5.182094534557305e+07  | 1.8752826185425484e+08 | 8.760924727551427e+07  | 8.352359803888217e+07  | 1.977023914873974e+07  |
  (B) 84f99e060a66                                              | 481   | 4.868626716238696e+07  | 2.3129365653821653e+08 | 9.622444278248079e+07  | 9.149304520876768e+07  | 2.5072713157997187e+07 |
                                                                |       | -6.05%                 | +23.34%                | +9.83%                 | +9.54%                 | +26.82%                | - is good
  hdd_bfq_read_antagonist_bw_read                               |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 479   | 49141.06748148148      | 92770.28154050466      | 72409.54922214251      | 72788.78264718903      | 7631.977908722226      |
  (B) 84f99e060a66                                              | 477   | 40042.492714937165     | 94590.7180168216       | 69249.11870520914      | 69176.88888888889      | 9833.606205014574      |
                                                                |       | -18.52%                | +1.96%                 | -4.36%                 | -4.96%                 | +28.85%                | <not defined>
  hdd_bfq_read_antagonist_lat_read                              |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 9.835670096201618e+07  | 4.235589828743171e+08  | 1.5040176964768988e+08 | 1.383429005580708e+08  | 4.320986674287423e+07  |
  (B) 84f99e060a66                                              | 481   | 9.417358957137848e+07  | 6.887692560481511e+08  | 1.8459262277609617e+08 | 1.657244056006056e+08  | 7.933471967203863e+07  |
                                                                |       | -4.25%                 | +62.61%                | +22.73%                | +19.79%                | +83.60%                | <not defined>
  hdd_mq_deadline_randread_sync1_8k_bw_read                     |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 480   | 437.3333333333333      | 561.4457724656927      | 495.29163403792217     | 493.95263390880916     | 19.203905318609316     |
  (B) 84f99e060a66                                              | 480   | 433.2952633908809      | 550.3762726870295      | 494.36658941271946     | 494.22222222222223     | 20.681036661081237     |
                                                                |       | -0.92%                 | -1.97%                 | -0.19%                 | +0.05%                 | +7.69%                 | + is good
  hdd_mq_deadline_randread_sync1_8k_lat_read                    |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 1.4287019730821868e+07 | 1.854003493473249e+07  | 1.624403335564022e+07  | 1.6215042711709704e+07 | 660211.3697913568      |
  (B) 84f99e060a66                                              | 481   | 1.457414547219332e+07  | 1.987693170638872e+07  | 1.6277774871744607e+07 | 1.6237274064249396e+07 | 697562.5497383556      |
                                                                |       | +2.01%                 | +7.21%                 | +0.21%                 | +0.14%                 | +5.66%                 | <not defined>
  hdd_mq_deadline_randread_sync1_8k_onedisk_bw_read             |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 509.26693227091636     | 800                    | 645.1839843619289      | 640                    | 48.390152611566336     |
  (B) 84f99e060a66                                              | 481   | 541.394422310757       | 800                    | 642.3397967382033      | 640                    | 46.36162021875187      |
                                                                |       | +6.31%                 | +0.00%                 | -0.44%                 | +0.00%                 | -4.19%                 | + is good
  hdd_mq_deadline_randread_sync1_8k_onedisk_lat_read            |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 9.958389815261044e+06  | 1.5173071762295082e+07 | 1.2445965580579078e+07 | 1.2398376431451613e+07 | 854781.9443522815      |
  (B) 84f99e060a66                                              | 481   | 1.004605020746888e+07  | 1.4730739472118959e+07 | 1.2498833967873506e+07 | 1.2471198e+07          | 835410.5859617554      |
                                                                |       | +0.88%                 | -2.92%                 | +0.42%                 | +0.59%                 | -2.27%                 | <not defined>
  hdd_mq_deadline_randread_sync1_8m_onedisk_bw_read             |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 65404.552              | 131072                 | 101739.09871906973     | 98304                  | 17121.347317133757     |
  (B) 84f99e060a66                                              | 481   | 32768                  | 131072                 | 101071.1305132899      | 98304                  | 17759.554496706638     |
                                                                |       | -49.90%                | +0.00%                 | -0.66%                 | +0.00%                 | +3.73%                 | + is good
  hdd_mq_deadline_randread_sync1_8m_onedisk_lat_read            |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 6.42105164229249e+07   | 1.1846633073443983e+08 | 8.12111689093428e+07   | 7.994055613761468e+07  | 8.305693600908015e+06  |
  (B) 84f99e060a66                                              | 481   | 6.3309874666666664e+07 | 1.8769406183846155e+08 | 8.191909383006239e+07  | 8.013581253846154e+07  | 1.035309668574437e+07  |
                                                                |       | -1.40%                 | +58.44%                | +0.87%                 | +0.24%                 | +24.65%                | <not defined>
  hdd_mq_deadline_randread_sync1_256k_bw_read                   |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 12288                  | 15345.823815847722     | 13955.313638231552     | 13988.24789729969      | 531.5198182748803      |
  (B) 84f99e060a66                                              | 481   | 12726.653386454183     | 15912.910137228862     | 13987.495069078263     | 13989.14386896857      | 502.3380051071074      |
                                                                |       | +3.57%                 | +3.70%                 | +0.23%                 | +0.01%                 | -5.49%                 | + is good
  hdd_mq_deadline_randread_sync1_256k_lat_read                  |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 1.6587447357276253e+07 | 2.1053193505200885e+07 | 1.843522900376671e+07  | 1.8378568200036444e+07 | 712649.1729512813      |
  (B) 84f99e060a66                                              | 481   | 1.6634398856924305e+07 | 2.0707193470965516e+07 | 1.8385433302425563e+07 | 1.83216160420976e+07   | 652160.1536307137      |
                                                                |       | +0.28%                 | -1.64%                 | -0.27%                 | -0.31%                 | -8.49%                 | <not defined>
  hdd_mq_deadline_randread_sync1_prio_256k_antagonized_bw_read  |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 480   | 1251.3554670208057     | 4662.554227534307      | 2590.199788013022      | 2614.277556440903      | 467.11616745046547     |
  (B) 84f99e060a66                                              | 480   | 1024                   | 4206.53652058433       | 2563.1885636551333     | 2503.1111111111113     | 502.574450952279       |
                                                                |       | -18.17%                | -9.78%                 | -1.04%                 | -4.25%                 | +7.59%                 | <not defined>
  hdd_mq_deadline_randread_sync1_prio_256k_antagonized_lat_read |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 7.006917667153478e+07  | 2.0013257296266732e+08 | 1.0504890526408787e+08 | 1.0381339802055775e+08 | 1.6038319039515162e+07 |
  (B) 84f99e060a66                                              | 481   | 6.771219982498592e+07  | 2.700827838838999e+08  | 1.0682645729081483e+08 | 1.0264751235651374e+08 | 2.0996399340127923e+07 |
                                                                |       | -3.36%                 | +34.95%                | +1.69%                 | -1.12%                 | +30.91%                | - is good
  hdd_mq_deadline_read_antagonist_bw_read                       |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 480   | 54598.84373616645      | 101857.998229305       | 78569.87317444892      | 80041.66799468792      | 7010.904844771771      |
  (B) 84f99e060a66                                              | 480   | 47317.0290530051       | 98304                  | 76752.34087065593      | 76458.66666666667      | 9130.285131845274      |
                                                                |       | -13.34%                | -3.49%                 | -2.31%                 | -4.48%                 | +30.23%                | <not defined>
  hdd_mq_deadline_read_antagonist_lat_read                      |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 9.07430549319759e+07   | 2.0507063259337398e+08 | 1.1080574314078984e+08 | 1.0644658997988726e+08 | 1.6111792088606713e+07 |
  (B) 84f99e060a66                                              | 481   | 8.921996962268418e+07  | 2.059040225760676e+08  | 1.1478550705415063e+08 | 1.0669493362907396e+08 | 2.0720287473059524e+07 |
                                                                |       | -1.68%                 | +0.41%                 | +3.59%                 | +0.23%                 | +28.60%                | <not defined>
  hdd_mq_none_randread_sync1_8k_bw_read                         |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 480   | 433.58477202301907     | 547.5555555555555      | 495.651889187694       | 495.44444444444446     | 18.920574696942346     |
  (B) 84f99e060a66                                              | 480   | 408.8888888888889      | 568.5568835768039      | 495.5592125092227      | 495.5502434705622      | 19.850744247491154     |
                                                                |       | -5.70%                 | +3.84%                 | -0.02%                 | +0.02%                 | +4.92%                 | + is good
  hdd_mq_none_randread_sync1_8k_lat_read                        |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 1.4739365473777534e+07 | 1.8363069459155805e+07 | 1.6220706242792107e+07 | 1.6167072555843754e+07 | 624498.9705654499      |
  (B) 84f99e060a66                                              | 481   | 1.4144933475971542e+07 | 1.999434545073922e+07  | 1.6228009886758411e+07 | 1.6184862094213549e+07 | 655002.1366949107      |
                                                                |       | -4.03%                 | +8.88%                 | +0.05%                 | +0.11%                 | +4.88%                 | <not defined>
  hdd_mq_none_randread_sync1_8k_onedisk_bw_read                 |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 480                    | 768                    | 642.5459059230853      | 640                    | 48.349233074838665     |
  (B) 84f99e060a66                                              | 481   | 512                    | 864                    | 643.8444091409829      | 640                    | 48.39906828252791      |
                                                                |       | +6.67%                 | +12.50%                | +0.20%                 | +0.00%                 | +0.10%                 | + is good
  hdd_mq_none_randread_sync1_8k_onedisk_lat_read                |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 1.0255825841463415e+07 | 1.6170251333333334e+07 | 1.2493725390487112e+07 | 1.249267448e+07        | 873499.0003406552      |
  (B) 84f99e060a66                                              | 481   | 9.744641596153846e+06  | 1.55867633187251e+07   | 1.2472582618731404e+07 | 1.239832636437247e+07  | 890636.2226532341      |
                                                                |       | -4.98%                 | -3.61%                 | -0.17%                 | -0.76%                 | +1.96%                 | <not defined>
  hdd_mq_none_randread_sync1_8m_onedisk_bw_read                 |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 65405.593625498004     | 131072                 | 101828.76370165072     | 98304                  | 17081.367956301114     |
  (B) 84f99e060a66                                              | 481   | 65404.552              | 131072                 | 101917.20606140923     | 98304                  | 15646.78555952052      |
                                                                |       | -0.00%                 | +0.00%                 | +0.09%                 | +0.00%                 | -8.40%                 | + is good
  hdd_mq_none_randread_sync1_8m_onedisk_lat_read                |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 6.4492800356862746e+07 | 1.1046080762445414e+08 | 8.107383330775987e+07  | 8.005707593333334e+07  | 7.835925234954876e+06  |
  (B) 84f99e060a66                                              | 481   | 6.516480875897436e+07  | 1.0510505822274882e+08 | 8.078266977005598e+07  | 8.006642728971963e+07  | 7.525479082183226e+06  |
                                                                |       | +1.04%                 | -4.85%                 | -0.36%                 | +0.01%                 | -3.96%                 | <not defined>
  hdd_mq_none_randread_sync1_256k_bw_read                       |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 12395.359008410802     | 15466.46303674192      | 13965.324254696627     | 13986.445329791944     | 540.5537898230401      |
  (B) 84f99e060a66                                              | 480   | 12047.32802124834      | 15465.56706507304      | 14005.528092522502     | 13994.666666666666     | 533.422194380433       |
                                                                |       | -2.81%                 | -0.01%                 | +0.29%                 | +0.06%                 | -1.32%                 | + is good
  hdd_mq_none_randread_sync1_256k_lat_read                      |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 1.6759992100125207e+07 | 2.1025788906987265e+07 | 1.841854575650044e+07  | 1.8403686714389365e+07 | 707193.8393873073      |
  (B) 84f99e060a66                                              | 481   | 1.6703674210050877e+07 | 2.1005133391747385e+07 | 1.8354179024851043e+07 | 1.8299694568854973e+07 | 691484.2208106014      |
                                                                |       | -0.34%                 | -0.10%                 | -0.35%                 | -0.57%                 | -2.22%                 | <not defined>
  hdd_mq_none_randread_sync1_prio_256k_antagonized_bw_read      |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 1193.9154316069057     | 3404.9694555112883     | 2470.783381806479      | 2501.119079238601      | 352.37169323696077     |
  (B) 84f99e060a66                                              | 480   | 1251.5555555555557     | 3413.3333333333335     | 2462.8524977844877     | 2502.347056219566      | 353.6393090865356      |
                                                                |       | +4.83%                 | +0.25%                 | -0.32%                 | +0.05%                 | +0.36%                 | <not defined>
  hdd_mq_none_randread_sync1_prio_256k_antagonized_lat_read     |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 7.92758670781183e+07   | 3.6828972945182586e+08 | 1.2276833274554719e+08 | 1.1244387408005123e+08 | 3.580428866616608e+07  |
  (B) 84f99e060a66                                              | 481   | 8.165940581725675e+07  | 5.55477427650102e+08   | 1.2173782155950747e+08 | 1.1184941037184313e+08 | 3.943540948359279e+07  |
                                                                |       | +3.01%                 | +50.83%                | -0.84%                 | -0.53%                 | +10.14%                | - is good
  hdd_mq_none_read_antagonist_bw_read                           |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 480   | 50943.45728198318      | 98304                  | 77545.46814265505      | 76458.67463479415      | 7768.638181488058      |
  (B) 84f99e060a66                                              | 480   | 54584.33820274458      | 98304                  | 78673.59394682551      | 80041.66799468792      | 7623.4260348939215     |
                                                                |       | +7.15%                 | +0.00%                 | +1.45%                 | +4.69%                 | -1.87%                 | <not defined>
  hdd_mq_none_read_antagonist_lat_read                          |       |                        |                        |                        |                        |                        |
  (A) 4db0b9c9d0f3                                              | 481   | 9.16133477798651e+07   | 3.2367121380585676e+08 | 1.1565995741373444e+08 | 1.0929578126101434e+08 | 2.4595102484695204e+07 |
  (B) 84f99e060a66                                              | 481   | 9.223103197939356e+07  | 2.6360696231197134e+08 | 1.1001900110101567e+08 | 1.0668273801871714e+08 | 1.5779026561421016e+07 |
                                                                |       | +0.67%                 | -18.56%                | -4.88%                 | -2.39%                 | -35.84%                | <not defined>

[*] BENCHMARK
Name: lmbench
Description: lmbench is a suite of simple, portable, ANSI/C
microbenchmarks for UNIX/POSIX. In general, it measures two key
features (latency and bandwidth). lmbench is intended to give system
developers insight into basic costs of key operations.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 43
  machine_total_logical_cores  | 12
  machine_total_physical_cores | 6
  machine_total_ram_gib        | 32

[*] METRICS
                             LABEL                             | COUNT |    MIN    |    MAX    |   MEAN    |  MEDIAN   | STDDEV |   DIRECTION
---------------------------------------------------------------+-------+-----------+-----------+-----------+-----------+--------+----------------
  development_1_BCOPY_50__BCOPY_libc_bandwidth_MB_sec          |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 7805.47   | 7805.47   | 7805.47   | 7805.47   | 0      |
  (B) 84f99e060a66                                             | 1     | 7742.96   | 7742.96   | 7742.96   | 7742.96   | 0      |
                                                               |       | -0.80%    | -0.80%    | -0.80%    | -0.80%    | ---    | + is good
  development_1_BCOPY_50__BCOPY_memory_read_bandwidth_MB_sec   |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 8073.16   | 8073.16   | 8073.16   | 8073.16   | 0      |
  (B) 84f99e060a66                                             | 1     | 8256.44   | 8256.44   | 8256.44   | 8256.44   | 0      |
                                                               |       | +2.27%    | +2.27%    | +2.27%    | +2.27%    | ---    | + is good
  development_1_BCOPY_50__BCOPY_memory_write_bandwidth_MB_sec  |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 6493.75   | 6493.75   | 6493.75   | 6493.75   | 0      |
  (B) 84f99e060a66                                             | 1     | 6516.47   | 6516.47   | 6516.47   | 6516.47   | 0      |
                                                               |       | +0.35%    | +0.35%    | +0.35%    | +0.35%    | ---    | + is good
  development_1_BCOPY_50__BCOPY_unrolled_bandwidth_MB_sec      |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 4361.41   | 4361.41   | 4361.41   | 4361.41   | 0      |
  (B) 84f99e060a66                                             | 1     | 4395.6    | 4395.6    | 4395.6    | 4395.6    | 0      |
                                                               |       | +0.78%    | +0.78%    | +0.78%    | +0.78%    | ---    | + is good
  development_1_CONNECT_50__CONNECT_localhost_latency_us       |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 26.3817   | 26.3817   | 26.3817   | 26.3817   | 0      |
  (B) 84f99e060a66                                             | 1     | 24.6721   | 24.6721   | 24.6721   | 24.6721   | 0      |
                                                               |       | -6.48%    | -6.48%    | -6.48%    | -6.48%    | ---    | - is good
  development_1_CTX_50__CTX_96P_0K_latency_us                  |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 3.18      | 3.18      | 3.18      | 3.18      | 0      |
  (B) 84f99e060a66                                             | 1     | 3.26      | 3.26      | 3.26      | 3.26      | 0      |
                                                               |       | +2.52%    | +2.52%    | +2.52%    | +2.52%    | ---    | - is good
  development_1_CTX_50__CTX_96P_16K_latency_us                 |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 3.64      | 3.64      | 3.64      | 3.64      | 0      |
  (B) 84f99e060a66                                             | 1     | 3.71      | 3.71      | 3.71      | 3.71      | 0      |
                                                               |       | +1.92%    | +1.92%    | +1.92%    | +1.92%    | ---    | - is good
  development_1_CTX_50__CTX_96P_64K_latency_us                 |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 6         | 6         | 6         | 6         | 0      |
  (B) 84f99e060a66                                             | 1     | 6.49      | 6.49      | 6.49      | 6.49      | 0      |
                                                               |       | +8.17%    | +8.17%    | +8.17%    | +8.17%    | ---    | - is good
  development_1_FILE_50__FILE_read_bandwidth_MB_sec            |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 4977.81   | 4977.81   | 4977.81   | 4977.81   | 0      |
  (B) 84f99e060a66                                             | 1     | 4916.3    | 4916.3    | 4916.3    | 4916.3    | 0      |
                                                               |       | -1.24%    | -1.24%    | -1.24%    | -1.24%    | ---    | + is good
  development_1_MMAP_50__MMAP_read_bandwidth_MB_sec            |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 9510.52   | 9510.52   | 9510.52   | 9510.52   | 0      |
  (B) 84f99e060a66                                             | 1     | 9538.52   | 9538.52   | 9538.52   | 9538.52   | 0      |
                                                               |       | +0.29%    | +0.29%    | +0.29%    | +0.29%    | ---    | + is good
  development_1_MMAP_50__MMAP_read_open2close_bandwidth_MB_sec |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 4870.85   | 4870.85   | 4870.85   | 4870.85   | 0      |
  (B) 84f99e060a66                                             | 1     | 4841.19   | 4841.19   | 4841.19   | 4841.19   | 0      |
                                                               |       | -0.61%    | -0.61%    | -0.61%    | -0.61%    | ---    | + is good
  development_1_OPS_50__OPS_double_add_latency_ns              |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 1.4       | 1.4       | 1.4       | 1.4       | 0      |
  (B) 84f99e060a66                                             | 1     | 1.39      | 1.39      | 1.39      | 1.39      | 0      |
                                                               |       | -0.71%    | -0.71%    | -0.71%    | -0.71%    | ---    | - is good
  development_1_OPS_50__OPS_double_div_latency_ns              |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 6.58      | 6.58      | 6.58      | 6.58      | 0      |
  (B) 84f99e060a66                                             | 1     | 6.55      | 6.55      | 6.55      | 6.55      | 0      |
                                                               |       | -0.46%    | -0.46%    | -0.46%    | -0.46%    | ---    | - is good
  development_1_OPS_50__OPS_double_mul_latency_ns              |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 1.4       | 1.4       | 1.4       | 1.4       | 0      |
  (B) 84f99e060a66                                             | 1     | 1.39      | 1.39      | 1.39      | 1.39      | 0      |
                                                               |       | -0.71%    | -0.71%    | -0.71%    | -0.71%    | ---    | - is good
  development_1_OPS_50__OPS_float_add_latency_ns               |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 1.4       | 1.4       | 1.4       | 1.4       | 0      |
  (B) 84f99e060a66                                             | 1     | 1.39      | 1.39      | 1.39      | 1.39      | 0      |
                                                               |       | -0.71%    | -0.71%    | -0.71%    | -0.71%    | ---    | - is good
  development_1_OPS_50__OPS_float_div_latency_ns               |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 5.17      | 5.17      | 5.17      | 5.17      | 0      |
  (B) 84f99e060a66                                             | 1     | 5.15      | 5.15      | 5.15      | 5.15      | 0      |
                                                               |       | -0.39%    | -0.39%    | -0.39%    | -0.39%    | ---    | - is good
  development_1_OPS_50__OPS_float_mul_latency_ns               |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 1.4       | 1.4       | 1.4       | 1.4       | 0      |
  (B) 84f99e060a66                                             | 1     | 1.39      | 1.39      | 1.39      | 1.39      | 0      |
                                                               |       | -0.71%    | -0.71%    | -0.71%    | -0.71%    | ---    | - is good
  development_1_OPS_50__OPS_int64_bit_latency_ns               |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 0.32      | 0.32      | 0.32      | 0.32      | 0      |
  (B) 84f99e060a66                                             | 1     | 0.31      | 0.31      | 0.31      | 0.31      | 0      |
                                                               |       | -3.13%    | -3.13%    | -3.13%    | -3.13%    | ---    | - is good
  development_1_OPS_50__OPS_int64_div_latency_ns               |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 19.65     | 19.65     | 19.65     | 19.65     | 0      |
  (B) 84f99e060a66                                             | 1     | 19.54     | 19.54     | 19.54     | 19.54     | 0      |
                                                               |       | -0.56%    | -0.56%    | -0.56%    | -0.56%    | ---    | - is good
  development_1_OPS_50__OPS_int64_mul_latency_ns               |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 1.41      | 1.41      | 1.41      | 1.41      | 0      |
  (B) 84f99e060a66                                             | 1     | 1.41      | 1.41      | 1.41      | 1.41      | 0      |
                                                               |       | +0.00%    | +0.00%    | +0.00%    | +0.00%    | ---    | <not defined>
  development_1_OPS_50__OPS_integer_bit_latency_ns             |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 0.31      | 0.31      | 0.31      | 0.31      | 0      |
  (B) 84f99e060a66                                             | 1     | 0.31      | 0.31      | 0.31      | 0.31      | 0      |
                                                               |       | +0.00%    | +0.00%    | +0.00%    | +0.00%    | ---    | - is good
  development_1_OPS_50__OPS_integer_div_latency_ns             |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 10.97     | 10.97     | 10.97     | 10.97     | 0      |
  (B) 84f99e060a66                                             | 1     | 10.92     | 10.92     | 10.92     | 10.92     | 0      |
                                                               |       | -0.46%    | -0.46%    | -0.46%    | -0.46%    | ---    | - is good
  development_1_OPS_50__OPS_integer_mul_latency_ns             |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 1.44      | 1.44      | 1.44      | 1.44      | 0      |
  (B) 84f99e060a66                                             | 1     | 1.44      | 1.44      | 1.44      | 1.44      | 0      |
                                                               |       | +0.00%    | +0.00%    | +0.00%    | +0.00%    | ---    | - is good
  development_1_PAGEFAULT_50__Pagefaults_ms                    |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 0.4384    | 0.4384    | 0.4384    | 0.4384    | 0      |
  (B) 84f99e060a66                                             | 1     | 0.4584    | 0.4584    | 0.4584    | 0.4584    | 0      |
                                                               |       | +4.56%    | +4.56%    | +4.56%    | +4.56%    | ---    | - is good
  development_1_PIPE_50__PIPE_bandwidth_MB_sec                 |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 2236.41   | 2236.41   | 2236.41   | 2236.41   | 0      |
  (B) 84f99e060a66                                             | 1     | 2230.42   | 2230.42   | 2230.42   | 2230.42   | 0      |
                                                               |       | -0.27%    | -0.27%    | -0.27%    | -0.27%    | ---    | + is good
  development_1_PIPE_50__PIPE_latency_us                       |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 7.646     | 7.646     | 7.646     | 7.646     | 0      |
  (B) 84f99e060a66                                             | 1     | 7.7923    | 7.7923    | 7.7923    | 7.7923    | 0      |
                                                               |       | +1.91%    | +1.91%    | +1.91%    | +1.91%    | ---    | - is good
  development_1_PROC_50__Process_fork__bin_sh_latency_us       |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 1403.875  | 1403.875  | 1403.875  | 1403.875  | 0      |
  (B) 84f99e060a66                                             | 1     | 1408.8684 | 1408.8684 | 1408.8684 | 1408.8684 | 0      |
                                                               |       | +0.36%    | +0.36%    | +0.36%    | +0.36%    | ---    | - is good
  development_1_PROC_50__Process_fork_execve_latency_us        |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 629.3372  | 629.3372  | 629.3372  | 629.3372  | 0      |
  (B) 84f99e060a66                                             | 1     | 634.5765  | 634.5765  | 634.5765  | 634.5765  | 0      |
                                                               |       | +0.83%    | +0.83%    | +0.83%    | +0.83%    | ---    | - is good
  development_1_PROC_50__Process_fork_exit_latency_us          |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 209.3123  | 209.3123  | 209.3123  | 209.3123  | 0      |
  (B) 84f99e060a66                                             | 1     | 204.6007  | 204.6007  | 204.6007  | 204.6007  | 0      |
                                                               |       | -2.25%    | -2.25%    | -2.25%    | -2.25%    | ---    | - is good
  development_1_SELECT_50__Select_100fd_latency_us             |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 2.5596    | 2.5596    | 2.5596    | 2.5596    | 0      |
  (B) 84f99e060a66                                             | 1     | 2.641     | 2.641     | 2.641     | 2.641     | 0      |
                                                               |       | +3.18%    | +3.18%    | +3.18%    | +3.18%    | ---    | - is good
  development_1_SELECT_50__Select_100tcp_latency_us            |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 4.5837    | 4.5837    | 4.5837    | 4.5837    | 0      |
  (B) 84f99e060a66                                             | 1     | 4.7101    | 4.7101    | 4.7101    | 4.7101    | 0      |
                                                               |       | +2.76%    | +2.76%    | +2.76%    | +2.76%    | ---    | - is good
  development_1_SYSCALL_50__syscall_fstat_latency_us           |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 0.8384    | 0.8384    | 0.8384    | 0.8384    | 0      |
  (B) 84f99e060a66                                             | 1     | 0.8248    | 0.8248    | 0.8248    | 0.8248    | 0      |
                                                               |       | -1.62%    | -1.62%    | -1.62%    | -1.62%    | ---    | - is good
  development_1_SYSCALL_50__syscall_open_close_latency_us      |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 3.7714    | 3.7714    | 3.7714    | 3.7714    | 0      |
  (B) 84f99e060a66                                             | 1     | 3.7521    | 3.7521    | 3.7521    | 3.7521    | 0      |
                                                               |       | -0.51%    | -0.51%    | -0.51%    | -0.51%    | ---    | - is good
  development_1_SYSCALL_50__syscall_read_latency_us            |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 0.6679    | 0.6679    | 0.6679    | 0.6679    | 0      |
  (B) 84f99e060a66                                             | 1     | 0.6671    | 0.6671    | 0.6671    | 0.6671    | 0      |
                                                               |       | -0.12%    | -0.12%    | -0.12%    | -0.12%    | ---    | - is good
  development_1_SYSCALL_50__syscall_stat_latency_us            |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 1.5253    | 1.5253    | 1.5253    | 1.5253    | 0      |
  (B) 84f99e060a66                                             | 1     | 1.5942    | 1.5942    | 1.5942    | 1.5942    | 0      |
                                                               |       | +4.52%    | +4.52%    | +4.52%    | +4.52%    | ---    | - is good
  development_1_SYSCALL_50__syscall_syscall_latency_us         |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 0.6118    | 0.6118    | 0.6118    | 0.6118    | 0      |
  (B) 84f99e060a66                                             | 1     | 0.6114    | 0.6114    | 0.6114    | 0.6114    | 0      |
                                                               |       | -0.07%    | -0.07%    | -0.07%    | -0.07%    | ---    | - is good
  development_1_SYSCALL_50__syscall_write_latency_us           |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 0.6313    | 0.6313    | 0.6313    | 0.6313    | 0      |
  (B) 84f99e060a66                                             | 1     | 0.6292    | 0.6292    | 0.6292    | 0.6292    | 0      |
                                                               |       | -0.33%    | -0.33%    | -0.33%    | -0.33%    | ---    | - is good
  development_1_TCP_50__TCP_localhost_latency                  |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 14.7063   | 14.7063   | 14.7063   | 14.7063   | 0      |
  (B) 84f99e060a66                                             | 1     | 15.0935   | 15.0935   | 15.0935   | 15.0935   | 0      |
                                                               |       | +2.63%    | +2.63%    | +2.63%    | +2.63%    | ---    | - is good
  development_1_TCP_50__TCP_socket_bandwidth_10MB_MB_sec       |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 4387.19   | 4387.19   | 4387.19   | 4387.19   | 0      |
  (B) 84f99e060a66                                             | 1     | 4435.23   | 4435.23   | 4435.23   | 4435.23   | 0      |
                                                               |       | +1.10%    | +1.10%    | +1.10%    | +1.10%    | ---    | + is good
  development_1_TCP_50__TCP_socket_bandwidth_64B_MB_sec        |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 63.26     | 63.26     | 63.26     | 63.26     | 0      |
  (B) 84f99e060a66                                             | 1     | 66.65     | 66.65     | 66.65     | 66.65     | 0      |
                                                               |       | +5.36%    | +5.36%    | +5.36%    | +5.36%    | ---    | + is good
  development_1_UDP_50__UDP_usinglocalhost_latency_us          |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 11.5949   | 11.5949   | 11.5949   | 11.5949   | 0      |
  (B) 84f99e060a66                                             | 1     | 11.5451   | 11.5451   | 11.5451   | 11.5451   | 0      |
                                                               |       | -0.43%    | -0.43%    | -0.43%    | -0.43%    | ---    | - is good
  development_1_UNIX_50__AF_UNIX_sock_stream_bandwidth_MB_sec  |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 5700.99   | 5700.99   | 5700.99   | 5700.99   | 0      |
  (B) 84f99e060a66                                             | 1     | 5722.12   | 5722.12   | 5722.12   | 5722.12   | 0      |
                                                               |       | +0.37%    | +0.37%    | +0.37%    | +0.37%    | ---    | + is good
  development_1_UNIX_50__AF_UNIX_sock_stream_latency_us        |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3                                             | 1     | 12.9419   | 12.9419   | 12.9419   | 12.9419   | 0      |
  (B) 84f99e060a66                                             | 1     | 12.8052   | 12.8052   | 12.8052   | 12.8052   | 0      |
                                                               |       | -1.06%    | -1.06%    | -1.06%    | -1.06%    | ---    | - is good

[*] BENCHMARK
Name: parallelio
Description: parallelio measures the impact of IO on the performance
of an in-memory workload. The primary worklod in-memory workload. The
primary workload is a memcached server with a memcachetest client
tuned to use 80% of memory. The test runs multiple times starting with
no parallel IO andthen doing IO with dd to a file sized to be  10% of
physical memory and finishing with a file 80% of physical memory. The
metrics of the test are how much performance is lost by running the IO
in parallel and checking if it prematurely pushed to swap. A positive
swapout figure is not necessarily bad but high swaptotals imply that
the in-memory workload is thrashing.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 10
  machine_total_logical_cores  | 72
  machine_total_physical_cores | 36
  machine_total_ram_gib        | 256

[*] METRICS
              LABEL              | COUNT |  MIN   |  MAX   |       MEAN        | MEDIAN |       STDDEV       |   DIRECTION
---------------------------------+-------+--------+--------+-------------------+--------+--------------------+----------------
  parallelio_memcachetest_0M     |       |        |        |                   |        |                    |
  (A) 4db0b9c9d0f3               | 3     | 405274 | 414589 | 409674            | 409159 | 3820.2290507245766 |
  (B) 84f99e060a66               | 3     | 408722 | 410821 | 409731            | 409650 | 858.825166530806   |
                                 |       | +0.85% | -0.91% | +0.01%            | +0.12% | -77.52%            | <not defined>
  parallelio_memcachetest_25784M |       |        |        |                   |        |                    |
  (A) 4db0b9c9d0f3               | 3     | 408843 | 414268 | 411967.6666666667 | 412792 | 2290.167436867645  |
  (B) 84f99e060a66               | 1     | 410109 | 410109 | 410109            | 410109 | 0                  |
                                 |       | +0.31% | -1.00% | -0.45%            | -0.65% | -100.00%           | <not defined>
  parallelio_memcachetest_maxM   |       |        |        |                   |        |                    |
  (A) 4db0b9c9d0f3               | 3     | 403039 | 407154 | 405639.3333333333 | 406725 | 1847.0355227288503 |
  (B) 84f99e060a66               | 1     | 410109 | 410109 | 410109            | 410109 | 0                  |
                                 |       | +1.75% | +0.73% | +1.10%            | +0.83% | -100.00%           | + is good
  parallelio_memcachetest_minM   |       |        |        |                   |        |                    |
  (A) 4db0b9c9d0f3               | 3     | 405274 | 414589 | 409674            | 409159 | 3820.2290507245766 |
  (B) 84f99e060a66               | 3     | 408722 | 410821 | 409731            | 409650 | 858.825166530806   |
                                 |       | +0.85% | -0.91% | +0.01%            | +0.12% | -77.52%            | + is good

[*] BENCHMARK
Name: postmark
Description: PostMark is a benchmark that demonstrates system
performance for short-lived small files seen typically in Internet
applications such as electronic mail, netnews and web-based commerce.
The benchmark creates an initial pool of random text files of varying
size. Once the pool has been created, the benchmark performs two types
of transactions on the pool: (i) create or delete a file; (ii) read
from or append to a file. The incidence of each transaction and its
subtype are chosen randomly to eliminate the effect of caching and
read-ahead.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 43
  machine_total_logical_cores  | 12
  machine_total_physical_cores | 6
  machine_total_ram_gib        | 32

[*] METRICS
           LABEL          | COUNT |   MIN   |   MAX   |  MEAN   | MEDIAN  | STDDEV | DIRECTION
--------------------------+-------+---------+---------+---------+---------+--------+------------
  postmark_CreateTransact |       |         |         |         |         |        |
  (A) 4db0b9c9d0f3        | 1     | 12      | 12      | 12      | 12      | 0      |
  (B) 84f99e060a66        | 1     | 12      | 12      | 12      | 12      | 0      |
                          |       | +0.00%  | +0.00%  | +0.00%  | +0.00%  | ---    | + is good
  postmark_DataRead_MB    |       |         |         |         |         |        |
  (A) 4db0b9c9d0f3        | 1     | 20.61   | 20.61   | 20.61   | 20.61   | 0      |
  (B) 84f99e060a66        | 1     | 19.84   | 19.84   | 19.84   | 19.84   | 0      |
                          |       | -3.74%  | -3.74%  | -3.74%  | -3.74%  | ---    | + is good
  postmark_DataWrite_MB   |       |         |         |         |         |        |
  (A) 4db0b9c9d0f3        | 1     | 57.81   | 57.81   | 57.81   | 57.81   | 0      |
  (B) 84f99e060a66        | 1     | 55.66   | 55.66   | 55.66   | 55.66   | 0      |
                          |       | -3.72%  | -3.72%  | -3.72%  | -3.72%  | ---    | + is good
  postmark_DeleteTransact |       |         |         |         |         |        |
  (A) 4db0b9c9d0f3        | 1     | 12      | 12      | 12      | 12      | 0      |
  (B) 84f99e060a66        | 1     | 12      | 12      | 12      | 12      | 0      |
                          |       | +0.00%  | +0.00%  | +0.00%  | +0.00%  | ---    | + is good
  postmark_FilesCreate    |       |         |         |         |         |        |
  (A) 4db0b9c9d0f3        | 1     | 31      | 31      | 31      | 31      | 0      |
  (B) 84f99e060a66        | 1     | 29      | 29      | 29      | 29      | 0      |
                          |       | -6.45%  | -6.45%  | -6.45%  | -6.45%  | ---    | + is good
  postmark_FilesDeleted   |       |         |         |         |         |        |
  (A) 4db0b9c9d0f3        | 1     | 3095    | 3095    | 3095    | 3095    | 0      |
  (B) 84f99e060a66        | 1     | 4127    | 4127    | 4127    | 4127    | 0      |
                          |       | +33.34% | +33.34% | +33.34% | +33.34% | ---    | + is good
  postmark_Transactions   |       |         |         |         |         |        |
  (A) 4db0b9c9d0f3        | 1     | 24      | 24      | 24      | 24      | 0      |
  (B) 84f99e060a66        | 1     | 24      | 24      | 24      | 24      | 0      |
                          |       | +0.00%  | +0.00%  | +0.00%  | +0.00%  | ---    | + is good

[*] BENCHMARK
Name: hackbench
Description: Hackbench is both a benchmark and a stress test for the
Linux kernel scheduler. It's main job  is  to  create a specified
number of pairs of schedulable entities (either threads or traditional
processes) which communicate via either sockets or pipes and time how
long it takes for each pair to send data back and forth.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 0
  machine_total_logical_cores  | 112
  machine_total_physical_cores | 56
  machine_total_ram_gib        | 384

[*] METRICS
              LABEL             | COUNT |   MIN   |   MAX   |        MEAN        | MEDIAN |       STDDEV        |   DIRECTION
--------------------------------+-------+---------+---------+--------------------+--------+---------------------+----------------
  hackbench_process_pipes_234   |       |         |         |                    |        |                     |
  (A) 4db0b9c9d0f3              | 7     | 3.052   | 3.773   | 3.3674285714285714 | 3.329  | 0.285106925878123   |
  (B) 84f99e060a66              | 7     | 2.998   | 3.359   | 3.1277142857142857 | 3.074  | 0.11753262682058513 |
                                |       | -1.77%  | -10.97% | -7.12%             | -7.66% | -58.78%             | <not defined>
  hackbench_process_pipes_max   |       |         |         |                    |        |                     |
  (A) 4db0b9c9d0f3              | 7     | 3.052   | 3.773   | 3.3674285714285714 | 3.329  | 0.285106925878123   |
  (B) 84f99e060a66              | 7     | 2.998   | 3.359   | 3.1277142857142857 | 3.074  | 0.11753262682058513 |
                                |       | -1.77%  | -10.97% | -7.12%             | -7.66% | -58.78%             | - is good
  hackbench_process_sockets_234 |       |         |         |                    |        |                     |
  (A) 4db0b9c9d0f3              | 7     | 6.156   | 6.36    | 6.284571428571429  | 6.285  | 0.06349771006637552 |
  (B) 84f99e060a66              | 7     | 6.194   | 6.806   | 6.408428571428572  | 6.335  | 0.20896606501183543 |
                                |       | +0.62%  | +7.01%  | +1.97%             | +0.80% | +229.09%            | <not defined>
  hackbench_process_sockets_max |       |         |         |                    |        |                     |
  (A) 4db0b9c9d0f3              | 7     | 6.156   | 6.36    | 6.284571428571429  | 6.285  | 0.06349771006637552 |
  (B) 84f99e060a66              | 7     | 6.194   | 6.806   | 6.408428571428572  | 6.335  | 0.20896606501183543 |
                                |       | +0.62%  | +7.01%  | +1.97%             | +0.80% | +229.09%            | - is good
  hackbench_thread_pipes_234    |       |         |         |                    |        |                     |
  (A) 4db0b9c9d0f3              | 7     | 4.222   | 4.651   | 4.377857142857143  | 4.361  | 0.14447879584554812 |
  (B) 84f99e060a66              | 7     | 3.71    | 4.328   | 4.072              | 4.105  | 0.20410991436688505 |
                                |       | -12.13% | -6.94%  | -6.99%             | -5.87% | +41.27%             | <not defined>
  hackbench_thread_pipes_max    |       |         |         |                    |        |                     |
  (A) 4db0b9c9d0f3              | 7     | 4.222   | 4.651   | 4.377857142857143  | 4.361  | 0.14447879584554812 |
  (B) 84f99e060a66              | 7     | 3.71    | 4.328   | 4.072              | 4.105  | 0.20410991436688505 |
                                |       | -12.13% | -6.94%  | -6.99%             | -5.87% | +41.27%             | - is good
  hackbench_thread_sockets_234  |       |         |         |                    |        |                     |
  (A) 4db0b9c9d0f3              | 7     | 6.163   | 6.274   | 6.194857142857143  | 6.18   | 0.03647867702577801 |
  (B) 84f99e060a66              | 7     | 5.993   | 6.286   | 6.166857142857143  | 6.235  | 0.10362766945922779 |
                                |       | -2.76%  | +0.19%  | -0.45%             | +0.89% | +184.08%            | <not defined>
  hackbench_thread_sockets_max  |       |         |         |                    |        |                     |
  (A) 4db0b9c9d0f3              | 7     | 6.163   | 6.274   | 6.194857142857143  | 6.18   | 0.03647867702577801 |
  (B) 84f99e060a66              | 7     | 5.993   | 6.286   | 6.166857142857143  | 6.235  | 0.10362766945922779 |
                                |       | -2.76%  | +0.19%  | -0.45%             | +0.89% | +184.08%            | - is good

[*] BENCHMARK
Name: kernbench
Description: kernbench is a simple average of five kernel compiles of
the vmlinuxbinary image. kernbench checks out and builds v5.9.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 0
  machine_total_logical_cores  | 112
  machine_total_physical_cores | 56
  machine_total_ram_gib        | 384

[*] METRICS
        LABEL        | COUNT |   MIN   |   MAX   |        MEAN        | MEDIAN  |       STDDEV       |   DIRECTION
---------------------+-------+---------+---------+--------------------+---------+--------------------+----------------
  kernbench_elsp_112 |       |         |         |                    |         |                    |
  (A) 4db0b9c9d0f3   | 5     | 51.13   | 53.51   | 52.581999999999994 | 52.64   | 0.7946672259505867 |
  (B) 84f99e060a66   | 5     | 51.52   | 53.22   | 52.128             | 52.08   | 0.6266865245080647 |
                     |       | +0.76%  | -0.54%  | -0.86%             | -1.06%  | -21.14%            | <not defined>
  kernbench_elsp_max |       |         |         |                    |         |                    |
  (A) 4db0b9c9d0f3   | 5     | 51.13   | 53.51   | 52.581999999999994 | 52.64   | 0.7946672259505867 |
  (B) 84f99e060a66   | 5     | 51.52   | 53.22   | 52.128             | 52.08   | 0.6266865245080647 |
                     |       | +0.76%  | -0.54%  | -0.86%             | -1.06%  | -21.14%            | - is good
  kernbench_syst_112 |       |         |         |                    |         |                    |
  (A) 4db0b9c9d0f3   | 5     | 478.47  | 498.33  | 493.546            | 496.52  | 7.582455011406262  |
  (B) 84f99e060a66   | 5     | 479.37  | 496.39  | 491.84200000000004 | 494.62  | 6.357538517382332  |
                     |       | +0.19%  | -0.39%  | -0.35%             | -0.38%  | -16.15%            | <not defined>
  kernbench_syst_max |       |         |         |                    |         |                    |
  (A) 4db0b9c9d0f3   | 5     | 478.47  | 498.33  | 493.546            | 496.52  | 7.582455011406262  |
  (B) 84f99e060a66   | 5     | 479.37  | 496.39  | 491.84200000000004 | 494.62  | 6.357538517382332  |
                     |       | +0.19%  | -0.39%  | -0.35%             | -0.38%  | -16.15%            | - is good
  kernbench_user_112 |       |         |         |                    |         |                    |
  (A) 4db0b9c9d0f3   | 5     | 3086.46 | 3091.98 | 3088.51            | 3086.97 | 2.2000000000000863 |
  (B) 84f99e060a66   | 5     | 3085.06 | 3092.73 | 3089.646           | 3090.99 | 2.9837466380374402 |
                     |       | -0.05%  | +0.02%  | +0.04%             | +0.13%  | +35.62%            | <not defined>
  kernbench_user_max |       |         |         |                    |         |                    |
  (A) 4db0b9c9d0f3   | 5     | 3086.46 | 3091.98 | 3088.51            | 3086.97 | 2.2000000000000863 |
  (B) 84f99e060a66   | 5     | 3085.06 | 3092.73 | 3089.646           | 3090.99 | 2.9837466380374402 |
                     |       | -0.05%  | +0.02%  | +0.04%             | +0.13%  | +35.62%            | - is good

[*] BENCHMARK
Name: memcached
Description: memcached benchmark drived by mcperf.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 0
  machine_total_logical_cores  | 256
  machine_total_physical_cores | 128
  machine_total_ram_gib        | 1024

[*] METRICS
       LABEL       | COUNT |      MIN      |      MAX      |          MEAN          |    MEDIAN     |      STDDEV       | DIRECTION
-------------------+-------+---------------+---------------+------------------------+---------------+-------------------+------------
  QPS              |       |               |               |                        |               |                   |
  (A) 4db0b9c9d0f3 | 5     | 1.0583485e+06 | 1.0695842e+06 | 1.0632277999999998e+06 | 1.0616673e+06 | 3820.724428691447 |
  (B) 84f99e060a66 | 5     | 1.0654889e+06 | 1.0724847e+06 | 1.06819692e+06         | 1.0683428e+06 | 2454.762893152803 |
                   |       | +0.67%        | +0.27%        | +0.47%                 | +0.63%        | -35.75%           | + is good

[*] BENCHMARK
Name: multichase
Description: multichase is a pointer chaser microbenchmark.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 0
  machine_total_logical_cores  | 256
  machine_total_physical_cores | 128
  machine_total_ram_gib        | 1024

[*] METRICS
                       LABEL                      | COUNT |  MIN   |  MAX   |        MEAN        | MEDIAN |       STDDEV        | DIRECTION
--------------------------------------------------+-------+--------+--------+--------------------+--------+---------------------+------------
  best_ns_per_chase_4_thread_local_2m             |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 100.9  | 101.2  | 101                | 100.9  | 0.12649110640673303 |
  (B) 84f99e060a66                                | 5     | 101    | 101.2  | 101.14             | 101.2  | 0.08000000000000318 |
                                                  |       | +0.10% | +0.00% | +0.14%             | +0.30% | -36.75%             | - is good
  best_ns_per_chase_4_thread_local_4k             |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 103.3  | 103.5  | 103.4              | 103.4  | 0.06324555320336923 |
  (B) 84f99e060a66                                | 5     | 103.3  | 103.5  | 103.4              | 103.4  | 0.06324555320336793 |
                                                  |       | +0.00% | +0.00% | +0.00%             | +0.00% | -0.00%              | - is good
  best_ns_per_chase_all_cores                     |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 388.7  | 389.8  | 389.22             | 389.2  | 0.43081318457077317 |
  (B) 84f99e060a66                                | 5     | 388.6  | 389.4  | 388.98             | 389    | 0.2993325909419067  |
                                                  |       | -0.03% | -0.10% | -0.06%             | -0.05% | -30.52%             | - is good
  best_ns_per_chase_all_cores_interleaved         |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 321.7  | 322.8  | 322.26             | 322.3  | 0.36110940170536426 |
  (B) 84f99e060a66                                | 5     | 322.2  | 323.3  | 322.61999999999995 | 322.5  | 0.3762977544445408  |
                                                  |       | +0.16% | +0.15% | +0.11%             | +0.06% | +4.21%              | - is good
  best_ns_per_chase_all_thread_local_2m           |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 172.3  | 211    | 191.51999999999998 | 188.2  | 13.47136221768238   |
  (B) 84f99e060a66                                | 5     | 173.3  | 206.7  | 193.18             | 193.1  | 11.561729974359364  |
                                                  |       | +0.58% | -2.04% | +0.87%             | +2.60% | -14.18%             | - is good
  best_ns_per_chase_all_thread_local_4k           |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 181    | 187.2  | 183.62             | 182.7  | 2.609521028848008   |
  (B) 84f99e060a66                                | 5     | 179    | 184    | 182.38             | 182.7  | 1.8148278155241067  |
                                                  |       | -1.10% | -1.71% | -0.68%             | +0.00% | -30.45%             | - is good
  best_ns_per_chase_one_socket_local              |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 173.3  | 173.8  | 173.56             | 173.6  | 0.1854723699099109  |
  (B) 84f99e060a66                                | 5     | 173.6  | 173.7  | 173.64             | 173.6  | 0.04898979485566097 |
                                                  |       | +0.17% | -0.06% | +0.05%             | +0.00% | -73.59%             | - is good
  best_ns_per_chase_one_socket_remote             |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 254.4  | 255    | 254.57999999999998 | 254.5  | 0.21354156504062177 |
  (B) 84f99e060a66                                | 5     | 254.2  | 254.9  | 254.48000000000002 | 254.4  | 0.27856776554368984 |
                                                  |       | -0.08% | -0.04% | -0.04%             | -0.04% | +30.45%             | - is good
  best_ns_per_chase_single_thread_local_0         |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 142.8  | 144.8  | 144.20000000000002 | 144.6  | 0.7589466384404142  |
  (B) 84f99e060a66                                | 5     | 143.3  | 145.1  | 144.12             | 144    | 0.6554387843269566  |
                                                  |       | +0.35% | +0.21% | -0.06%             | -0.41% | -13.64%             | - is good
  best_ns_per_chase_single_thread_local_1         |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 142.8  | 143.9  | 143.48             | 143.6  | 0.3762977544445269  |
  (B) 84f99e060a66                                | 5     | 141.7  | 144.4  | 142.84             | 142.3  | 1.098362417419681   |
                                                  |       | -0.77% | +0.35% | -0.45%             | -0.91% | +191.89%            | - is good
  best_ns_per_chase_single_thread_local_2m        |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 69.006 | 69.578 | 69.399             | 69.466 | 0.2022117701816582  |
  (B) 84f99e060a66                                | 5     | 69.506 | 69.838 | 69.64559999999999  | 69.657 | 0.11971399249879164 |
                                                  |       | +0.72% | +0.37% | +0.36%             | +0.27% | -40.80%             | - is good
  best_ns_per_chase_single_thread_local_4k        |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 69.949 | 70.246 | 70.0564            | 69.999 | 0.11009741141371474 |
  (B) 84f99e060a66                                | 5     | 69.954 | 70.332 | 70.104             | 70.103 | 0.127741927337898   |
                                                  |       | +0.01% | +0.12% | +0.07%             | +0.15% | +16.03%             | - is good
  best_ns_per_chase_single_thread_local_50g_2m    |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 111.6  | 111.7  | 111.66000000000001 | 111.7  | 0.0489897948556674  |
  (B) 84f99e060a66                                | 5     | 111    | 111.6  | 111.42             | 111.5  | 0.22271057451320025 |
                                                  |       | -0.54% | -0.09% | -0.21%             | -0.18% | +354.61%            | - is good
  best_ns_per_chase_single_thread_local_50g_4k    |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 141.9  | 144.2  | 142.9              | 142.9  | 0.8173126696681018  |
  (B) 84f99e060a66                                | 5     | 141.7  | 144.5  | 143.16             | 143.4  | 0.9134549797335416  |
                                                  |       | -0.14% | +0.21% | +0.18%             | +0.35% | +11.76%             | - is good
  best_ns_per_chase_single_thread_remote          |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 248.8  | 256.6  | 252.98000000000002 | 254.4  | 3.3695103501844326  |
  (B) 84f99e060a66                                | 5     | 247.5  | 259.4  | 252.45999999999998 | 252.3  | 4.070675619599272   |
                                                  |       | -0.52% | +1.09% | -0.21%             | -0.83% | +20.81%             | - is good
  best_ns_per_chase_single_thread_remote_reversed |       |        |        |                    |        |                     |
  (A) 4db0b9c9d0f3                                | 5     | 268.3  | 272.1  | 270.76000000000005 | 271    | 1.3124023773218374  |
  (B) 84f99e060a66                                | 5     | 251.7  | 270.8  | 265.72             | 268.4  | 7.135656942426554   |
                                                  |       | -6.19% | -0.48% | -1.86%             | -0.96% | +443.71%            | - is good

[*] BENCHMARK
Name: nginx
Description: nginx web server benchmark.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 0
  machine_total_logical_cores  | 256
  machine_total_physical_cores | 128
  machine_total_ram_gib        | 1024

[*] METRICS
       LABEL       | COUNT |    MIN    |    MAX    |   MEAN    |  MEDIAN   | STDDEV | DIRECTION
-------------------+-------+-----------+-----------+-----------+-----------+--------+------------
  requests_per_sec |       |           |           |           |           |        |
  (A) 4db0b9c9d0f3 | 1     | 239142.25 | 239142.25 | 239142.25 | 239142.25 | 0      |
  (B) 84f99e060a66 | 1     | 240934.99 | 240934.99 | 240934.99 | 240934.99 | 0      |
                   |       | +0.75%    | +0.75%    | +0.75%    | +0.75%    | ---    | + is good

[*] BENCHMARK
Name: pft
Description: pft is a page fault microbenchmark.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 0
  machine_total_logical_cores  | 112
  machine_total_physical_cores | 56
  machine_total_ram_gib        | 384

[*] METRICS
        LABEL        | COUNT |       MIN       |       MAX       |          MEAN          |         MEDIAN         |       STDDEV       |   DIRECTION
---------------------+-------+-----------------+-----------------+------------------------+------------------------+--------------------+----------------
  pft_faults_cpu_1   |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 484154.455      | 498063.798      | 492351.76326666656     | 492720.2755            | 4126.981613976473  |
  (B) 84f99e060a66   | 30    | 482742.695      | 494479.926      | 489330.9015999999      | 489143.3515            | 2618.891200946336  |
                     |       | -0.29%          | -0.72%          | -0.61%                 | -0.73%                 | -36.54%            | <not defined>
  pft_faults_cpu_4   |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 417255.026      | 436288.218      | 428640.1996333333      | 430395.16150000005     | 4746.028941246967  |
  (B) 84f99e060a66   | 30    | 415617.241      | 437189.129      | 427759.93749999994     | 429933.131             | 5261.030587409123  |
                     |       | -0.39%          | +0.21%          | -0.21%                 | -0.11%                 | +10.85%            | <not defined>
  pft_faults_cpu_7   |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 372190.395      | 400357.397      | 387933.1743333333      | 387656.1515            | 6724.31140527816   |
  (B) 84f99e060a66   | 30    | 369391.438      | 409099.695      | 388038.53069999994     | 387980.819             | 8197.679887071488  |
                     |       | -0.75%          | +2.18%          | +0.03%                 | +0.08%                 | +21.91%            | <not defined>
  pft_faults_cpu_12  |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 309306.251      | 368030.388      | 334522.85160000005     | 334821.131             | 11465.852240019683 |
  (B) 84f99e060a66   | 30    | 318117.629      | 391477.132      | 350409.11253333336     | 342996.01749999996     | 19334.051623320604 |
                     |       | +2.85%          | +6.37%          | +4.75%                 | +2.44%                 | +68.62%            | <not defined>
  pft_faults_cpu_21  |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 212079.413      | 240668.262      | 223933.0883666667      | 223797.9645            | 6708.986752443093  |
  (B) 84f99e060a66   | 30    | 209471.97       | 253796.094      | 231684.6007            | 232578.2415            | 8994.590384813879  |
                     |       | -1.23%          | +5.45%          | +3.46%                 | +3.92%                 | +34.07%            | <not defined>
  pft_faults_cpu_30  |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 149503.828      | 169490.187      | 157576.0666666667      | 156733.52000000002     | 4880.567067589405  |
  (B) 84f99e060a66   | 30    | 146606.506      | 159328.049      | 153893.34433333337     | 153843.4095            | 3658.1395313921544 |
                     |       | -1.94%          | -6.00%          | -2.34%                 | -1.84%                 | -25.05%            | <not defined>
  pft_faults_cpu_48  |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 89186.867       | 105649.211      | 96896.45703333334      | 96477.243              | 3547.1707756648643 |
  (B) 84f99e060a66   | 30    | 95000.878       | 104614.989      | 99554.7385             | 99217.7575             | 2777.940310004704  |
                     |       | +6.52%          | -0.98%          | +2.74%                 | +2.84%                 | -21.69%            | <not defined>
  pft_faults_cpu_79  |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 57662.997       | 65575.52        | 61964.21549999999      | 62236.382              | 2375.343252427598  |
  (B) 84f99e060a66   | 30    | 59007.184       | 64639.405       | 62038.662000000004     | 62079.3465             | 1445.370658156032  |
                     |       | +2.33%          | -1.43%          | +0.12%                 | -0.25%                 | -39.15%            | <not defined>
  pft_faults_cpu_110 |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 38555.581       | 43263.373       | 41597.62980000001      | 41827.095              | 1083.5797160778825 |
  (B) 84f99e060a66   | 30    | 38994.493       | 42748.637       | 40923.156866666664     | 40935.1565             | 803.2742110120612  |
                     |       | +1.14%          | -1.19%          | -1.62%                 | -2.13%                 | -25.87%            | <not defined>
  pft_faults_cpu_112 |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 37132.764       | 42444.162       | 39839.86603333334      | 39874.229              | 1333.3398674946425 |
  (B) 84f99e060a66   | 30    | 38258.017       | 41967.179       | 40467.65890000001      | 40828.6995             | 1129.7326503081576 |
                     |       | +3.03%          | -1.12%          | +1.58%                 | +2.39%                 | -15.27%            | <not defined>
  pft_faults_cpu_max |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 37132.764       | 42444.162       | 39839.86603333334      | 39874.229              | 1333.3398674946425 |
  (B) 84f99e060a66   | 30    | 38258.017       | 41967.179       | 40467.65890000001      | 40828.6995             | 1129.7326503081576 |
                     |       | +3.03%          | -1.12%          | +1.58%                 | +2.39%                 | -15.27%            | + is good
  pft_faults_cpu_min |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 484154.455      | 498063.798      | 492351.76326666656     | 492720.2755            | 4126.981613976473  |
  (B) 84f99e060a66   | 30    | 482742.695      | 494479.926      | 489330.9015999999      | 489143.3515            | 2618.891200946336  |
                     |       | -0.29%          | -0.72%          | -0.61%                 | -0.73%                 | -36.54%            | <not defined>
  pft_faults_sec_1   |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 482655.212      | 496337.449      | 490741.9783666667      | 491119.00399999996     | 4087.2010942607367 |
  (B) 84f99e060a66   | 30    | 481121.66       | 492991.898      | 487799.8169666667      | 487654.93200000003     | 2614.246674701353  |
                     |       | -0.32%          | -0.67%          | -0.60%                 | -0.71%                 | -36.04%            | <not defined>
  pft_faults_sec_4   |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 1.656675025e+06 | 1.734861822e+06 | 1.7028609316666666e+06 | 1.708249671e+06        | 18396.918176364477 |
  (B) 84f99e060a66   | 30    | 1.65463698e+06  | 1.740377412e+06 | 1.6982376496e+06       | 1.7050573065e+06       | 21839.976016046272 |
                     |       | -0.12%          | +0.32%          | -0.27%                 | -0.19%                 | +18.72%            | <not defined>
  pft_faults_sec_7   |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 2.576784434e+06 | 2.781667761e+06 | 2.691555006733333e+06  | 2.688371363e+06        | 49862.74427563325  |
  (B) 84f99e060a66   | 30    | 2.542283262e+06 | 2.842252505e+06 | 2.688004269666667e+06  | 2.6849550785e+06       | 60323.253971796265 |
                     |       | -1.34%          | +2.18%          | -0.13%                 | -0.13%                 | +20.98%            | <not defined>
  pft_faults_sec_12  |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 3.633401181e+06 | 4.374679466e+06 | 3.938532944666667e+06  | 3.939308249e+06        | 149662.241812175   |
  (B) 84f99e060a66   | 30    | 3.725404786e+06 | 4.656017688e+06 | 4.1528862671666667e+06 | 4.0556863770000003e+06 | 246622.15667207883 |
                     |       | +2.53%          | +6.43%          | +5.44%                 | +2.95%                 | +64.79%            | <not defined>
  pft_faults_sec_21  |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 4.257593175e+06 | 4.97357222e+06  | 4.576851050233333e+06  | 4.593935882999999e+06  | 168307.08800032237 |
  (B) 84f99e060a66   | 30    | 4.171296711e+06 | 5.280228369e+06 | 4.7697711534e+06       | 4.7752310005e+06       | 219077.6547498594  |
                     |       | -2.03%          | +6.17%          | +4.22%                 | +3.95%                 | +30.17%            | <not defined>
  pft_faults_sec_30  |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 4.1342909e+06   | 4.83459836e+06  | 4.389753519166667e+06  | 4.361737706499999e+06  | 178674.78729718164 |
  (B) 84f99e060a66   | 30    | 3.876906006e+06 | 4.532188933e+06 | 4.238873766100001e+06  | 4.247939301e+06        | 133754.5309846267  |
                     |       | -6.23%          | -6.26%          | -3.44%                 | -2.61%                 | -25.14%            | <not defined>
  pft_faults_sec_48  |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 3.92643344e+06  | 4.513617248e+06 | 4.2207080896333335e+06 | 4.1957761735e+06       | 126421.31890762268 |
  (B) 84f99e060a66   | 30    | 4.094672788e+06 | 4.456402988e+06 | 4.276243805466667e+06  | 4.27633757e+06         | 108063.75636518569 |
                     |       | +4.28%          | -1.27%          | +1.32%                 | +1.92%                 | -14.52%            | <not defined>
  pft_faults_sec_79  |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 3.973659764e+06 | 4.432766382e+06 | 4.249352675600002e+06  | 4.2673665505e+06       | 125455.19112125924 |
  (B) 84f99e060a66   | 30    | 4.056523429e+06 | 4.393364151e+06 | 4.229541857999999e+06  | 4.223422776e+06        | 93487.38184650465  |
                     |       | +2.09%          | -0.89%          | -0.47%                 | -1.03%                 | -25.48%            | <not defined>
  pft_faults_sec_110 |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 3.924296032e+06 | 4.323501374e+06 | 4.198218468433334e+06  | 4.1928832964999997e+06 | 83650.67974854974  |
  (B) 84f99e060a66   | 30    | 3.869292322e+06 | 4.273456407e+06 | 4.109747521433334e+06  | 4.1008859835e+06       | 85885.92884576393  |
                     |       | -1.40%          | -1.16%          | -2.11%                 | -2.19%                 | +2.67%             | <not defined>
  pft_faults_sec_112 |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 3.883455039e+06 | 4.315943652e+06 | 4.1168769454e+06       | 4.14878067e+06         | 118389.0875606618  |
  (B) 84f99e060a66   | 30    | 3.912952449e+06 | 4.28290919e+06  | 4.1253468313666666e+06 | 4.1466393205000004e+06 | 98247.69508444188  |
                     |       | +0.76%          | -0.77%          | +0.21%                 | -0.05%                 | -17.01%            | <not defined>
  pft_faults_sec_max |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 3.883455039e+06 | 4.315943652e+06 | 4.1168769454e+06       | 4.14878067e+06         | 118389.0875606618  |
  (B) 84f99e060a66   | 30    | 3.912952449e+06 | 4.28290919e+06  | 4.1253468313666666e+06 | 4.1466393205000004e+06 | 98247.69508444188  |
                     |       | +0.76%          | -0.77%          | +0.21%                 | -0.05%                 | -17.01%            | + is good
  pft_faults_sec_min |       |                 |                 |                        |                        |                    |
  (A) 4db0b9c9d0f3   | 30    | 482655.212      | 496337.449      | 490741.9783666667      | 491119.00399999996     | 4087.2010942607367 |
  (B) 84f99e060a66   | 30    | 481121.66       | 492991.898      | 487799.8169666667      | 487654.93200000003     | 2614.246674701353  |
                     |       | -0.32%          | -0.67%          | -0.60%                 | -0.71%                 | -36.04%            | <not defined>

[*] BENCHMARK
Name: pgsql-hammerdb
Description: Postgres benchmark drived by hammerdb.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 0
  machine_total_logical_cores  | 256
  machine_total_physical_cores | 128
  machine_total_ram_gib        | 1024

[*] METRICS
          LABEL         | COUNT |  MIN   |  MAX   |  MEAN  | MEDIAN | STDDEV | DIRECTION
------------------------+-------+--------+--------+--------+--------+--------+------------
  new_orders_per_minute |       |        |        |        |        |        |
  (A) 4db0b9c9d0f3      | 1     | 337717 | 337717 | 337717 | 337717 | 0      |
  (B) 84f99e060a66      | 1     | 341671 | 341671 | 341671 | 341671 | 0      |
                        |       | +1.17% | +1.17% | +1.17% | +1.17% | ---    | + is good

[*] BENCHMARK
Name: redis
Description: redis is a single-threaded server in-memory data store.
This is a basic configuration that uses one server even if running on
a NUMA machine with clients unbound.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 0
  machine_total_logical_cores  | 112
  machine_total_physical_cores | 56
  machine_total_ram_gib        | 384

[*] METRICS
             LABEL             | COUNT |    MIN    |    MAX    |        MEAN        |       MEDIAN       |       STDDEV       |   DIRECTION
-------------------------------+-------+-----------+-----------+--------------------+--------------------+--------------------+----------------
  redis_large_112_GET          |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 86121.52  | 97641.95  | 89782.828          | 87288.98           | 4211.594426691152  |
  (B) 84f99e060a66             | 5     | 81245.33  | 106947.3  | 99600.116          | 103938.22          | 9595.9089538086    |
                               |       | -5.66%    | +9.53%    | +10.93%            | +19.07%            | +127.85%           | <not defined>
  redis_large_112_INCR         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 83982.8   | 99338.41  | 91855.92199999999  | 91971.79           | 4859.882740146716  |
  (B) 84f99e060a66             | 5     | 94742.73  | 107902.8  | 102147.482         | 103681.74          | 5328.512361773783  |
                               |       | +12.81%   | +8.62%    | +11.20%            | +12.73%            | +9.64%             | <not defined>
  redis_large_112_LPOP         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 77050.51  | 103915.53 | 91193.33           | 90605.98           | 8760.399742863337  |
  (B) 84f99e060a66             | 5     | 102777.04 | 106855.88 | 105447.25          | 105883.97          | 1508.874453186883  |
                               |       | +33.39%   | +2.83%    | +15.63%            | +16.86%            | -82.78%            | <not defined>
  redis_large_112_LPUSH        |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 10    | 76444.42  | 105362.97 | 91002.66000000002  | 91349.625          | 8440.749480048564  |
  (B) 84f99e060a66             | 10    | 92986.93  | 108131.48 | 103406.56999999999 | 105310.51500000001 | 5454.677779080998  |
                               |       | +21.64%   | +2.63%    | +13.63%            | +15.28%            | -35.38%            | <not defined>
  redis_large_112_LRANGE_100   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 63462.25  | 67180.82  | 65070.407999999996 | 64373.27           | 1328.4272375768319 |
  (B) 84f99e060a66             | 5     | 66256.32  | 67993.45  | 67324.19           | 67227.34           | 637.2648417730207  |
                               |       | +4.40%    | +1.21%    | +3.46%             | +4.43%             | -52.03%            | <not defined>
  redis_large_112_LRANGE_300   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 28794.13  | 30684.35  | 29872.71           | 29611.64           | 720.4084873181326  |
  (B) 84f99e060a66             | 5     | 30024.98  | 30781.64  | 30501.838          | 30559.27           | 260.3957379374709  |
                               |       | +4.27%    | +0.32%    | +2.11%             | +3.20%             | -63.85%            | <not defined>
  redis_large_112_LRANGE_500   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 20009.64  | 22382.88  | 21281.039999999997 | 21232.28           | 945.9369798881954  |
  (B) 84f99e060a66             | 5     | 21256.83  | 22582.08  | 22141.614          | 22271.86           | 471.52600961558824 |
                               |       | +6.23%    | +0.89%    | +4.04%             | +4.90%             | -50.15%            | <not defined>
  redis_large_112_LRANGE_600   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 15732.15  | 17724.53  | 16962.964          | 17026.55           | 738.9997225872282  |
  (B) 84f99e060a66             | 5     | 17638.86  | 18001.54  | 17813.78           | 17760.41           | 132.1847308882529  |
                               |       | +12.12%   | +1.56%    | +5.02%             | +4.31%             | -82.11%            | <not defined>
  redis_large_112_MSET_10      |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 69913.45  | 83562.44  | 75068.09           | 73061.49           | 4699.897939789759  |
  (B) 84f99e060a66             | 5     | 82621.41  | 86909.67  | 85500.666          | 86207.64           | 1513.6861191224532 |
                               |       | +18.18%   | +4.01%    | +13.90%            | +17.99%            | -67.79%            | <not defined>
  redis_large_112_PING_BULK    |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 76210.8   | 91588.52  | 81885.012          | 76924.86           | 6672.886753669362  |
  (B) 84f99e060a66             | 5     | 97981.58  | 104706.56 | 102659.61          | 103046.04          | 2454.010128585455  |
                               |       | +28.57%   | +14.32%   | +25.37%            | +33.96%            | -63.22%            | <not defined>
  redis_large_112_PING_INLINE  |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 85209.36  | 95981.27  | 88791.15199999999  | 87157.36           | 3883.6930501026986 |
  (B) 84f99e060a66             | 5     | 84930.74  | 100740.45 | 91988.232          | 94507.24           | 6083.486538712485  |
                               |       | -0.33%    | +4.96%    | +3.60%             | +8.43%             | +56.64%            | <not defined>
  redis_large_112_SADD         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 87508.2   | 100579.34 | 96678.14           | 98047.87           | 4697.426045455105  |
  (B) 84f99e060a66             | 5     | 95320.71  | 109097.65 | 104946.528         | 107563.89          | 5187.759965481825  |
                               |       | +8.93%    | +8.47%    | +8.55%             | +9.71%             | +10.44%            | <not defined>
  redis_large_112_SET          |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 83984.21  | 88511.24  | 87263.608          | 88101.07           | 1681.5648135162658 |
  (B) 84f99e060a66             | 5     | 91495.49  | 108322.41 | 103931.492         | 107452.93          | 6323.713351904558  |
                               |       | +8.94%    | +22.38%   | +19.10%            | +21.97%            | +276.06%           | <not defined>
  redis_large_112_SPOP         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 75318.22  | 101266.85 | 90612.204          | 99986              | 12424.38900873697  |
  (B) 84f99e060a66             | 5     | 91482.94  | 108124.48 | 104229.332         | 107524.57          | 6411.221696748284  |
                               |       | +21.46%   | +6.77%    | +15.03%            | +7.54%             | -48.40%            | <not defined>
  redis_large_max_GET          |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 86121.52  | 97641.95  | 89782.828          | 87288.98           | 4211.594426691152  |
  (B) 84f99e060a66             | 5     | 81245.33  | 106947.3  | 99600.116          | 103938.22          | 9595.9089538086    |
                               |       | -5.66%    | +9.53%    | +10.93%            | +19.07%            | +127.85%           | + is good
  redis_large_max_INCR         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 83982.8   | 99338.41  | 91855.92199999999  | 91971.79           | 4859.882740146716  |
  (B) 84f99e060a66             | 5     | 94742.73  | 107902.8  | 102147.482         | 103681.74          | 5328.512361773783  |
                               |       | +12.81%   | +8.62%    | +11.20%            | +12.73%            | +9.64%             | + is good
  redis_large_max_LPOP         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 77050.51  | 103915.53 | 91193.33           | 90605.98           | 8760.399742863337  |
  (B) 84f99e060a66             | 5     | 102777.04 | 106855.88 | 105447.25          | 105883.97          | 1508.874453186883  |
                               |       | +33.39%   | +2.83%    | +15.63%            | +16.86%            | -82.78%            | + is good
  redis_large_max_LPUSH        |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 10    | 76444.42  | 105362.97 | 91002.66000000002  | 91349.625          | 8440.749480048564  |
  (B) 84f99e060a66             | 10    | 92986.93  | 108131.48 | 103406.56999999999 | 105310.51500000001 | 5454.677779080998  |
                               |       | +21.64%   | +2.63%    | +13.63%            | +15.28%            | -35.38%            | + is good
  redis_large_max_LRANGE_100   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 63462.25  | 67180.82  | 65070.407999999996 | 64373.27           | 1328.4272375768319 |
  (B) 84f99e060a66             | 5     | 66256.32  | 67993.45  | 67324.19           | 67227.34           | 637.2648417730207  |
                               |       | +4.40%    | +1.21%    | +3.46%             | +4.43%             | -52.03%            | + is good
  redis_large_max_LRANGE_300   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 28794.13  | 30684.35  | 29872.71           | 29611.64           | 720.4084873181326  |
  (B) 84f99e060a66             | 5     | 30024.98  | 30781.64  | 30501.838          | 30559.27           | 260.3957379374709  |
                               |       | +4.27%    | +0.32%    | +2.11%             | +3.20%             | -63.85%            | + is good
  redis_large_max_LRANGE_500   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 20009.64  | 22382.88  | 21281.039999999997 | 21232.28           | 945.9369798881954  |
  (B) 84f99e060a66             | 5     | 21256.83  | 22582.08  | 22141.614          | 22271.86           | 471.52600961558824 |
                               |       | +6.23%    | +0.89%    | +4.04%             | +4.90%             | -50.15%            | + is good
  redis_large_max_LRANGE_600   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 15732.15  | 17724.53  | 16962.964          | 17026.55           | 738.9997225872282  |
  (B) 84f99e060a66             | 5     | 17638.86  | 18001.54  | 17813.78           | 17760.41           | 132.1847308882529  |
                               |       | +12.12%   | +1.56%    | +5.02%             | +4.31%             | -82.11%            | + is good
  redis_large_max_MSET_10      |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 69913.45  | 83562.44  | 75068.09           | 73061.49           | 4699.897939789759  |
  (B) 84f99e060a66             | 5     | 82621.41  | 86909.67  | 85500.666          | 86207.64           | 1513.6861191224532 |
                               |       | +18.18%   | +4.01%    | +13.90%            | +17.99%            | -67.79%            | + is good
  redis_large_max_PING_BULK    |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 76210.8   | 91588.52  | 81885.012          | 76924.86           | 6672.886753669362  |
  (B) 84f99e060a66             | 5     | 97981.58  | 104706.56 | 102659.61          | 103046.04          | 2454.010128585455  |
                               |       | +28.57%   | +14.32%   | +25.37%            | +33.96%            | -63.22%            | + is good
  redis_large_max_PING_INLINE  |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 85209.36  | 95981.27  | 88791.15199999999  | 87157.36           | 3883.6930501026986 |
  (B) 84f99e060a66             | 5     | 84930.74  | 100740.45 | 91988.232          | 94507.24           | 6083.486538712485  |
                               |       | -0.33%    | +4.96%    | +3.60%             | +8.43%             | +56.64%            | + is good
  redis_large_max_SADD         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 87508.2   | 100579.34 | 96678.14           | 98047.87           | 4697.426045455105  |
  (B) 84f99e060a66             | 5     | 95320.71  | 109097.65 | 104946.528         | 107563.89          | 5187.759965481825  |
                               |       | +8.93%    | +8.47%    | +8.55%             | +9.71%             | +10.44%            | + is good
  redis_large_max_SET          |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 83984.21  | 88511.24  | 87263.608          | 88101.07           | 1681.5648135162658 |
  (B) 84f99e060a66             | 5     | 91495.49  | 108322.41 | 103931.492         | 107452.93          | 6323.713351904558  |
                               |       | +8.94%    | +22.38%   | +19.10%            | +21.97%            | +276.06%           | + is good
  redis_large_max_SPOP         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 75318.22  | 101266.85 | 90612.204          | 99986              | 12424.38900873697  |
  (B) 84f99e060a66             | 5     | 91482.94  | 108124.48 | 104229.332         | 107524.57          | 6411.221696748284  |
                               |       | +21.46%   | +6.77%    | +15.03%            | +7.54%             | -48.40%            | + is good
  redis_medium_112_GET         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 71464.3   | 105296.41 | 98298.09           | 104887.77          | 13418.040732782114 |
  (B) 84f99e060a66             | 5     | 71037.87  | 105764.15 | 88069.89799999999  | 86184.61           | 15326.191434544195 |
                               |       | -0.60%    | +0.44%    | -10.41%            | -17.83%            | +14.22%            | <not defined>
  redis_medium_112_INCR        |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 89952.33  | 106894.71 | 102742.21399999999 | 105374.08          | 6425.891039034509  |
  (B) 84f99e060a66             | 5     | 72035.73  | 108530.5  | 91255.438          | 97238.42           | 16017.995235722103 |
                               |       | -19.92%   | +1.53%    | -11.18%            | -7.72%             | +149.27%           | <not defined>
  redis_medium_112_LPOP        |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 70546.73  | 106202.2  | 98540.89600000001  | 105563.19          | 14009.409711759596 |
  (B) 84f99e060a66             | 5     | 99800.4   | 106281.22 | 104831.92599999999 | 106112.05          | 2520.25111201722   |
                               |       | +41.47%   | +0.07%    | +6.38%             | +0.52%             | -82.01%            | <not defined>
  redis_medium_112_LPUSH       |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 10    | 70596.54  | 109253.8  | 102989.75099999999 | 106360.395         | 10868.068935072552 |
  (B) 84f99e060a66             | 10    | 67317.41  | 107284.62 | 99283.43799999998  | 106264.28          | 14661.570625569964 |
                               |       | -4.64%    | -1.80%    | -3.60%             | -0.09%             | +34.91%            | <not defined>
  redis_medium_112_LRANGE_100  |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 67344.6   | 69536.2   | 68463.45199999999  | 68535.4            | 852.3302381213465  |
  (B) 84f99e060a66             | 5     | 66827.05  | 69237.7   | 68027.628          | 68157.03           | 811.0692465480314  |
                               |       | -0.77%    | -0.43%    | -0.64%             | -0.55%             | -4.84%             | <not defined>
  redis_medium_112_LRANGE_300  |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 30814.74  | 31701.75  | 31275.802          | 31251.95           | 349.8131587805122  |
  (B) 84f99e060a66             | 5     | 30562.35  | 31373.53  | 31060.512000000002 | 31065.55           | 298.0781213306358  |
                               |       | -0.82%    | -1.04%    | -0.69%             | -0.60%             | -14.79%            | <not defined>
  redis_medium_112_LRANGE_500  |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 22397.92  | 23195.94  | 22846.742000000002 | 22862.89           | 264.14444346985744 |
  (B) 84f99e060a66             | 5     | 22414.99  | 23173.9   | 22770.994          | 22789.43           | 259.2454181350172  |
                               |       | +0.08%    | -0.10%    | -0.33%             | -0.32%             | -1.85%             | <not defined>
  redis_medium_112_LRANGE_600  |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 17964.61  | 18386.75  | 18159.345999999998 | 18146.19           | 145.55850653259614 |
  (B) 84f99e060a66             | 5     | 17892.93  | 18372.22  | 18151.72           | 18141.91           | 158.25971072891696 |
                               |       | -0.40%    | -0.08%    | -0.04%             | -0.02%             | +8.73%             | <not defined>
  redis_medium_112_MSET_10     |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 82891.24  | 93764.65  | 90304.758          | 91224.23           | 3859.0309716476713 |
  (B) 84f99e060a66             | 5     | 82569.56  | 92498.38  | 87018.09199999999  | 88550.43           | 3799.8862641158103 |
                               |       | -0.39%    | -1.35%    | -3.64%             | -2.93%             | -1.53%             | <not defined>
  redis_medium_112_PING_BULK   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 70866.7   | 105719.42 | 88936.732          | 86625.09           | 14234.524281637798 |
  (B) 84f99e060a66             | 5     | 70656.4   | 82149.02  | 75741.51999999999  | 76091.91           | 4234.288704918454  |
                               |       | -0.30%    | -22.30%   | -14.84%            | -12.16%            | -70.25%            | <not defined>
  redis_medium_112_PING_INLINE |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 79164.03  | 101245.32 | 93598.43000000001  | 97924.02           | 8205.051511289863  |
  (B) 84f99e060a66             | 5     | 82054.65  | 101936.8  | 91002.572          | 86782.96           | 8258.393592532146  |
                               |       | +3.65%    | +0.68%    | -2.77%             | -11.38%            | +0.65%             | <not defined>
  redis_medium_112_SADD        |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 71204.79  | 108873.16 | 99893.516          | 106202.2           | 14385.239455086732 |
  (B) 84f99e060a66             | 5     | 107066.38 | 108577.63 | 107833.04000000001 | 107805.09          | 506.20947272843495 |
                               |       | +50.36%   | -0.27%    | +7.95%             | +1.51%             | -96.48%            | <not defined>
  redis_medium_112_SET         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 71078.26  | 107169.65 | 98957.382          | 106803.38          | 14032.888516120123 |
  (B) 84f99e060a66             | 5     | 71128.81  | 106746.37 | 84412.32400000001  | 78814.63           | 13971.086249029599 |
                               |       | +0.07%    | -0.39%    | -14.70%            | -26.21%            | -0.44%             | <not defined>
  redis_medium_112_SPOP        |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 90555.1   | 108108.11 | 103639.896         | 106849.02          | 6593.160307380974  |
  (B) 84f99e060a66             | 5     | 79706.68  | 108601.21 | 100128.342         | 106168.39          | 10971.534908081734 |
                               |       | -11.98%   | +0.46%    | -3.39%             | -0.64%             | +66.41%            | <not defined>
  redis_medium_max_GET         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 71464.3   | 105296.41 | 98298.09           | 104887.77          | 13418.040732782114 |
  (B) 84f99e060a66             | 5     | 71037.87  | 105764.15 | 88069.89799999999  | 86184.61           | 15326.191434544195 |
                               |       | -0.60%    | +0.44%    | -10.41%            | -17.83%            | +14.22%            | + is good
  redis_medium_max_INCR        |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 89952.33  | 106894.71 | 102742.21399999999 | 105374.08          | 6425.891039034509  |
  (B) 84f99e060a66             | 5     | 72035.73  | 108530.5  | 91255.438          | 97238.42           | 16017.995235722103 |
                               |       | -19.92%   | +1.53%    | -11.18%            | -7.72%             | +149.27%           | + is good
  redis_medium_max_LPOP        |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 70546.73  | 106202.2  | 98540.89600000001  | 105563.19          | 14009.409711759596 |
  (B) 84f99e060a66             | 5     | 99800.4   | 106281.22 | 104831.92599999999 | 106112.05          | 2520.25111201722   |
                               |       | +41.47%   | +0.07%    | +6.38%             | +0.52%             | -82.01%            | + is good
  redis_medium_max_LPUSH       |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 10    | 70596.54  | 109253.8  | 102989.75099999999 | 106360.395         | 10868.068935072552 |
  (B) 84f99e060a66             | 10    | 67317.41  | 107284.62 | 99283.43799999998  | 106264.28          | 14661.570625569964 |
                               |       | -4.64%    | -1.80%    | -3.60%             | -0.09%             | +34.91%            | + is good
  redis_medium_max_LRANGE_100  |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 67344.6   | 69536.2   | 68463.45199999999  | 68535.4            | 852.3302381213465  |
  (B) 84f99e060a66             | 5     | 66827.05  | 69237.7   | 68027.628          | 68157.03           | 811.0692465480314  |
                               |       | -0.77%    | -0.43%    | -0.64%             | -0.55%             | -4.84%             | + is good
  redis_medium_max_LRANGE_300  |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 30814.74  | 31701.75  | 31275.802          | 31251.95           | 349.8131587805122  |
  (B) 84f99e060a66             | 5     | 30562.35  | 31373.53  | 31060.512000000002 | 31065.55           | 298.0781213306358  |
                               |       | -0.82%    | -1.04%    | -0.69%             | -0.60%             | -14.79%            | + is good
  redis_medium_max_LRANGE_500  |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 22397.92  | 23195.94  | 22846.742000000002 | 22862.89           | 264.14444346985744 |
  (B) 84f99e060a66             | 5     | 22414.99  | 23173.9   | 22770.994          | 22789.43           | 259.2454181350172  |
                               |       | +0.08%    | -0.10%    | -0.33%             | -0.32%             | -1.85%             | + is good
  redis_medium_max_LRANGE_600  |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 17964.61  | 18386.75  | 18159.345999999998 | 18146.19           | 145.55850653259614 |
  (B) 84f99e060a66             | 5     | 17892.93  | 18372.22  | 18151.72           | 18141.91           | 158.25971072891696 |
                               |       | -0.40%    | -0.08%    | -0.04%             | -0.02%             | +8.73%             | + is good
  redis_medium_max_MSET_10     |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 82891.24  | 93764.65  | 90304.758          | 91224.23           | 3859.0309716476713 |
  (B) 84f99e060a66             | 5     | 82569.56  | 92498.38  | 87018.09199999999  | 88550.43           | 3799.8862641158103 |
                               |       | -0.39%    | -1.35%    | -3.64%             | -2.93%             | -1.53%             | + is good
  redis_medium_max_PING_BULK   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 70866.7   | 105719.42 | 88936.732          | 86625.09           | 14234.524281637798 |
  (B) 84f99e060a66             | 5     | 70656.4   | 82149.02  | 75741.51999999999  | 76091.91           | 4234.288704918454  |
                               |       | -0.30%    | -22.30%   | -14.84%            | -12.16%            | -70.25%            | + is good
  redis_medium_max_PING_INLINE |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 79164.03  | 101245.32 | 93598.43000000001  | 97924.02           | 8205.051511289863  |
  (B) 84f99e060a66             | 5     | 82054.65  | 101936.8  | 91002.572          | 86782.96           | 8258.393592532146  |
                               |       | +3.65%    | +0.68%    | -2.77%             | -11.38%            | +0.65%             | + is good
  redis_medium_max_SADD        |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 71204.79  | 108873.16 | 99893.516          | 106202.2           | 14385.239455086732 |
  (B) 84f99e060a66             | 5     | 107066.38 | 108577.63 | 107833.04000000001 | 107805.09          | 506.20947272843495 |
                               |       | +50.36%   | -0.27%    | +7.95%             | +1.51%             | -96.48%            | + is good
  redis_medium_max_SET         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 71078.26  | 107169.65 | 98957.382          | 106803.38          | 14032.888516120123 |
  (B) 84f99e060a66             | 5     | 71128.81  | 106746.37 | 84412.32400000001  | 78814.63           | 13971.086249029599 |
                               |       | +0.07%    | -0.39%    | -14.70%            | -26.21%            | -0.44%             | + is good
  redis_medium_max_SPOP        |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 90555.1   | 108108.11 | 103639.896         | 106849.02          | 6593.160307380974  |
  (B) 84f99e060a66             | 5     | 79706.68  | 108601.21 | 100128.342         | 106168.39          | 10971.534908081734 |
                               |       | -11.98%   | +0.46%    | -3.39%             | -0.64%             | +66.41%            | + is good
  redis_small_112_GET          |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 71942.45  | 99009.9   | 92823.784          | 97847.36           | 10459.059890851764 |
  (B) 84f99e060a66             | 5     | 97656.24  | 103199.18 | 99421.75399999999  | 99009.9            | 1965.130453416262  |
                               |       | +35.74%   | +4.23%    | +7.11%             | +1.19%             | -81.21%            | <not defined>
  redis_small_112_INCR         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 71428.57  | 100502.52 | 94127.908          | 99403.58           | 11357.9112414764   |
  (B) 84f99e060a66             | 5     | 99502.48  | 105263.16 | 101133.712         | 100300.91          | 2088.725085858836  |
                               |       | +39.30%   | +4.74%    | +7.44%             | +0.90%             | -81.61%            | <not defined>
  redis_small_112_LPOP         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 67430.88  | 97465.88  | 80725.542          | 71377.59           | 13650.5478862138   |
  (B) 84f99e060a66             | 5     | 97370.98  | 103519.66 | 99722.42199999999  | 99206.34           | 2051.336756560467  |
                               |       | +44.40%   | +6.21%    | +23.53%            | +38.99%            | -84.97%            | <not defined>
  redis_small_112_LPUSH        |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 10    | 71275.84  | 98039.22  | 84531.177          | 82620.555          | 11398.639723914474 |
  (B) 84f99e060a66             | 10    | 71275.84  | 102880.66 | 95015.00399999999  | 98961.13           | 10393.818308459315 |
                               |       | +0.00%    | +4.94%    | +12.40%            | +19.78%            | -8.82%             | <not defined>
  redis_small_112_LRANGE_100   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 52521.01  | 68166.33  | 60484.19           | 64516.13           | 6610.609484164073  |
  (B) 84f99e060a66             | 5     | 52631.58  | 67796.61  | 60568.95           | 64808.82           | 6536.080283922469  |
                               |       | +0.21%    | -0.54%    | +0.14%             | +0.45%             | -1.13%             | <not defined>
  redis_small_112_LRANGE_300   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 27693.16  | 31152.65  | 29220.967999999997 | 29568.3            | 1266.5903558356993 |
  (B) 84f99e060a66             | 5     | 27964.21  | 30892.8   | 29287.316          | 29726.52           | 1136.400625212782  |
                               |       | +0.98%    | -0.83%    | +0.23%             | +0.54%             | -10.28%            | <not defined>
  redis_small_112_LRANGE_500   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 20964.36  | 22872.83  | 21715.272          | 21588.95           | 642.6844450397102  |
  (B) 84f99e060a66             | 5     | 21043.77  | 22670.6   | 21761.058          | 21881.84           | 561.3368975365854  |
                               |       | +0.38%    | -0.88%    | +0.21%             | +1.36%             | -12.66%            | <not defined>
  redis_small_112_LRANGE_600   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 17424.64  | 18532.25  | 17707.992          | 17537.71           | 416.9294510777572  |
  (B) 84f99e060a66             | 5     | 17325.02  | 18315.02  | 17634.145999999997 | 17488.63           | 350.3809420388055  |
                               |       | -0.57%    | -1.17%    | -0.42%             | -0.28%             | -15.96%            | <not defined>
  redis_small_112_MSET_10      |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 71022.73  | 102564.1  | 89180.122          | 99403.58           | 14342.618805543705 |
  (B) 84f99e060a66             | 5     | 71377.59  | 104166.67 | 90567.052          | 98039.22           | 12918.439933028136 |
                               |       | +0.50%    | +1.56%    | +1.56%             | -1.37%             | -9.93%             | <not defined>
  redis_small_112_PING_BULK    |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 86058.52  | 98231.83  | 94980.186          | 96993.21           | 4506.456414144485  |
  (B) 84f99e060a66             | 5     | 88417.33  | 97656.24  | 95486.45           | 97087.38           | 3546.8647583238903 |
                               |       | +2.74%    | -0.59%    | +0.53%             | +0.10%             | -21.29%            | <not defined>
  redis_small_112_PING_INLINE  |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 98039.22  | 100200.4  | 98800.73599999999  | 98716.68           | 774.7318891487538  |
  (B) 84f99e060a66             | 5     | 91074.68  | 100603.62 | 97840.432          | 99108.03           | 3437.0745702553527 |
                               |       | -7.10%    | +0.40%    | -0.97%             | +0.40%             | +343.65%           | <not defined>
  redis_small_112_SADD         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 66711.14  | 98425.2   | 81154.564          | 71326.68           | 14081.393480563775 |
  (B) 84f99e060a66             | 5     | 99403.58  | 105042.02 | 101136.232         | 99900.09           | 2141.9583192153873 |
                               |       | +49.01%   | +6.72%    | +24.62%            | +40.06%            | -84.79%            | <not defined>
  redis_small_112_SET          |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 72358.9   | 99800.4   | 93916.536          | 99009.9            | 10782.800109525542 |
  (B) 84f99e060a66             | 5     | 99403.58  | 105485.23 | 101242.73          | 100200.4           | 2246.7532169955816 |
                               |       | +37.38%   | +5.70%    | +7.80%             | +1.20%             | -79.16%            | <not defined>
  redis_small_112_SPOP         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 67385.45  | 99900.09  | 81554.86600000001  | 71581.96           | 14338.592272095051 |
  (B) 84f99e060a66             | 5     | 71890.73  | 104275.29 | 94837.97600000001  | 99601.6            | 11640.699220503204 |
                               |       | +6.69%    | +4.38%    | +16.29%            | +39.14%            | -18.82%            | <not defined>
  redis_small_max_GET          |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 71942.45  | 99009.9   | 92823.784          | 97847.36           | 10459.059890851764 |
  (B) 84f99e060a66             | 5     | 97656.24  | 103199.18 | 99421.75399999999  | 99009.9            | 1965.130453416262  |
                               |       | +35.74%   | +4.23%    | +7.11%             | +1.19%             | -81.21%            | + is good
  redis_small_max_INCR         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 71428.57  | 100502.52 | 94127.908          | 99403.58           | 11357.9112414764   |
  (B) 84f99e060a66             | 5     | 99502.48  | 105263.16 | 101133.712         | 100300.91          | 2088.725085858836  |
                               |       | +39.30%   | +4.74%    | +7.44%             | +0.90%             | -81.61%            | + is good
  redis_small_max_LPOP         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 67430.88  | 97465.88  | 80725.542          | 71377.59           | 13650.5478862138   |
  (B) 84f99e060a66             | 5     | 97370.98  | 103519.66 | 99722.42199999999  | 99206.34           | 2051.336756560467  |
                               |       | +44.40%   | +6.21%    | +23.53%            | +38.99%            | -84.97%            | + is good
  redis_small_max_LPUSH        |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 10    | 71275.84  | 98039.22  | 84531.177          | 82620.555          | 11398.639723914474 |
  (B) 84f99e060a66             | 10    | 71275.84  | 102880.66 | 95015.00399999999  | 98961.13           | 10393.818308459315 |
                               |       | +0.00%    | +4.94%    | +12.40%            | +19.78%            | -8.82%             | + is good
  redis_small_max_LRANGE_100   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 52521.01  | 68166.33  | 60484.19           | 64516.13           | 6610.609484164073  |
  (B) 84f99e060a66             | 5     | 52631.58  | 67796.61  | 60568.95           | 64808.82           | 6536.080283922469  |
                               |       | +0.21%    | -0.54%    | +0.14%             | +0.45%             | -1.13%             | + is good
  redis_small_max_LRANGE_300   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 27693.16  | 31152.65  | 29220.967999999997 | 29568.3            | 1266.5903558356993 |
  (B) 84f99e060a66             | 5     | 27964.21  | 30892.8   | 29287.316          | 29726.52           | 1136.400625212782  |
                               |       | +0.98%    | -0.83%    | +0.23%             | +0.54%             | -10.28%            | + is good
  redis_small_max_LRANGE_500   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 20964.36  | 22872.83  | 21715.272          | 21588.95           | 642.6844450397102  |
  (B) 84f99e060a66             | 5     | 21043.77  | 22670.6   | 21761.058          | 21881.84           | 561.3368975365854  |
                               |       | +0.38%    | -0.88%    | +0.21%             | +1.36%             | -12.66%            | + is good
  redis_small_max_LRANGE_600   |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 17424.64  | 18532.25  | 17707.992          | 17537.71           | 416.9294510777572  |
  (B) 84f99e060a66             | 5     | 17325.02  | 18315.02  | 17634.145999999997 | 17488.63           | 350.3809420388055  |
                               |       | -0.57%    | -1.17%    | -0.42%             | -0.28%             | -15.96%            | + is good
  redis_small_max_MSET_10      |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 71022.73  | 102564.1  | 89180.122          | 99403.58           | 14342.618805543705 |
  (B) 84f99e060a66             | 5     | 71377.59  | 104166.67 | 90567.052          | 98039.22           | 12918.439933028136 |
                               |       | +0.50%    | +1.56%    | +1.56%             | -1.37%             | -9.93%             | + is good
  redis_small_max_PING_BULK    |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 86058.52  | 98231.83  | 94980.186          | 96993.21           | 4506.456414144485  |
  (B) 84f99e060a66             | 5     | 88417.33  | 97656.24  | 95486.45           | 97087.38           | 3546.8647583238903 |
                               |       | +2.74%    | -0.59%    | +0.53%             | +0.10%             | -21.29%            | + is good
  redis_small_max_PING_INLINE  |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 98039.22  | 100200.4  | 98800.73599999999  | 98716.68           | 774.7318891487538  |
  (B) 84f99e060a66             | 5     | 91074.68  | 100603.62 | 97840.432          | 99108.03           | 3437.0745702553527 |
                               |       | -7.10%    | +0.40%    | -0.97%             | +0.40%             | +343.65%           | + is good
  redis_small_max_SADD         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 66711.14  | 98425.2   | 81154.564          | 71326.68           | 14081.393480563775 |
  (B) 84f99e060a66             | 5     | 99403.58  | 105042.02 | 101136.232         | 99900.09           | 2141.9583192153873 |
                               |       | +49.01%   | +6.72%    | +24.62%            | +40.06%            | -84.79%            | + is good
  redis_small_max_SET          |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 72358.9   | 99800.4   | 93916.536          | 99009.9            | 10782.800109525542 |
  (B) 84f99e060a66             | 5     | 99403.58  | 105485.23 | 101242.73          | 100200.4           | 2246.7532169955816 |
                               |       | +37.38%   | +5.70%    | +7.80%             | +1.20%             | -79.16%            | + is good
  redis_small_max_SPOP         |       |           |           |                    |                    |                    |
  (A) 4db0b9c9d0f3             | 5     | 67385.45  | 99900.09  | 81554.86600000001  | 71581.96           | 14338.592272095051 |
  (B) 84f99e060a66             | 5     | 71890.73  | 104275.29 | 94837.97600000001  | 99601.6            | 11640.699220503204 |
                               |       | +6.69%    | +4.38%    | +16.29%            | +39.14%            | -18.82%            | + is good

[*] BENCHMARK
Name: stream
Description: The STREAM benchmark is a simple synthetic benchmark
program that measures sustainable memory bandwidth (in MB/s) and the
corresponding computation rate for simple vector kernels.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 0
  machine_total_logical_cores  | 112
  machine_total_physical_cores | 56
  machine_total_ram_gib        | 384

[*] METRICS
              LABEL             | COUNT |   MIN    |   MAX    |        MEAN        |  MEDIAN  |       STDDEV       |   DIRECTION
--------------------------------+-------+----------+----------+--------------------+----------+--------------------+----------------
  stream_omp_llcs_add           |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 22535.34 | 22843.54 | 22692.140000000003 | 22697.54 | 125.88004872364232 |
  (B) 84f99e060a66              | 3     | 22575.5  | 22817.18 | 22708.606666666667 | 22733.14 | 100.1789030795518  |
                                |       | +0.18%   | -0.12%   | +0.07%             | +0.16%   | -20.42%            | + is good
  stream_omp_llcs_cale          |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 21177.26 | 21481.86 | 21343.32           | 21370.84 | 125.86580896600515 |
  (B) 84f99e060a66              | 3     | 21181.64 | 21482.28 | 21363.833333333332 | 21427.58 | 130.75124099687224 |
                                |       | +0.02%   | +0.00%   | +0.10%             | +0.27%   | +3.88%             | <not defined>
  stream_omp_llcs_copy          |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 21672.48 | 21970.32 | 21848.106666666667 | 21901.52 | 127.32348443586098 |
  (B) 84f99e060a66              | 3     | 21729.34 | 21947.58 | 21859.36666666667  | 21901.18 | 93.87381790941001  |
                                |       | +0.26%   | -0.10%   | +0.05%             | -0.00%   | -26.27%            | + is good
  stream_omp_llcs_spread_add    |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 23957.28 | 24110.2  | 24008.460000000003 | 23957.9  | 71.94148918855313  |
  (B) 84f99e060a66              | 3     | 24048.46 | 24176.1  | 24122.753333333334 | 24143.7  | 54.17296086507334  |
                                |       | +0.38%   | +0.27%   | +0.48%             | +0.78%   | -24.70%            | + is good
  stream_omp_llcs_spread_cale   |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 22520.3  | 22686.48 | 22588.213333333333 | 22557.86 | 71.15683023356871  |
  (B) 84f99e060a66              | 3     | 22492.22 | 22590.84 | 22527.266666666666 | 22498.74 | 45.0318711827762   |
                                |       | -0.12%   | -0.42%   | -0.27%             | -0.26%   | -36.71%            | <not defined>
  stream_omp_llcs_spread_copy   |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 22939.62 | 23205.3  | 23077.006666666664 | 23086.1  | 108.65382991664622 |
  (B) 84f99e060a66              | 3     | 22990.68 | 23027.42 | 23006.306666666667 | 23000.82 | 15.492676406037758 |
                                |       | +0.22%   | -0.77%   | -0.31%             | -0.37%   | -85.74%            | + is good
  stream_omp_llcs_spread_triad  |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 23893.94 | 24038.62 | 23981.6            | 24012.24 | 62.91360637148918  |
  (B) 84f99e060a66              | 3     | 24101.42 | 24109.2  | 24105.993333333332 | 24107.36 | 3.3199330649230476 |
                                |       | +0.87%   | +0.29%   | +0.52%             | +0.40%   | -94.72%            | + is good
  stream_omp_llcs_triad         |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 22481.2  | 22847.08 | 22687.286666666667 | 22733.58 | 152.91467802521686 |
  (B) 84f99e060a66              | 3     | 22552.6  | 22847.1  | 22726.29333333333  | 22779.18 | 125.91084711899201 |
                                |       | +0.32%   | +0.00%   | +0.17%             | +0.20%   | -17.66%            | + is good
  stream_omp_nodes_add          |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 43473.82 | 44035.92 | 43819.69333333333  | 43949.34 | 247.11036148966852 |
  (B) 84f99e060a66              | 3     | 43966.06 | 44021.64 | 43988.86666666666  | 43978.9  | 23.75969883834887  |
                                |       | +1.13%   | -0.03%   | +0.39%             | +0.07%   | -90.38%            | + is good
  stream_omp_nodes_cale         |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 41266.66 | 41827.52 | 41541.420000000006 | 41530.08 | 229.11049968664912 |
  (B) 84f99e060a66              | 3     | 41509.96 | 41809.12 | 41676.520000000004 | 41710.48 | 124.46990961674327 |
                                |       | +0.59%   | -0.04%   | +0.33%             | +0.43%   | -45.67%            | <not defined>
  stream_omp_nodes_copy         |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 42152.32 | 42453.44 | 42340.46           | 42415.62 | 133.92804734881528 |
  (B) 84f99e060a66              | 3     | 42313.68 | 42651.36 | 42475.193333333336 | 42460.54 | 138.2461228710901  |
                                |       | +0.38%   | +0.47%   | +0.32%             | +0.11%   | +3.22%             | + is good
  stream_omp_nodes_spread_add   |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 46556.3  | 46572.58 | 46565.64           | 46568.04 | 6.8595237929947395 |
  (B) 84f99e060a66              | 3     | 46590.1  | 46628.86 | 46613.94666666666  | 46622.88 | 17.037952406972313 |
                                |       | +0.07%   | +0.12%   | +0.10%             | +0.12%   | +148.38%           | + is good
  stream_omp_nodes_spread_cale  |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 43479.12 | 43572.42 | 43516.58666666666  | 43498.22 | 40.24279093480293  |
  (B) 84f99e060a66              | 3     | 43421.88 | 43708.9  | 43519.96666666667  | 43429.12 | 133.628733769684   |
                                |       | -0.13%   | +0.31%   | +0.01%             | -0.16%   | +232.06%           | <not defined>
  stream_omp_nodes_spread_copy  |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 44680.66 | 44813.96 | 44766.71333333333  | 44805.52 | 60.94637278423656  |
  (B) 84f99e060a66              | 3     | 44484.82 | 44774.08 | 44597.08666666667  | 44532.36 | 126.64910299283304 |
                                |       | -0.44%   | -0.09%   | -0.38%             | -0.61%   | +107.80%           | + is good
  stream_omp_nodes_spread_triad |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 46425.72 | 46524.82 | 46476.62           | 46479.32 | 40.50242791076154  |
  (B) 84f99e060a66              | 3     | 46525.12 | 46829.82 | 46693.26666666667  | 46724.86 | 126.38334946590818 |
                                |       | +0.21%   | +0.66%   | +0.47%             | +0.53%   | +212.04%           | + is good
  stream_omp_nodes_triad        |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 43550.7  | 43982.02 | 43810.44666666666  | 43898.62 | 186.79781963990425 |
  (B) 84f99e060a66              | 3     | 43898.6  | 44159.54 | 44019.7            | 44000.96 | 107.34931019806498 |
                                |       | +0.80%   | +0.40%   | +0.48%             | +0.23%   | -42.53%            | + is good
  stream_single_add             |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 11777.74 | 11914.54 | 11832.186666666666 | 11804.28 | 59.232010114652255 |
  (B) 84f99e060a66              | 3     | 11607.02 | 12019.86 | 11824.220000000001 | 11845.78 | 169.22931503337858 |
                                |       | -1.45%   | +0.88%   | -0.07%             | +0.35%   | +185.71%           | + is good
  stream_single_cale            |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 11108.2  | 11173.12 | 11133.539999999999 | 11119.3  | 28.3517759584829   |
  (B) 84f99e060a66              | 3     | 10884.56 | 11234.86 | 11068.66           | 11086.56 | 143.56840413777255 |
                                |       | -2.01%   | +0.55%   | -0.58%             | -0.29%   | +406.38%           | <not defined>
  stream_single_copy            |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 11396.58 | 11485.98 | 11436.106666666667 | 11425.76 | 37.2234704931653   |
  (B) 84f99e060a66              | 3     | 11125.68 | 11495.18 | 11329.753333333334 | 11368.4  | 153.30304657406094 |
                                |       | -2.38%   | +0.08%   | -0.93%             | -0.50%   | +311.85%           | + is good
  stream_single_triad           |       |          |          |                    |          |                    |
  (A) 4db0b9c9d0f3              | 3     | 11780.48 | 11898.12 | 11827.6            | 11804.2  | 50.79673480320089  |
  (B) 84f99e060a66              | 3     | 11586.2  | 12035.82 | 11835.193333333335 | 11883.56 | 186.71553574592673 |
                                |       | -1.65%   | +1.16%   | +0.06%             | +0.67%   | +267.57%           | + is good

[*] BENCHMARK
Name: sysbenchthread
Description: Sysbench is a suite of microbenchmarks. This runs the
thread microbench which has threads loop on a lock, yield, unlock. It
is considered to be of questionable value given its vunerability to
load balancing and sched_yield decisions.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 0
  machine_total_logical_cores  | 256
  machine_total_physical_cores | 128
  machine_total_ram_gib        | 1024

[*] METRICS
        LABEL        | COUNT |  MIN   |  MAX   |        MEAN        | MEDIAN |        STDDEV        |   DIRECTION
---------------------+-------+--------+--------+--------------------+--------+----------------------+----------------
  sysbenchthread_1   |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.71   | 2.76   | 2.742857142857143  | 2.75   | 0.014846149779161799 |
  (B) 84f99e060a66   | 7     | 2.67   | 2.74   | 2.7128571428571426 | 2.71   | 0.022497165354319542 |
                     |       | -1.48% | -0.72% | -1.09%             | -1.45% | +51.54%              | <not defined>
  sysbenchthread_4   |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.74   | 2.79   | 2.7699999999999996 | 2.77   | 0.015118578920369087 |
  (B) 84f99e060a66   | 7     | 2.72   | 2.76   | 2.747142857142857  | 2.75   | 0.012777531299998568 |
                     |       | -0.73% | -1.08% | -0.83%             | -0.72% | -15.48%              | <not defined>
  sysbenchthread_7   |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.72   | 2.75   | 2.738571428571429  | 2.74   | 0.011248582677159752 |
  (B) 84f99e060a66   | 7     | 2.73   | 2.76   | 2.748571428571428  | 2.75   | 0.008329931278350345 |
                     |       | +0.37% | +0.36% | +0.37%             | +0.36% | -25.95%              | <not defined>
  sysbenchthread_12  |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.71   | 2.76   | 2.744285714285714  | 2.75   | 0.016781914463529526 |
  (B) 84f99e060a66   | 7     | 2.72   | 2.74   | 2.7271428571428573 | 2.73   | 0.006998542122237587 |
                     |       | +0.37% | -0.72% | -0.62%             | -0.73% | -58.30%              | <not defined>
  sysbenchthread_21  |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.73   | 2.79   | 2.7585714285714285 | 2.77   | 0.020995626366712965 |
  (B) 84f99e060a66   | 7     | 2.72   | 2.78   | 2.738571428571429  | 2.73   | 0.020995626366712813 |
                     |       | -0.37% | -0.36% | -0.73%             | -1.44% | -0.00%               | <not defined>
  sysbenchthread_30  |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.72   | 2.78   | 2.74               | 2.73   | 0.020701966780270524 |
  (B) 84f99e060a66   | 7     | 2.71   | 2.76   | 2.734285714285714  | 2.74   | 0.017612611437054292 |
                     |       | -0.37% | -0.72% | -0.21%             | +0.37% | -14.92%              | <not defined>
  sysbenchthread_48  |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.73   | 2.77   | 2.7528571428571427 | 2.76   | 0.014846149779161651 |
  (B) 84f99e060a66   | 7     | 2.73   | 2.76   | 2.742857142857143  | 2.74   | 0.010301575072754249 |
                     |       | +0.00% | -0.36% | -0.36%             | -0.72% | -30.61%              | <not defined>
  sysbenchthread_79  |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.7    | 2.76   | 2.7300000000000004 | 2.73   | 0.019999999999999945 |
  (B) 84f99e060a66   | 7     | 2.69   | 2.75   | 2.722857142857143  | 2.72   | 0.02249716535431943  |
                     |       | -0.37% | -0.36% | -0.26%             | -0.37% | +12.49%              | <not defined>
  sysbenchthread_110 |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.72   | 2.76   | 2.742857142857143  | 2.74   | 0.013850513878332155 |
  (B) 84f99e060a66   | 7     | 2.72   | 2.75   | 2.73               | 2.73   | 0.010690449676496893 |
                     |       | +0.00% | -0.36% | -0.47%             | -0.36% | -22.82%              | <not defined>
  sysbenchthread_141 |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.73   | 2.77   | 2.75               | 2.75   | 0.013093073414159504 |
  (B) 84f99e060a66   | 7     | 2.71   | 2.75   | 2.73               | 2.73   | 0.013093073414159578 |
                     |       | -0.73% | -0.72% | -0.73%             | -0.73% | +0.00%               | <not defined>
  sysbenchthread_172 |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.73   | 2.76   | 2.7471428571428573 | 2.75   | 0.010301575072754037 |
  (B) 84f99e060a66   | 7     | 2.7    | 2.76   | 2.73               | 2.73   | 0.021380899352993758 |
                     |       | -1.10% | +0.00% | -0.62%             | -0.73% | +107.55%             | <not defined>
  sysbenchthread_203 |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.74   | 2.82   | 2.7671428571428573 | 2.76   | 0.024327694808466156 |
  (B) 84f99e060a66   | 7     | 2.67   | 2.77   | 2.727142857142857  | 2.73   | 0.02864276807966222  |
                     |       | -2.55% | -1.77% | -1.45%             | -1.09% | +17.74%              | <not defined>
  sysbenchthread_234 |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.74   | 2.78   | 2.7599999999999993 | 2.77   | 0.015118578920368945 |
  (B) 84f99e060a66   | 7     | 2.72   | 2.77   | 2.745714285714286  | 2.74   | 0.016781914463529463 |
                     |       | -0.73% | -0.36% | -0.52%             | -1.08% | +11.00%              | <not defined>
  sysbenchthread_265 |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.73   | 2.79   | 2.76               | 2.76   | 0.023299294900428702 |
  (B) 84f99e060a66   | 7     | 2.71   | 2.77   | 2.737142857142857  | 2.73   | 0.019059520091608894 |
                     |       | -0.73% | -0.72% | -0.83%             | -1.09% | -18.20%              | <not defined>
  sysbenchthread_296 |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.73   | 2.79   | 2.7514285714285713 | 2.74   | 0.01958758457257428  |
  (B) 84f99e060a66   | 7     | 2.72   | 2.75   | 2.735714285714286  | 2.74   | 0.00903507902905251  |
                     |       | -0.37% | -1.43% | -0.57%             | +0.00% | -53.87%              | <not defined>
  sysbenchthread_327 |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.72   | 2.75   | 2.738571428571429  | 2.74   | 0.009897433186107772 |
  (B) 84f99e060a66   | 7     | 2.71   | 2.77   | 2.7357142857142858 | 2.74   | 0.017612611437054282 |
                     |       | -0.37% | +0.73% | -0.10%             | +0.00% | +77.95%              | <not defined>
  sysbenchthread_358 |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.74   | 2.79   | 2.7614285714285716 | 2.76   | 0.016413036132965755 |
  (B) 84f99e060a66   | 7     | 2.71   | 2.76   | 2.7399999999999998 | 2.74   | 0.015118578920369066 |
                     |       | -1.09% | -1.08% | -0.78%             | -0.72% | -7.89%               | <not defined>
  sysbenchthread_389 |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.73   | 2.76   | 2.744285714285714  | 2.74   | 0.011780301787479    |
  (B) 84f99e060a66   | 7     | 2.71   | 2.73   | 2.724285714285714  | 2.73   | 0.009035079029052493 |
                     |       | -0.73% | -1.09% | -0.73%             | -0.36% | -23.30%              | <not defined>
  sysbenchthread_420 |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.74   | 2.79   | 2.76               | 2.76   | 0.017728105208558324 |
  (B) 84f99e060a66   | 7     | 2.7    | 2.77   | 2.73               | 2.72   | 0.0239045721866878   |
                     |       | -1.46% | -0.72% | -1.09%             | -1.45% | +34.84%              | <not defined>
  sysbenchthread_451 |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.72   | 2.77   | 2.7457142857142856 | 2.75   | 0.018405855323892807 |
  (B) 84f99e060a66   | 7     | 2.7    | 2.74   | 2.72               | 2.72   | 0.014142135623731006 |
                     |       | -0.74% | -1.08% | -0.94%             | -1.09% | -23.17%              | <not defined>
  sysbenchthread_482 |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.71   | 2.8    | 2.742857142857143  | 2.74   | 0.028139593719417412 |
  (B) 84f99e060a66   | 7     | 2.69   | 2.75   | 2.727142857142857  | 2.73   | 0.02050385727772484  |
                     |       | -0.74% | -1.79% | -0.57%             | -0.36% | -27.14%              | <not defined>
  sysbenchthread_512 |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.7    | 2.75   | 2.7399999999999998 | 2.75   | 0.01690308509457031  |
  (B) 84f99e060a66   | 7     | 2.71   | 2.75   | 2.7285714285714286 | 2.73   | 0.01245399698154476  |
                     |       | +0.37% | +0.00% | -0.42%             | -0.73% | -26.32%              | <not defined>
  sysbenchthread_max |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.7    | 2.75   | 2.7399999999999998 | 2.75   | 0.01690308509457031  |
  (B) 84f99e060a66   | 7     | 2.71   | 2.75   | 2.7285714285714286 | 2.73   | 0.01245399698154476  |
                     |       | +0.37% | +0.00% | -0.42%             | -0.73% | -26.32%              | - is good
  sysbenchthread_min |       |        |        |                    |        |                      |
  (A) 4db0b9c9d0f3   | 7     | 2.71   | 2.76   | 2.742857142857143  | 2.75   | 0.014846149779161799 |
  (B) 84f99e060a66   | 7     | 2.67   | 2.74   | 2.7128571428571426 | 2.71   | 0.022497165354319542 |
                     |       | -1.48% | -0.72% | -1.09%             | -1.45% | +51.54%              | <not defined>

[*] BENCHMARK
Name: tpcc_spark
Description: spark sql drived by tpcc.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 0
  machine_total_logical_cores  | 256
  machine_total_physical_cores | 128
  machine_total_ram_gib        | 1024

[*] METRICS
           LABEL           | COUNT |  MIN   |  MAX   |  MEAN  | MEDIAN | STDDEV | DIRECTION
---------------------------+-------+--------+--------+--------+--------+--------+------------
  geo_mean_of_running_time |       |        |        |        |        |        |
  (A) 4db0b9c9d0f3         | 1     | 196.95 | 196.95 | 196.95 | 196.95 | 0      |
  (B) 84f99e060a66         | 1     | 194.48 | 194.48 | 194.48 | 194.48 | 0      |
                           |       | -1.25% | -1.25% | -1.25% | -1.25% | ---    | - is good

[*] BENCHMARK
Name: unixbench
Description: UnixBench is the original BYTE UNIX benchmark suite,
updated and revised by many people over the years. The purpose of
UnixBench is to provide a basic indicator of the performance of a
Unix-like system; hence, multiple tests are used to test various
aspects of the system's performance. These test results are then
compared to the scores from a baseline system to produce an index
value, which is generally easier to handle than the raw scores. The
entire set of index values is then combined to make an overall index
for the system.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 0
  machine_total_logical_cores  | 112
  machine_total_physical_cores | 56
  machine_total_ram_gib        | 384

[*] METRICS
            LABEL            | COUNT |      MIN       |      MAX       |          MEAN          |     MEDIAN     |       STDDEV       |   DIRECTION
-----------------------------+-------+----------------+----------------+------------------------+----------------+--------------------+----------------
  unixbench_dhry2reg_1       |       |                |                |                        |                |                    |
  (A) 4db0b9c9d0f3           | 5     | 4.34865736e+07 | 4.39381302e+07 | 4.360061844e+07        | 4.35072825e+07 | 170784.08300336136 |
  (B) 84f99e060a66           | 5     | 4.30655821e+07 | 4.41854107e+07 | 4.3621910260000005e+07 | 4.34371903e+07 | 417733.3283639879  |
                             |       | -0.97%         | +0.56%         | +0.05%                 | -0.16%         | +144.60%           | <not defined>
  unixbench_dhry2reg_lbr_1   |       |                |                |                        |                |                    |
  (A) 4db0b9c9d0f3           | 5     | 4.27938746e+07 | 4.36690393e+07 | 4.316068858e+07        | 4.30693911e+07 | 331395.00245936995 |
  (B) 84f99e060a66           | 5     | 4.27925274e+07 | 4.35598498e+07 | 4.3150110199999996e+07 | 4.31818175e+07 | 285228.20294770785 |
                             |       | -0.00%         | -0.25%         | -0.02%                 | +0.26%         | -13.93%            | <not defined>
  unixbench_dhry2reg_lbr_max |       |                |                |                        |                |                    |
  (A) 4db0b9c9d0f3           | 5     | 4.27938746e+07 | 4.36690393e+07 | 4.316068858e+07        | 4.30693911e+07 | 331395.00245936995 |
  (B) 84f99e060a66           | 5     | 4.27925274e+07 | 4.35598498e+07 | 4.3150110199999996e+07 | 4.31818175e+07 | 285228.20294770785 |
                             |       | -0.00%         | -0.25%         | -0.02%                 | +0.26%         | -13.93%            | + is good
  unixbench_dhry2reg_max     |       |                |                |                        |                |                    |
  (A) 4db0b9c9d0f3           | 5     | 4.34865736e+07 | 4.39381302e+07 | 4.360061844e+07        | 4.35072825e+07 | 170784.08300336136 |
  (B) 84f99e060a66           | 5     | 4.30655821e+07 | 4.41854107e+07 | 4.3621910260000005e+07 | 4.34371903e+07 | 417733.3283639879  |
                             |       | -0.97%         | +0.56%         | +0.05%                 | -0.16%         | +144.60%           | + is good
  unixbench_execl_1          |       |                |                |                        |                |                    |
  (A) 4db0b9c9d0f3           | 5     | 4727.9         | 4792.1         | 4765.64                | 4768           | 21.35093440578216  |
  (B) 84f99e060a66           | 5     | 4723.7         | 4759.4         | 4742.96                | 4741.8         | 11.76343487251919  |
                             |       | -0.09%         | -0.68%         | -0.48%                 | -0.55%         | -44.90%            | <not defined>
  unixbench_execl_max        |       |                |                |                        |                |                    |
  (A) 4db0b9c9d0f3           | 5     | 4727.9         | 4792.1         | 4765.64                | 4768           | 21.35093440578216  |
  (B) 84f99e060a66           | 5     | 4723.7         | 4759.4         | 4742.96                | 4741.8         | 11.76343487251919  |
                             |       | -0.09%         | -0.68%         | -0.48%                 | -0.55%         | -44.90%            | + is good
  unixbench_pipe_1           |       |                |                |                        |                |                    |
  (A) 4db0b9c9d0f3           | 5     | 920680.7       | 937292         | 928936.9800000001      | 929920.6       | 6293.150348243733  |
  (B) 84f99e060a66           | 5     | 933339.5       | 945690.7       | 938268.84              | 937086.6       | 4363.416421383563  |
                             |       | +1.37%         | +0.90%         | +1.00%                 | +0.77%         | -30.66%            | <not defined>
  unixbench_pipe_max         |       |                |                |                        |                |                    |
  (A) 4db0b9c9d0f3           | 5     | 920680.7       | 937292         | 928936.9800000001      | 929920.6       | 6293.150348243733  |
  (B) 84f99e060a66           | 5     | 933339.5       | 945690.7       | 938268.84              | 937086.6       | 4363.416421383563  |
                             |       | +1.37%         | +0.90%         | +1.00%                 | +0.77%         | -30.66%            | + is good
  unixbench_spawn_1          |       |                |                |                        |                |                    |
  (A) 4db0b9c9d0f3           | 5     | 4387.2         | 4430.4         | 4409.04                | 4402.9         | 15.872315521057407 |
  (B) 84f99e060a66           | 5     | 4320           | 4426.7         | 4377.78                | 4398.3         | 43.46213984607764  |
                             |       | -1.53%         | -0.08%         | -0.71%                 | -0.10%         | +173.82%           | <not defined>
  unixbench_spawn_max        |       |                |                |                        |                |                    |
  (A) 4db0b9c9d0f3           | 5     | 4387.2         | 4430.4         | 4409.04                | 4402.9         | 15.872315521057407 |
  (B) 84f99e060a66           | 5     | 4320           | 4426.7         | 4377.78                | 4398.3         | 43.46213984607764  |
                             |       | -1.53%         | -0.08%         | -0.71%                 | -0.10%         | +173.82%           | + is good
  unixbench_syscall_1        |       |                |                |                        |                |                    |
  (A) 4db0b9c9d0f3           | 5     | 478285         | 486881.9       | 484686.64              | 485948.1       | 3236.679989495396  |
  (B) 84f99e060a66           | 5     | 487976         | 495269.6       | 491429.3               | 491264.2       | 2527.8799022105263 |
                             |       | +2.03%         | +1.72%         | +1.39%                 | +1.09%         | -21.90%            | <not defined>
  unixbench_syscall_max      |       |                |                |                        |                |                    |
  (A) 4db0b9c9d0f3           | 5     | 478285         | 486881.9       | 484686.64              | 485948.1       | 3236.679989495396  |
  (B) 84f99e060a66           | 5     | 487976         | 495269.6       | 491429.3               | 491264.2       | 2527.8799022105263 |
                             |       | +2.03%         | +1.72%         | +1.39%                 | +1.09%         | -21.90%            | + is good

[*] BENCHMARK
Name: vm-scalability
Description: The motivation behind this suite is to exercise functions
and regions of Linux kernel mm/ which are of interest to us.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 0
  machine_total_logical_cores  | 112
  machine_total_physical_cores | 56
  machine_total_ram_gib        | 384

[*] METRICS
                 LABEL                 | COUNT |       MIN        |       MAX        |       MEAN        |      MEDIAN      |         STDDEV         | DIRECTION
---------------------------------------+-------+------------------+------------------+-------------------+------------------+------------------------+------------
  300s_128G_truncate_throughput        |       |                  |                  |                   |                  |                        |
  (A) 4db0b9c9d0f3                     | 5     | 3.1402485727e+10 | 3.3524240955e+10 | 3.23467699884e+10 | 3.2213360935e+10 | 6.990084677110682e+08  |
  (B) 84f99e060a66                     | 5     | 3.2437265981e+10 | 3.3540529674e+10 | 3.2928619191e+10  | 3.2833916458e+10 | 3.9777278144953775e+08 |
                                       |       | +3.30%           | +0.05%           | +1.80%            | +1.93%           | -43.09%                | + is good
  300s_512G_anon_wx_rand_mt_throughput |       |                  |                  |                   |                  |                        |
  (A) 4db0b9c9d0f3                     | 5     | 7.913872e+06     | 8.193942e+06     | 8.019662e+06      | 7.956032e+06     | 105183.50364196849     |
  (B) 84f99e060a66                     | 5     | 8.110757e+06     | 8.295155e+06     | 8.1804372e+06     | 8.156313e+06     | 62091.47716844879      |
                                       |       | +2.49%           | +1.24%           | +2.00%            | +2.52%           | -40.97%                | + is good

[*] BENCHMARK
Name: will-it-scale
Description: Will It Scale takes a testcase and runs it from 1 through
to n parallel copies to see if the testcase will scale. It builds both
a process and threads based test in order to see any differences
between the two.

[*] KERNELS
+-----------------+
| BASE KERNEL (A) |
+-----------------+
CommitId: 4db0b9c9d0f3ee84d954c94d7f48aeccc348b354
Describe: v5.16-rc7-12-g4db0b9c9d0f3

+-----------------+
| TEST KERNEL (B) |
+-----------------+
CommitId: 84f99e060a66b8ae5f1867e60f73e47a77a567aa
Describe: v5.16-rc7-22-g84f99e060a66

[*] TAGS
             LABEL             |                  VALUE
-------------------------------+-------------------------------------------
  machine_platform_arch        | X86_64
  machine_total_disks          | 0
  machine_total_logical_cores  | 112
  machine_total_physical_cores | 56
  machine_total_ram_gib        | 384

[*] METRICS
               LABEL               | COUNT |      MIN      |      MAX      |     MEAN      |    MEDIAN     | STDDEV | DIRECTION
-----------------------------------+-------+---------------+---------------+---------------+---------------+--------+------------
  brk1_per_process_ops             |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.182196e+06  | 1.182196e+06  | 1.182196e+06  | 1.182196e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 1.180751e+06  | 1.180751e+06  | 1.180751e+06  | 1.180751e+06  | 0      |
                                   |       | -0.12%        | -0.12%        | -0.12%        | -0.12%        | ---    | + is good
  brk1_per_thread_ops              |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 262491        | 262491        | 262491        | 262491        | 0      |
  (B) 84f99e060a66                 | 1     | 271219        | 271219        | 271219        | 271219        | 0      |
                                   |       | +3.33%        | +3.33%        | +3.33%        | +3.33%        | ---    | + is good
  brk1_scalability                 |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.110562      | 1.110562      | 1.110562      | 1.110562      | 0      |
  (B) 84f99e060a66                 | 1     | 1.118674      | 1.118674      | 1.118674      | 1.118674      | 0      |
                                   |       | +0.73%        | +0.73%        | +0.73%        | +0.73%        | ---    | + is good
  context_switch1_per_process_ops  |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 329041        | 329041        | 329041        | 329041        | 0      |
  (B) 84f99e060a66                 | 1     | 335598        | 335598        | 335598        | 335598        | 0      |
                                   |       | +1.99%        | +1.99%        | +1.99%        | +1.99%        | ---    | + is good
  context_switch1_per_thread_ops   |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 297899        | 297899        | 297899        | 297899        | 0      |
  (B) 84f99e060a66                 | 1     | 304690        | 304690        | 304690        | 304690        | 0      |
                                   |       | +2.28%        | +2.28%        | +2.28%        | +2.28%        | ---    | + is good
  context_switch1_scalability      |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.093104      | 1.093104      | 1.093104      | 1.093104      | 0      |
  (B) 84f99e060a66                 | 1     | 1.166822      | 1.166822      | 1.166822      | 1.166822      | 0      |
                                   |       | +6.74%        | +6.74%        | +6.74%        | +6.74%        | ---    | + is good
  dup1_per_process_ops             |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 914239        | 914239        | 914239        | 914239        | 0      |
  (B) 84f99e060a66                 | 1     | 916208        | 916208        | 916208        | 916208        | 0      |
                                   |       | +0.22%        | +0.22%        | +0.22%        | +0.22%        | ---    | + is good
  dup1_per_thread_ops              |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 319906        | 319906        | 319906        | 319906        | 0      |
  (B) 84f99e060a66                 | 1     | 313047        | 313047        | 313047        | 313047        | 0      |
                                   |       | -2.14%        | -2.14%        | -2.14%        | -2.14%        | ---    | + is good
  dup1_scalability                 |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.988697      | 0.988697      | 0.988697      | 0.988697      | 0      |
  (B) 84f99e060a66                 | 1     | 0.993808      | 0.993808      | 0.993808      | 0.993808      | 0      |
                                   |       | +0.52%        | +0.52%        | +0.52%        | +0.52%        | ---    | + is good
  eventfd1_per_process_ops         |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 883174        | 883174        | 883174        | 883174        | 0      |
  (B) 84f99e060a66                 | 1     | 884553        | 884553        | 884553        | 884553        | 0      |
                                   |       | +0.16%        | +0.16%        | +0.16%        | +0.16%        | ---    | + is good
  eventfd1_per_thread_ops          |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 840497        | 840497        | 840497        | 840497        | 0      |
  (B) 84f99e060a66                 | 1     | 839392        | 839392        | 839392        | 839392        | 0      |
                                   |       | -0.13%        | -0.13%        | -0.13%        | -0.13%        | ---    | + is good
  eventfd1_scalability             |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.984995      | 0.984995      | 0.984995      | 0.984995      | 0      |
  (B) 84f99e060a66                 | 1     | 0.992564      | 0.992564      | 0.992564      | 0.992564      | 0      |
                                   |       | +0.77%        | +0.77%        | +0.77%        | +0.77%        | ---    | + is good
  futex1_per_process_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.685172e+06  | 1.685172e+06  | 1.685172e+06  | 1.685172e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 1.690119e+06  | 1.690119e+06  | 1.690119e+06  | 1.690119e+06  | 0      |
                                   |       | +0.29%        | +0.29%        | +0.29%        | +0.29%        | ---    | + is good
  futex1_per_thread_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.682975e+06  | 1.682975e+06  | 1.682975e+06  | 1.682975e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 1.687864e+06  | 1.687864e+06  | 1.687864e+06  | 1.687864e+06  | 0      |
                                   |       | +0.29%        | +0.29%        | +0.29%        | +0.29%        | ---    | + is good
  futex1_scalability               |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.977692      | 0.977692      | 0.977692      | 0.977692      | 0      |
  (B) 84f99e060a66                 | 1     | 0.992208      | 0.992208      | 0.992208      | 0.992208      | 0      |
                                   |       | +1.48%        | +1.48%        | +1.48%        | +1.48%        | ---    | + is good
  futex2_per_process_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.477896e+06  | 1.477896e+06  | 1.477896e+06  | 1.477896e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 1.477849e+06  | 1.477849e+06  | 1.477849e+06  | 1.477849e+06  | 0      |
                                   |       | -0.00%        | -0.00%        | -0.00%        | -0.00%        | ---    | + is good
  futex2_per_thread_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.475865e+06  | 1.475865e+06  | 1.475865e+06  | 1.475865e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 1.472128e+06  | 1.472128e+06  | 1.472128e+06  | 1.472128e+06  | 0      |
                                   |       | -0.25%        | -0.25%        | -0.25%        | -0.25%        | ---    | + is good
  futex2_scalability               |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.987299      | 0.987299      | 0.987299      | 0.987299      | 0      |
  (B) 84f99e060a66                 | 1     | 0.998003      | 0.998003      | 0.998003      | 0.998003      | 0      |
                                   |       | +1.08%        | +1.08%        | +1.08%        | +1.08%        | ---    | + is good
  futex3_per_process_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.947335e+06  | 1.947335e+06  | 1.947335e+06  | 1.947335e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 1.947543e+06  | 1.947543e+06  | 1.947543e+06  | 1.947543e+06  | 0      |
                                   |       | +0.01%        | +0.01%        | +0.01%        | +0.01%        | ---    | + is good
  futex3_per_thread_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.94926e+06   | 1.94926e+06   | 1.94926e+06   | 1.94926e+06   | 0      |
  (B) 84f99e060a66                 | 1     | 1.943119e+06  | 1.943119e+06  | 1.943119e+06  | 1.943119e+06  | 0      |
                                   |       | -0.32%        | -0.32%        | -0.32%        | -0.32%        | ---    | + is good
  futex3_scalability               |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.98336       | 0.98336       | 0.98336       | 0.98336       | 0      |
  (B) 84f99e060a66                 | 1     | 0.993609      | 0.993609      | 0.993609      | 0.993609      | 0      |
                                   |       | +1.04%        | +1.04%        | +1.04%        | +1.04%        | ---    | + is good
  futex4_per_process_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.663509e+06  | 1.663509e+06  | 1.663509e+06  | 1.663509e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 1.661453e+06  | 1.661453e+06  | 1.661453e+06  | 1.661453e+06  | 0      |
                                   |       | -0.12%        | -0.12%        | -0.12%        | -0.12%        | ---    | + is good
  futex4_per_thread_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.663546e+06  | 1.663546e+06  | 1.663546e+06  | 1.663546e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 1.663659e+06  | 1.663659e+06  | 1.663659e+06  | 1.663659e+06  | 0      |
                                   |       | +0.01%        | +0.01%        | +0.01%        | +0.01%        | ---    | + is good
  futex4_scalability               |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.983372      | 0.983372      | 0.983372      | 0.983372      | 0      |
  (B) 84f99e060a66                 | 1     | 0.994158      | 0.994158      | 0.994158      | 0.994158      | 0      |
                                   |       | +1.10%        | +1.10%        | +1.10%        | +1.10%        | ---    | + is good
  getppid1_per_process_ops         |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 2.040868e+06  | 2.040868e+06  | 2.040868e+06  | 2.040868e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 2.036659e+06  | 2.036659e+06  | 2.036659e+06  | 2.036659e+06  | 0      |
                                   |       | -0.21%        | -0.21%        | -0.21%        | -0.21%        | ---    | + is good
  getppid1_per_thread_ops          |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 2.037917e+06  | 2.037917e+06  | 2.037917e+06  | 2.037917e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 2.035906e+06  | 2.035906e+06  | 2.035906e+06  | 2.035906e+06  | 0      |
                                   |       | -0.10%        | -0.10%        | -0.10%        | -0.10%        | ---    | + is good
  getppid1_scalability             |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.983373      | 0.983373      | 0.983373      | 0.983373      | 0      |
  (B) 84f99e060a66                 | 1     | 0.993657      | 0.993657      | 0.993657      | 0.993657      | 0      |
                                   |       | +1.05%        | +1.05%        | +1.05%        | +1.05%        | ---    | + is good
  lock1_per_process_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.110729e+06  | 1.110729e+06  | 1.110729e+06  | 1.110729e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 1.10133e+06   | 1.10133e+06   | 1.10133e+06   | 1.10133e+06   | 0      |
                                   |       | -0.85%        | -0.85%        | -0.85%        | -0.85%        | ---    | + is good
  lock1_per_thread_ops             |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 741007        | 741007        | 741007        | 741007        | 0      |
  (B) 84f99e060a66                 | 1     | 751044        | 751044        | 751044        | 751044        | 0      |
                                   |       | +1.35%        | +1.35%        | +1.35%        | +1.35%        | ---    | + is good
  lock1_scalability                |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.945011      | 0.945011      | 0.945011      | 0.945011      | 0      |
  (B) 84f99e060a66                 | 1     | 0.968271      | 0.968271      | 0.968271      | 0.968271      | 0      |
                                   |       | +2.46%        | +2.46%        | +2.46%        | +2.46%        | ---    | + is good
  lseek1_per_process_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 2.029997e+06  | 2.029997e+06  | 2.029997e+06  | 2.029997e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 2.029771e+06  | 2.029771e+06  | 2.029771e+06  | 2.029771e+06  | 0      |
                                   |       | -0.01%        | -0.01%        | -0.01%        | -0.01%        | ---    | + is good
  lseek1_per_thread_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.874244e+06  | 1.874244e+06  | 1.874244e+06  | 1.874244e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 1.877163e+06  | 1.877163e+06  | 1.877163e+06  | 1.877163e+06  | 0      |
                                   |       | +0.16%        | +0.16%        | +0.16%        | +0.16%        | ---    | + is good
  lseek1_scalability               |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.985154      | 0.985154      | 0.985154      | 0.985154      | 0      |
  (B) 84f99e060a66                 | 1     | 0.993361      | 0.993361      | 0.993361      | 0.993361      | 0      |
                                   |       | +0.83%        | +0.83%        | +0.83%        | +0.83%        | ---    | + is good
  lseek2_per_process_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 2.026038e+06  | 2.026038e+06  | 2.026038e+06  | 2.026038e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 2.022826e+06  | 2.022826e+06  | 2.022826e+06  | 2.022826e+06  | 0      |
                                   |       | -0.16%        | -0.16%        | -0.16%        | -0.16%        | ---    | + is good
  lseek2_per_thread_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.873717e+06  | 1.873717e+06  | 1.873717e+06  | 1.873717e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 1.872512e+06  | 1.872512e+06  | 1.872512e+06  | 1.872512e+06  | 0      |
                                   |       | -0.06%        | -0.06%        | -0.06%        | -0.06%        | ---    | + is good
  lseek2_scalability               |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.986396      | 0.986396      | 0.986396      | 0.986396      | 0      |
  (B) 84f99e060a66                 | 1     | 0.993642      | 0.993642      | 0.993642      | 0.993642      | 0      |
                                   |       | +0.73%        | +0.73%        | +0.73%        | +0.73%        | ---    | + is good
  malloc1_per_process_ops          |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 97835         | 97835         | 97835         | 97835         | 0      |
  (B) 84f99e060a66                 | 1     | 98645         | 98645         | 98645         | 98645         | 0      |
                                   |       | +0.83%        | +0.83%        | +0.83%        | +0.83%        | ---    | + is good
  malloc1_per_thread_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 41808         | 41808         | 41808         | 41808         | 0      |
  (B) 84f99e060a66                 | 1     | 42333         | 42333         | 42333         | 42333         | 0      |
                                   |       | +1.26%        | +1.26%        | +1.26%        | +1.26%        | ---    | + is good
  malloc1_scalability              |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.306071      | 0.306071      | 0.306071      | 0.306071      | 0      |
  (B) 84f99e060a66                 | 1     | 0.311208      | 0.311208      | 0.311208      | 0.311208      | 0      |
                                   |       | +1.68%        | +1.68%        | +1.68%        | +1.68%        | ---    | + is good
  malloc2_per_process_ops          |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 8.5964713e+07 | 8.5964713e+07 | 8.5964713e+07 | 8.5964713e+07 | 0      |
  (B) 84f99e060a66                 | 1     | 8.5729456e+07 | 8.5729456e+07 | 8.5729456e+07 | 8.5729456e+07 | 0      |
                                   |       | -0.27%        | -0.27%        | -0.27%        | -0.27%        | ---    | + is good
  malloc2_per_thread_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 8.5909382e+07 | 8.5909382e+07 | 8.5909382e+07 | 8.5909382e+07 | 0      |
  (B) 84f99e060a66                 | 1     | 8.5643483e+07 | 8.5643483e+07 | 8.5643483e+07 | 8.5643483e+07 | 0      |
                                   |       | -0.31%        | -0.31%        | -0.31%        | -0.31%        | ---    | + is good
  malloc2_scalability              |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.990744      | 0.990744      | 0.990744      | 0.990744      | 0      |
  (B) 84f99e060a66                 | 1     | 0.999912      | 0.999912      | 0.999912      | 0.999912      | 0      |
                                   |       | +0.93%        | +0.93%        | +0.93%        | +0.93%        | ---    | + is good
  mmap1_per_process_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 405846        | 405846        | 405846        | 405846        | 0      |
  (B) 84f99e060a66                 | 1     | 411130        | 411130        | 411130        | 411130        | 0      |
                                   |       | +1.30%        | +1.30%        | +1.30%        | +1.30%        | ---    | + is good
  mmap1_per_thread_ops             |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 123503        | 123503        | 123503        | 123503        | 0      |
  (B) 84f99e060a66                 | 1     | 127042        | 127042        | 127042        | 127042        | 0      |
                                   |       | +2.87%        | +2.87%        | +2.87%        | +2.87%        | ---    | + is good
  mmap1_scalability                |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.975408      | 0.975408      | 0.975408      | 0.975408      | 0      |
  (B) 84f99e060a66                 | 1     | 0.978192      | 0.978192      | 0.978192      | 0.978192      | 0      |
                                   |       | +0.29%        | +0.29%        | +0.29%        | +0.29%        | ---    | + is good
  mmap2_per_process_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 358352        | 358352        | 358352        | 358352        | 0      |
  (B) 84f99e060a66                 | 1     | 363506        | 363506        | 363506        | 363506        | 0      |
                                   |       | +1.44%        | +1.44%        | +1.44%        | +1.44%        | ---    | + is good
  mmap2_per_thread_ops             |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 110050        | 110050        | 110050        | 110050        | 0      |
  (B) 84f99e060a66                 | 1     | 113049        | 113049        | 113049        | 113049        | 0      |
                                   |       | +2.73%        | +2.73%        | +2.73%        | +2.73%        | ---    | + is good
  mmap2_scalability                |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.973879      | 0.973879      | 0.973879      | 0.973879      | 0      |
  (B) 84f99e060a66                 | 1     | 0.969964      | 0.969964      | 0.969964      | 0.969964      | 0      |
                                   |       | -0.40%        | -0.40%        | -0.40%        | -0.40%        | ---    | + is good
  open1_per_process_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 204637        | 204637        | 204637        | 204637        | 0      |
  (B) 84f99e060a66                 | 1     | 206390        | 206390        | 206390        | 206390        | 0      |
                                   |       | +0.86%        | +0.86%        | +0.86%        | +0.86%        | ---    | + is good
  open1_per_thread_ops             |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 182428        | 182428        | 182428        | 182428        | 0      |
  (B) 84f99e060a66                 | 1     | 184978        | 184978        | 184978        | 184978        | 0      |
                                   |       | +1.40%        | +1.40%        | +1.40%        | +1.40%        | ---    | + is good
  open1_scalability                |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.270073      | 0.270073      | 0.270073      | 0.270073      | 0      |
  (B) 84f99e060a66                 | 1     | 0.280171      | 0.280171      | 0.280171      | 0.280171      | 0      |
                                   |       | +3.74%        | +3.74%        | +3.74%        | +3.74%        | ---    | + is good
  open2_per_process_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 187624        | 187624        | 187624        | 187624        | 0      |
  (B) 84f99e060a66                 | 1     | 188959        | 188959        | 188959        | 188959        | 0      |
                                   |       | +0.71%        | +0.71%        | +0.71%        | +0.71%        | ---    | + is good
  open2_per_thread_ops             |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 175546        | 175546        | 175546        | 175546        | 0      |
  (B) 84f99e060a66                 | 1     | 178140        | 178140        | 178140        | 178140        | 0      |
                                   |       | +1.48%        | +1.48%        | +1.48%        | +1.48%        | ---    | + is good
  open2_scalability                |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.240907      | 0.240907      | 0.240907      | 0.240907      | 0      |
  (B) 84f99e060a66                 | 1     | 0.246053      | 0.246053      | 0.246053      | 0.246053      | 0      |
                                   |       | +2.14%        | +2.14%        | +2.14%        | +2.14%        | ---    | + is good
  page_fault1_per_process_ops      |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 296104        | 296104        | 296104        | 296104        | 0      |
  (B) 84f99e060a66                 | 1     | 293518        | 293518        | 293518        | 293518        | 0      |
                                   |       | -0.87%        | -0.87%        | -0.87%        | -0.87%        | ---    | + is good
  page_fault1_per_thread_ops       |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 245302        | 245302        | 245302        | 245302        | 0      |
  (B) 84f99e060a66                 | 1     | 239501        | 239501        | 239501        | 239501        | 0      |
                                   |       | -2.36%        | -2.36%        | -2.36%        | -2.36%        | ---    | + is good
  page_fault1_scalability          |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.591157      | 0.591157      | 0.591157      | 0.591157      | 0      |
  (B) 84f99e060a66                 | 1     | 0.557408      | 0.557408      | 0.557408      | 0.557408      | 0      |
                                   |       | -5.71%        | -5.71%        | -5.71%        | -5.71%        | ---    | + is good
  page_fault2_per_process_ops      |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 263242        | 263242        | 263242        | 263242        | 0      |
  (B) 84f99e060a66                 | 1     | 265093        | 265093        | 265093        | 265093        | 0      |
                                   |       | +0.70%        | +0.70%        | +0.70%        | +0.70%        | ---    | + is good
  page_fault2_per_thread_ops       |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 209335        | 209335        | 209335        | 209335        | 0      |
  (B) 84f99e060a66                 | 1     | 204871        | 204871        | 204871        | 204871        | 0      |
                                   |       | -2.13%        | -2.13%        | -2.13%        | -2.13%        | ---    | + is good
  page_fault2_scalability          |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.583137      | 0.583137      | 0.583137      | 0.583137      | 0      |
  (B) 84f99e060a66                 | 1     | 0.588389      | 0.588389      | 0.588389      | 0.588389      | 0      |
                                   |       | +0.90%        | +0.90%        | +0.90%        | +0.90%        | ---    | + is good
  page_fault3_per_process_ops      |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 649712        | 649712        | 649712        | 649712        | 0      |
  (B) 84f99e060a66                 | 1     | 652223        | 652223        | 652223        | 652223        | 0      |
                                   |       | +0.39%        | +0.39%        | +0.39%        | +0.39%        | ---    | + is good
  page_fault3_per_thread_ops       |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 490220        | 490220        | 490220        | 490220        | 0      |
  (B) 84f99e060a66                 | 1     | 497988        | 497988        | 497988        | 497988        | 0      |
                                   |       | +1.58%        | +1.58%        | +1.58%        | +1.58%        | ---    | + is good
  page_fault3_scalability          |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.985479      | 0.985479      | 0.985479      | 0.985479      | 0      |
  (B) 84f99e060a66                 | 1     | 0.998238      | 0.998238      | 0.998238      | 0.998238      | 0      |
                                   |       | +1.29%        | +1.29%        | +1.29%        | +1.29%        | ---    | + is good
  pipe1_per_process_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 768258        | 768258        | 768258        | 768258        | 0      |
  (B) 84f99e060a66                 | 1     | 767503        | 767503        | 767503        | 767503        | 0      |
                                   |       | -0.10%        | -0.10%        | -0.10%        | -0.10%        | ---    | + is good
  pipe1_per_thread_ops             |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 735575        | 735575        | 735575        | 735575        | 0      |
  (B) 84f99e060a66                 | 1     | 734427        | 734427        | 734427        | 734427        | 0      |
                                   |       | -0.16%        | -0.16%        | -0.16%        | -0.16%        | ---    | + is good
  pipe1_scalability                |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.992941      | 0.992941      | 0.992941      | 0.992941      | 0      |
  (B) 84f99e060a66                 | 1     | 0.99995       | 0.99995       | 0.99995       | 0.99995       | 0      |
                                   |       | +0.71%        | +0.71%        | +0.71%        | +0.71%        | ---    | + is good
  poll1_per_process_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.704129e+06  | 1.704129e+06  | 1.704129e+06  | 1.704129e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 1.701779e+06  | 1.701779e+06  | 1.701779e+06  | 1.701779e+06  | 0      |
                                   |       | -0.14%        | -0.14%        | -0.14%        | -0.14%        | ---    | + is good
  poll1_per_thread_ops             |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.626484e+06  | 1.626484e+06  | 1.626484e+06  | 1.626484e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 1.625619e+06  | 1.625619e+06  | 1.625619e+06  | 1.625619e+06  | 0      |
                                   |       | -0.05%        | -0.05%        | -0.05%        | -0.05%        | ---    | + is good
  poll1_scalability                |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.98568       | 0.98568       | 0.98568       | 0.98568       | 0      |
  (B) 84f99e060a66                 | 1     | 0.993592      | 0.993592      | 0.993592      | 0.993592      | 0      |
                                   |       | +0.80%        | +0.80%        | +0.80%        | +0.80%        | ---    | + is good
  poll2_per_process_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 520641        | 520641        | 520641        | 520641        | 0      |
  (B) 84f99e060a66                 | 1     | 515893        | 515893        | 515893        | 515893        | 0      |
                                   |       | -0.91%        | -0.91%        | -0.91%        | -0.91%        | ---    | + is good
  poll2_per_thread_ops             |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 201450        | 201450        | 201450        | 201450        | 0      |
  (B) 84f99e060a66                 | 1     | 201523        | 201523        | 201523        | 201523        | 0      |
                                   |       | +0.04%        | +0.04%        | +0.04%        | +0.04%        | ---    | + is good
  poll2_scalability                |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.996518      | 0.996518      | 0.996518      | 0.996518      | 0      |
  (B) 84f99e060a66                 | 1     | 1.018321      | 1.018321      | 1.018321      | 1.018321      | 0      |
                                   |       | +2.19%        | +2.19%        | +2.19%        | +2.19%        | ---    | + is good
  posix_semaphore1_per_process_ops |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 4.16846e+07   | 4.16846e+07   | 4.16846e+07   | 4.16846e+07   | 0      |
  (B) 84f99e060a66                 | 1     | 4.153911e+07  | 4.153911e+07  | 4.153911e+07  | 4.153911e+07  | 0      |
                                   |       | -0.35%        | -0.35%        | -0.35%        | -0.35%        | ---    | + is good
  posix_semaphore1_per_thread_ops  |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 4.1673991e+07 | 4.1673991e+07 | 4.1673991e+07 | 4.1673991e+07 | 0      |
  (B) 84f99e060a66                 | 1     | 4.1539286e+07 | 4.1539286e+07 | 4.1539286e+07 | 4.1539286e+07 | 0      |
                                   |       | -0.32%        | -0.32%        | -0.32%        | -0.32%        | ---    | + is good
  posix_semaphore1_scalability     |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.990928      | 0.990928      | 0.990928      | 0.990928      | 0      |
  (B) 84f99e060a66                 | 1     | 0.998181      | 0.998181      | 0.998181      | 0.998181      | 0      |
                                   |       | +0.73%        | +0.73%        | +0.73%        | +0.73%        | ---    | + is good
  pread1_per_process_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 964152        | 964152        | 964152        | 964152        | 0      |
  (B) 84f99e060a66                 | 1     | 961951        | 961951        | 961951        | 961951        | 0      |
                                   |       | -0.23%        | -0.23%        | -0.23%        | -0.23%        | ---    | + is good
  pread1_per_thread_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 942991        | 942991        | 942991        | 942991        | 0      |
  (B) 84f99e060a66                 | 1     | 941389        | 941389        | 941389        | 941389        | 0      |
                                   |       | -0.17%        | -0.17%        | -0.17%        | -0.17%        | ---    | + is good
  pread1_scalability               |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.430538      | 0.430538      | 0.430538      | 0.430538      | 0      |
  (B) 84f99e060a66                 | 1     | 0.442705      | 0.442705      | 0.442705      | 0.442705      | 0      |
                                   |       | +2.83%        | +2.83%        | +2.83%        | +2.83%        | ---    | + is good
  pread2_per_process_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 461091        | 461091        | 461091        | 461091        | 0      |
  (B) 84f99e060a66                 | 1     | 469057        | 469057        | 469057        | 469057        | 0      |
                                   |       | +1.73%        | +1.73%        | +1.73%        | +1.73%        | ---    | + is good
  pread2_per_thread_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 458218        | 458218        | 458218        | 458218        | 0      |
  (B) 84f99e060a66                 | 1     | 462848        | 462848        | 462848        | 462848        | 0      |
                                   |       | +1.01%        | +1.01%        | +1.01%        | +1.01%        | ---    | + is good
  pread2_scalability               |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.072925      | 0.072925      | 0.072925      | 0.072925      | 0      |
  (B) 84f99e060a66                 | 1     | 0.074731      | 0.074731      | 0.074731      | 0.074731      | 0      |
                                   |       | +2.48%        | +2.48%        | +2.48%        | +2.48%        | ---    | + is good
  pread3_per_process_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 957692        | 957692        | 957692        | 957692        | 0      |
  (B) 84f99e060a66                 | 1     | 952552        | 952552        | 952552        | 952552        | 0      |
                                   |       | -0.54%        | -0.54%        | -0.54%        | -0.54%        | ---    | + is good
  pread3_per_thread_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 938537        | 938537        | 938537        | 938537        | 0      |
  (B) 84f99e060a66                 | 1     | 928731        | 928731        | 928731        | 928731        | 0      |
                                   |       | -1.04%        | -1.04%        | -1.04%        | -1.04%        | ---    | + is good
  pread3_scalability               |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.431635      | 0.431635      | 0.431635      | 0.431635      | 0      |
  (B) 84f99e060a66                 | 1     | 0.437535      | 0.437535      | 0.437535      | 0.437535      | 0      |
                                   |       | +1.37%        | +1.37%        | +1.37%        | +1.37%        | ---    | + is good
  pthread_mutex1_per_process_ops   |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 4.4554995e+07 | 4.4554995e+07 | 4.4554995e+07 | 4.4554995e+07 | 0      |
  (B) 84f99e060a66                 | 1     | 4.4471991e+07 | 4.4471991e+07 | 4.4471991e+07 | 4.4471991e+07 | 0      |
                                   |       | -0.19%        | -0.19%        | -0.19%        | -0.19%        | ---    | + is good
  pthread_mutex1_per_thread_ops    |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.0863168e+07 | 1.0863168e+07 | 1.0863168e+07 | 1.0863168e+07 | 0      |
  (B) 84f99e060a66                 | 1     | 1.0721714e+07 | 1.0721714e+07 | 1.0721714e+07 | 1.0721714e+07 | 0      |
                                   |       | -1.30%        | -1.30%        | -1.30%        | -1.30%        | ---    | + is good
  pthread_mutex1_scalability       |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.957534      | 0.957534      | 0.957534      | 0.957534      | 0      |
  (B) 84f99e060a66                 | 1     | 0.971139      | 0.971139      | 0.971139      | 0.971139      | 0      |
                                   |       | +1.42%        | +1.42%        | +1.42%        | +1.42%        | ---    | + is good
  pthread_mutex2_per_process_ops   |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 4.4283917e+07 | 4.4283917e+07 | 4.4283917e+07 | 4.4283917e+07 | 0      |
  (B) 84f99e060a66                 | 1     | 4.4068084e+07 | 4.4068084e+07 | 4.4068084e+07 | 4.4068084e+07 | 0      |
                                   |       | -0.49%        | -0.49%        | -0.49%        | -0.49%        | ---    | + is good
  pthread_mutex2_per_thread_ops    |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 4.5017533e+07 | 4.5017533e+07 | 4.5017533e+07 | 4.5017533e+07 | 0      |
  (B) 84f99e060a66                 | 1     | 4.4966205e+07 | 4.4966205e+07 | 4.4966205e+07 | 4.4966205e+07 | 0      |
                                   |       | -0.11%        | -0.11%        | -0.11%        | -0.11%        | ---    | + is good
  pthread_mutex2_scalability       |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.976762      | 0.976762      | 0.976762      | 0.976762      | 0      |
  (B) 84f99e060a66                 | 1     | 0.979919      | 0.979919      | 0.979919      | 0.979919      | 0      |
                                   |       | +0.32%        | +0.32%        | +0.32%        | +0.32%        | ---    | + is good
  pwrite1_per_process_ops          |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 842395        | 842395        | 842395        | 842395        | 0      |
  (B) 84f99e060a66                 | 1     | 858472        | 858472        | 858472        | 858472        | 0      |
                                   |       | +1.91%        | +1.91%        | +1.91%        | +1.91%        | ---    | + is good
  pwrite1_per_thread_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 793710        | 793710        | 793710        | 793710        | 0      |
  (B) 84f99e060a66                 | 1     | 803656        | 803656        | 803656        | 803656        | 0      |
                                   |       | +1.25%        | +1.25%        | +1.25%        | +1.25%        | ---    | + is good
  pwrite1_scalability              |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.52403       | 0.52403       | 0.52403       | 0.52403       | 0      |
  (B) 84f99e060a66                 | 1     | 0.548598      | 0.548598      | 0.548598      | 0.548598      | 0      |
                                   |       | +4.69%        | +4.69%        | +4.69%        | +4.69%        | ---    | + is good
  pwrite2_per_process_ops          |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 282885        | 282885        | 282885        | 282885        | 0      |
  (B) 84f99e060a66                 | 1     | 280819        | 280819        | 280819        | 280819        | 0      |
                                   |       | -0.73%        | -0.73%        | -0.73%        | -0.73%        | ---    | + is good
  pwrite2_per_thread_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 264000        | 264000        | 264000        | 264000        | 0      |
  (B) 84f99e060a66                 | 1     | 259404        | 259404        | 259404        | 259404        | 0      |
                                   |       | -1.74%        | -1.74%        | -1.74%        | -1.74%        | ---    | + is good
  pwrite2_scalability              |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.038412      | 0.038412      | 0.038412      | 0.038412      | 0      |
  (B) 84f99e060a66                 | 1     | 0.038888      | 0.038888      | 0.038888      | 0.038888      | 0      |
                                   |       | +1.24%        | +1.24%        | +1.24%        | +1.24%        | ---    | + is good
  pwrite3_per_process_ops          |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 323618        | 323618        | 323618        | 323618        | 0      |
  (B) 84f99e060a66                 | 1     | 330871        | 330871        | 330871        | 330871        | 0      |
                                   |       | +2.24%        | +2.24%        | +2.24%        | +2.24%        | ---    | + is good
  pwrite3_per_thread_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 300116        | 300116        | 300116        | 300116        | 0      |
  (B) 84f99e060a66                 | 1     | 305806        | 305806        | 305806        | 305806        | 0      |
                                   |       | +1.90%        | +1.90%        | +1.90%        | +1.90%        | ---    | + is good
  pwrite3_scalability              |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.062845      | 0.062845      | 0.062845      | 0.062845      | 0      |
  (B) 84f99e060a66                 | 1     | 0.068264      | 0.068264      | 0.068264      | 0.068264      | 0      |
                                   |       | +8.62%        | +8.62%        | +8.62%        | +8.62%        | ---    | + is good
  read1_per_process_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 849497        | 849497        | 849497        | 849497        | 0      |
  (B) 84f99e060a66                 | 1     | 851557        | 851557        | 851557        | 851557        | 0      |
                                   |       | +0.24%        | +0.24%        | +0.24%        | +0.24%        | ---    | + is good
  read1_per_thread_ops             |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 825148        | 825148        | 825148        | 825148        | 0      |
  (B) 84f99e060a66                 | 1     | 825696        | 825696        | 825696        | 825696        | 0      |
                                   |       | +0.07%        | +0.07%        | +0.07%        | +0.07%        | ---    | + is good
  read1_scalability                |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.492216      | 0.492216      | 0.492216      | 0.492216      | 0      |
  (B) 84f99e060a66                 | 1     | 0.504087      | 0.504087      | 0.504087      | 0.504087      | 0      |
                                   |       | +2.41%        | +2.41%        | +2.41%        | +2.41%        | ---    | + is good
  read2_per_process_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 804636        | 804636        | 804636        | 804636        | 0      |
  (B) 84f99e060a66                 | 1     | 835800        | 835800        | 835800        | 835800        | 0      |
                                   |       | +3.87%        | +3.87%        | +3.87%        | +3.87%        | ---    | + is good
  read2_per_thread_ops             |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 782155        | 782155        | 782155        | 782155        | 0      |
  (B) 84f99e060a66                 | 1     | 813638        | 813638        | 813638        | 813638        | 0      |
                                   |       | +4.03%        | +4.03%        | +4.03%        | +4.03%        | ---    | + is good
  read2_scalability                |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.474276      | 0.474276      | 0.474276      | 0.474276      | 0      |
  (B) 84f99e060a66                 | 1     | 0.509206      | 0.509206      | 0.509206      | 0.509206      | 0      |
                                   |       | +7.36%        | +7.36%        | +7.36%        | +7.36%        | ---    | + is good
  readseek1_per_process_ops        |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 700211        | 700211        | 700211        | 700211        | 0      |
  (B) 84f99e060a66                 | 1     | 702424        | 702424        | 702424        | 702424        | 0      |
                                   |       | +0.32%        | +0.32%        | +0.32%        | +0.32%        | ---    | + is good
  readseek1_per_thread_ops         |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 666974        | 666974        | 666974        | 666974        | 0      |
  (B) 84f99e060a66                 | 1     | 669790        | 669790        | 669790        | 669790        | 0      |
                                   |       | +0.42%        | +0.42%        | +0.42%        | +0.42%        | ---    | + is good
  readseek1_scalability            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.709055      | 0.709055      | 0.709055      | 0.709055      | 0      |
  (B) 84f99e060a66                 | 1     | 0.723066      | 0.723066      | 0.723066      | 0.723066      | 0      |
                                   |       | +1.98%        | +1.98%        | +1.98%        | +1.98%        | ---    | + is good
  readseek2_per_process_ops        |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 359075        | 359075        | 359075        | 359075        | 0      |
  (B) 84f99e060a66                 | 1     | 362790        | 362790        | 362790        | 362790        | 0      |
                                   |       | +1.03%        | +1.03%        | +1.03%        | +1.03%        | ---    | + is good
  readseek2_per_thread_ops         |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 353384        | 353384        | 353384        | 353384        | 0      |
  (B) 84f99e060a66                 | 1     | 358400        | 358400        | 358400        | 358400        | 0      |
                                   |       | +1.42%        | +1.42%        | +1.42%        | +1.42%        | ---    | + is good
  readseek2_scalability            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.121419      | 0.121419      | 0.121419      | 0.121419      | 0      |
  (B) 84f99e060a66                 | 1     | 0.124333      | 0.124333      | 0.124333      | 0.124333      | 0      |
                                   |       | +2.40%        | +2.40%        | +2.40%        | +2.40%        | ---    | + is good
  readseek3_per_process_ops        |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 421886        | 421886        | 421886        | 421886        | 0      |
  (B) 84f99e060a66                 | 1     | 414691        | 414691        | 414691        | 414691        | 0      |
                                   |       | -1.71%        | -1.71%        | -1.71%        | -1.71%        | ---    | + is good
  readseek3_per_thread_ops         |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 400726        | 400726        | 400726        | 400726        | 0      |
  (B) 84f99e060a66                 | 1     | 397327        | 397327        | 397327        | 397327        | 0      |
                                   |       | -0.85%        | -0.85%        | -0.85%        | -0.85%        | ---    | + is good
  readseek3_scalability            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.195407      | 0.195407      | 0.195407      | 0.195407      | 0      |
  (B) 84f99e060a66                 | 1     | 0.20046       | 0.20046       | 0.20046       | 0.20046       | 0      |
                                   |       | +2.59%        | +2.59%        | +2.59%        | +2.59%        | ---    | + is good
  sched_yield_per_process_ops      |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.426126e+06  | 1.426126e+06  | 1.426126e+06  | 1.426126e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 1.428767e+06  | 1.428767e+06  | 1.428767e+06  | 1.428767e+06  | 0      |
                                   |       | +0.19%        | +0.19%        | +0.19%        | +0.19%        | ---    | + is good
  sched_yield_per_thread_ops       |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.356631e+06  | 1.356631e+06  | 1.356631e+06  | 1.356631e+06  | 0      |
  (B) 84f99e060a66                 | 1     | 1.357941e+06  | 1.357941e+06  | 1.357941e+06  | 1.357941e+06  | 0      |
                                   |       | +0.10%        | +0.10%        | +0.10%        | +0.10%        | ---    | + is good
  sched_yield_scalability          |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 1.007993      | 1.007993      | 1.007993      | 1.007993      | 0      |
  (B) 84f99e060a66                 | 1     | 1.014633      | 1.014633      | 1.014633      | 1.014633      | 0      |
                                   |       | +0.66%        | +0.66%        | +0.66%        | +0.66%        | ---    | + is good
  signal1_per_process_ops          |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 226669        | 226669        | 226669        | 226669        | 0      |
  (B) 84f99e060a66                 | 1     | 227201        | 227201        | 227201        | 227201        | 0      |
                                   |       | +0.23%        | +0.23%        | +0.23%        | +0.23%        | ---    | + is good
  signal1_per_thread_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 93562         | 93562         | 93562         | 93562         | 0      |
  (B) 84f99e060a66                 | 1     | 95743         | 95743         | 95743         | 95743         | 0      |
                                   |       | +2.33%        | +2.33%        | +2.33%        | +2.33%        | ---    | + is good
  signal1_scalability              |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.957449      | 0.957449      | 0.957449      | 0.957449      | 0      |
  (B) 84f99e060a66                 | 1     | 0.963292      | 0.963292      | 0.963292      | 0.963292      | 0      |
                                   |       | +0.61%        | +0.61%        | +0.61%        | +0.61%        | ---    | + is good
  unix1_per_process_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 522468        | 522468        | 522468        | 522468        | 0      |
  (B) 84f99e060a66                 | 1     | 530562        | 530562        | 530562        | 530562        | 0      |
                                   |       | +1.55%        | +1.55%        | +1.55%        | +1.55%        | ---    | + is good
  unix1_per_thread_ops             |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 510443        | 510443        | 510443        | 510443        | 0      |
  (B) 84f99e060a66                 | 1     | 518060        | 518060        | 518060        | 518060        | 0      |
                                   |       | +1.49%        | +1.49%        | +1.49%        | +1.49%        | ---    | + is good
  unix1_scalability                |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.979953      | 0.979953      | 0.979953      | 0.979953      | 0      |
  (B) 84f99e060a66                 | 1     | 0.959629      | 0.959629      | 0.959629      | 0.959629      | 0      |
                                   |       | -2.07%        | -2.07%        | -2.07%        | -2.07%        | ---    | + is good
  unlink1_per_process_ops          |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 25502         | 25502         | 25502         | 25502         | 0      |
  (B) 84f99e060a66                 | 1     | 25657         | 25657         | 25657         | 25657         | 0      |
                                   |       | +0.61%        | +0.61%        | +0.61%        | +0.61%        | ---    | + is good
  unlink1_per_thread_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 25408         | 25408         | 25408         | 25408         | 0      |
  (B) 84f99e060a66                 | 1     | 25585         | 25585         | 25585         | 25585         | 0      |
                                   |       | +0.70%        | +0.70%        | +0.70%        | +0.70%        | ---    | + is good
  unlink1_scalability              |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.070072      | 0.070072      | 0.070072      | 0.070072      | 0      |
  (B) 84f99e060a66                 | 1     | 0.070865      | 0.070865      | 0.070865      | 0.070865      | 0      |
                                   |       | +1.13%        | +1.13%        | +1.13%        | +1.13%        | ---    | + is good
  unlink2_per_process_ops          |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 52050         | 52050         | 52050         | 52050         | 0      |
  (B) 84f99e060a66                 | 1     | 51677         | 51677         | 51677         | 51677         | 0      |
                                   |       | -0.72%        | -0.72%        | -0.72%        | -0.72%        | ---    | + is good
  unlink2_per_thread_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 51363         | 51363         | 51363         | 51363         | 0      |
  (B) 84f99e060a66                 | 1     | 51159         | 51159         | 51159         | 51159         | 0      |
                                   |       | -0.40%        | -0.40%        | -0.40%        | -0.40%        | ---    | + is good
  unlink2_scalability              |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.302468      | 0.302468      | 0.302468      | 0.302468      | 0      |
  (B) 84f99e060a66                 | 1     | 0.29703       | 0.29703       | 0.29703       | 0.29703       | 0      |
                                   |       | -1.80%        | -1.80%        | -1.80%        | -1.80%        | ---    | + is good
  write1_per_process_ops           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 770496        | 770496        | 770496        | 770496        | 0      |
  (B) 84f99e060a66                 | 1     | 762628        | 762628        | 762628        | 762628        | 0      |
                                   |       | -1.02%        | -1.02%        | -1.02%        | -1.02%        | ---    | + is good
  write1_per_thread_ops            |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 757006        | 757006        | 757006        | 757006        | 0      |
  (B) 84f99e060a66                 | 1     | 746414        | 746414        | 746414        | 746414        | 0      |
                                   |       | -1.40%        | -1.40%        | -1.40%        | -1.40%        | ---    | + is good
  write1_scalability               |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.602205      | 0.602205      | 0.602205      | 0.602205      | 0      |
  (B) 84f99e060a66                 | 1     | 0.596005      | 0.596005      | 0.596005      | 0.596005      | 0      |
                                   |       | -1.03%        | -1.03%        | -1.03%        | -1.03%        | ---    | + is good
  writeseek1_per_process_ops       |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 629967        | 629967        | 629967        | 629967        | 0      |
  (B) 84f99e060a66                 | 1     | 626663        | 626663        | 626663        | 626663        | 0      |
                                   |       | -0.52%        | -0.52%        | -0.52%        | -0.52%        | ---    | + is good
  writeseek1_per_thread_ops        |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 582490        | 582490        | 582490        | 582490        | 0      |
  (B) 84f99e060a66                 | 1     | 579400        | 579400        | 579400        | 579400        | 0      |
                                   |       | -0.53%        | -0.53%        | -0.53%        | -0.53%        | ---    | + is good
  writeseek1_scalability           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.785506      | 0.785506      | 0.785506      | 0.785506      | 0      |
  (B) 84f99e060a66                 | 1     | 0.800547      | 0.800547      | 0.800547      | 0.800547      | 0      |
                                   |       | +1.91%        | +1.91%        | +1.91%        | +1.91%        | ---    | + is good
  writeseek2_per_process_ops       |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 206887        | 206887        | 206887        | 206887        | 0      |
  (B) 84f99e060a66                 | 1     | 207179        | 207179        | 207179        | 207179        | 0      |
                                   |       | +0.14%        | +0.14%        | +0.14%        | +0.14%        | ---    | + is good
  writeseek2_per_thread_ops        |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 195575        | 195575        | 195575        | 195575        | 0      |
  (B) 84f99e060a66                 | 1     | 193851        | 193851        | 193851        | 193851        | 0      |
                                   |       | -0.88%        | -0.88%        | -0.88%        | -0.88%        | ---    | + is good
  writeseek2_scalability           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.057881      | 0.057881      | 0.057881      | 0.057881      | 0      |
  (B) 84f99e060a66                 | 1     | 0.060053      | 0.060053      | 0.060053      | 0.060053      | 0      |
                                   |       | +3.75%        | +3.75%        | +3.75%        | +3.75%        | ---    | + is good
  writeseek3_per_process_ops       |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 252163        | 252163        | 252163        | 252163        | 0      |
  (B) 84f99e060a66                 | 1     | 251815        | 251815        | 251815        | 251815        | 0      |
                                   |       | -0.14%        | -0.14%        | -0.14%        | -0.14%        | ---    | + is good
  writeseek3_per_thread_ops        |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 236912        | 236912        | 236912        | 236912        | 0      |
  (B) 84f99e060a66                 | 1     | 236544        | 236544        | 236544        | 236544        | 0      |
                                   |       | -0.16%        | -0.16%        | -0.16%        | -0.16%        | ---    | + is good
  writeseek3_scalability           |       |               |               |               |               |        |
  (A) 4db0b9c9d0f3                 | 1     | 0.098322      | 0.098322      | 0.098322      | 0.098322      | 0      |
  (B) 84f99e060a66                 | 1     | 0.090771      | 0.090771      | 0.090771      | 0.090771      | 0      |
                                   |       | -7.68%        | -7.68%        | -7.68%        | -7.68%        | ---    | + is good


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.7 required=3.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,FREEMAIL_FORGED_FROMDOMAIN,FREEMAIL_FROM,
	HEADER_FROM_DIFFERENT_DOMAINS,INCLUDES_CR_TRAILER,MAILING_LIST_MULTI,
	SPF_HELO_NONE,SPF_PASS,URIBL_BLOCKED autolearn=no autolearn_force=no
	version=3.4.0
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 1016DC433B4
	for <linux-kernel@archiver.kernel.org>; Thu, 29 Apr 2021 23:56:45 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by mail.kernel.org (Postfix) with ESMTP id DC1C361006
	for <linux-kernel@archiver.kernel.org>; Thu, 29 Apr 2021 23:56:44 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S229711AbhD2Xyp (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Thu, 29 Apr 2021 19:54:45 -0400
Received: from forward102o.mail.yandex.net ([37.140.190.182]:33871 "EHLO
        forward102o.mail.yandex.net" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S229557AbhD2Xyo (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 29 Apr 2021 19:54:44 -0400
X-Greylist: delayed 415 seconds by postgrey-1.27 at vger.kernel.org; Thu, 29 Apr 2021 19:54:44 EDT
Received: from sas2-a12590589674.qloud-c.yandex.net (sas2-a12590589674.qloud-c.yandex.net [IPv6:2a02:6b8:c08:b7a3:0:640:a125:9058])
        by forward102o.mail.yandex.net (Yandex) with ESMTP id ABC226680F48;
        Fri, 30 Apr 2021 02:46:58 +0300 (MSK)
Received: from sas8-b61c542d7279.qloud-c.yandex.net (sas8-b61c542d7279.qloud-c.yandex.net [2a02:6b8:c1b:2912:0:640:b61c:542d])
        by sas2-a12590589674.qloud-c.yandex.net (mxback/Yandex) with ESMTP id 3mpKp0g9Fg-kuIK1IWX;
        Fri, 30 Apr 2021 02:46:58 +0300
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yandex.ru; s=mail; t=1619740018;
        bh=XnBunxh2JzsdX0ViieYhmfFAtho+UllNfVKoQZrf04M=;
        h=In-Reply-To:Cc:To:From:Subject:Message-ID:References:Date;
        b=l8jw9ranooGaIXEbltI5acYo0amJT+QZokdP/gGaO0vIJ+v8jUj1ciY4YKOcf0bzY
         kx408rRb1Rwgs9RZapBOC/RY86QiAzsXdhYWKNjuX292iabUwEerD01w8sQkvknk2l
         FVJEYX6xntO+8IUL1imV2R2BwH1/Kth/ohJUpaTw=
Authentication-Results: sas2-a12590589674.qloud-c.yandex.net; dkim=pass header.i=@yandex.ru
Received: by sas8-b61c542d7279.qloud-c.yandex.net (smtp/Yandex) with ESMTPSA id fgDCQ3HtFG-krMCAUJY;
        Fri, 30 Apr 2021 02:46:54 +0300
        (using TLSv1.3 with cipher TLS_AES_256_GCM_SHA384 (256/256 bits))
        (Client certificate not present)
Message-ID: <140226722f2032c86301fbd326d91baefe3d7d23.camel@yandex.ru>
Subject: Re: [PATCH v2 00/16] Multigenerational LRU Framework
From:   Konstantin Kharlamov <hi-angel@yandex.ru>
To:     Yu Zhao <yuzhao@google.com>, linux-mm@kvack.org
Cc:     Alex Shi <alexs@kernel.org>, Andi Kleen <ak@linux.intel.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Benjamin Manes <ben.manes@gmail.com>,
        Dave Chinner <david@fromorbit.com>,
        Dave Hansen <dave.hansen@linux.intel.com>,
        Hillf Danton <hdanton@sina.com>, Jens Axboe <axboe@kernel.dk>,
        Johannes Weiner <hannes@cmpxchg.org>,
        Jonathan Corbet <corbet@lwn.net>,
        Joonsoo Kim <iamjoonsoo.kim@lge.com>,
        Matthew Wilcox <willy@infradead.org>,
        Mel Gorman <mgorman@suse.de>,
        Miaohe Lin <linmiaohe@huawei.com>,
        Michael Larabel <michael@michaellarabel.com>,
        Michal Hocko <mhocko@suse.com>,
        Michel Lespinasse <michel@lespinasse.org>,
        Rik van Riel <riel@surriel.com>,
        Roman Gushchin <guro@fb.com>,
        Rong Chen <rong.a.chen@intel.com>,
        SeongJae Park <sjpark@amazon.de>,
        Tim Chen <tim.c.chen@linux.intel.com>,
        Vlastimil Babka <vbabka@suse.cz>,
        Yang Shi <shy828301@gmail.com>,
        Ying Huang <ying.huang@intel.com>, Zi Yan <ziy@nvidia.com>,
        linux-kernel@vger.kernel.org, lkp@lists.01.org,
        page-reclaim@google.com
Date:   Fri, 30 Apr 2021 02:46:53 +0300
In-Reply-To: <20210413065633.2782273-1-yuzhao@google.com>
References: <20210413065633.2782273-1-yuzhao@google.com>
Content-Type: text/plain; charset="UTF-8"
User-Agent: Evolution 3.40.0 
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

In case you need it yet, this series is:

Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>

My success story: I have Archlinux with 8G RAM + zswap + swap. While developing,
I have lots of apps opened such as multiple LSP-servers for different langs,
chats, two browsers, etc… Usually, my system gets quickly to a point of SWAP-
storms, where I have to kill LSP-servers, restart browsers to free memory, etc,
otherwise the system lags heavily and is barely usable.

1.5 day ago I migrated from 5.11.15 kernel to 5.12 + the LRU patchset, and I
started up by opening lots of apps to create memory pressure, and worked for a
day like this. Till now I had *not a single SWAP-storm*, and mind you I got 3.4G
in SWAP. I was never getting to the point of 3G in SWAP before without a single
SWAP-storm.

Right now my gf on Fedora 33 also suffers from SWAP-storms on her old Macbook
2013 with 4G RAM + zswap + swap, I think the next week I'll build for her 5.12 +
LRU patchset as well. Will see how it goes, I expect it will improve her
experience by a lot too.

P.S.: upon replying please keep me CCed, I'm not subscribed to the list

On Tue, 2021-04-13 at 00:56 -0600, Yu Zhao wrote:
> What's new in v2
> ================
> Special thanks to Jens Axboe for reporting a regression in buffered
> I/O and helping test the fix.
> 
> This version includes the support of tiers, which represent levels of
> usage from file descriptors only. Pages accessed N times via file
> descriptors belong to tier order_base_2(N). Each generation contains
> at most MAX_NR_TIERS tiers, and they require additional MAX_NR_TIERS-2
> bits in page->flags. In contrast to moving across generations which
> requires the lru lock, moving across tiers only involves an atomic
> operation on page->flags and therefore has a negligible cost. A
> feedback loop modeled after the well-known PID controller monitors the
> refault rates across all tiers and decides when to activate pages from
> which tiers, on the reclaim path.
> 
> This feedback model has a few advantages over the current feedforward
> model:
> 1) It has a negligible overhead in the buffered I/O access path
>    because activations are done in the reclaim path.
> 2) It takes mapped pages into account and avoids overprotecting pages
>    accessed multiple times via file descriptors.
> 3) More tiers offer better protection to pages accessed more than
>    twice when buffered-I/O-intensive workloads are under memory
>    pressure.
> 
> The fio/io_uring benchmark shows 14% improvement in IOPS when randomly
> accessing Samsung PM981a in the buffered I/O mode.
> 
> Highlights from the discussions on v1
> =====================================
> Thanks to Ying Huang and Dave Hansen for the comments and suggestions
> on page table scanning.
> 
> A simple worst-case scenario test did not find page table scanning
> underperforms the rmap because of the following optimizations:
> 1) It will not scan page tables from processes that have been sleeping
>    since the last scan.
> 2) It will not scan PTE tables under non-leaf PMD entries that do not
>    have the accessed bit set, when
>    CONFIG_HAVE_ARCH_PARENT_PMD_YOUNG=y.
> 3) It will not zigzag between the PGD table and the same PMD or PTE
>    table spanning multiple VMAs. In other words, it finishes all the
>    VMAs with the range of the same PMD or PTE table before it returns
>    to the PGD table. This optimizes workloads that have large numbers
>    of tiny VMAs, especially when CONFIG_PGTABLE_LEVELS=5.
> 
> TLDR
> ====
> The current page reclaim is too expensive in terms of CPU usage and
> often making poor choices about what to evict. We would like to offer
> an alternative framework that is performant, versatile and
> straightforward.
> 
> Repo
> ====
> git fetch https://linux-mm.googlesource.com/page-reclaim refs/changes/73/1173/1
> 
> Gerrit https://linux-mm-review.googlesource.com/c/page-reclaim/+/1173
> 
> Background
> ==========
> DRAM is a major factor in total cost of ownership, and improving
> memory overcommit brings a high return on investment. Over the past
> decade of research and experimentation in memory overcommit, we
> observed a distinct trend across millions of servers and clients: the
> size of page cache has been decreasing because of the growing
> popularity of cloud storage. Nowadays anon pages account for more than
> 90% of our memory consumption and page cache contains mostly
> executable pages.
> 
> Problems
> ========
> Notion of active/inactive
> -------------------------
> For servers equipped with hundreds of gigabytes of memory, the
> granularity of the active/inactive is too coarse to be useful for job
> scheduling. False active/inactive rates are relatively high, and thus
> the assumed savings may not materialize.
> 
> For phones and laptops, executable pages are frequently evicted
> despite the fact that there are many less recently used anon pages.
> Major faults on executable pages cause "janks" (slow UI renderings)
> and negatively impact user experience.
> 
> For lruvecs from different memcgs or nodes, comparisons are impossible
> due to the lack of a common frame of reference.
> 
> Incremental scans via rmap
> --------------------------
> Each incremental scan picks up at where the last scan left off and
> stops after it has found a handful of unreferenced pages. For
> workloads using a large amount of anon memory, incremental scans lose
> the advantage under sustained memory pressure due to high ratios of
> the number of scanned pages to the number of reclaimed pages. In our
> case, the average ratio of pgscan to pgsteal is above 7.
> 
> On top of that, the rmap has poor memory locality due to its complex
> data structures. The combined effects typically result in a high
> amount of CPU usage in the reclaim path. For example, with zram, a
> typical kswapd profile on v5.11 looks like:
>   31.03%  page_vma_mapped_walk
>   25.59%  lzo1x_1_do_compress
>    4.63%  do_raw_spin_lock
>    3.89%  vma_interval_tree_iter_next
>    3.33%  vma_interval_tree_subtree_search
> 
> And with real swap, it looks like:
>   45.16%  page_vma_mapped_walk
>    7.61%  do_raw_spin_lock
>    5.69%  vma_interval_tree_iter_next
>    4.91%  vma_interval_tree_subtree_search
>    3.71%  page_referenced_one
> 
> Solutions
> =========
> Notion of generation numbers
> ----------------------------
> The notion of generation numbers introduces a quantitative approach to
> memory overcommit. A larger number of pages can be spread out across
> a configurable number of generations, and each generation includes all
> pages that have been referenced since the last generation. This
> improved granularity yields relatively low false active/inactive
> rates.
> 
> Given an lruvec, scans of anon and file types and selections between
> them are all based on direct comparisons of generation numbers, which
> are simple and yet effective. For different lruvecs, comparisons are
> still possible based on birth times of generations.
> 
> Differential scans via page tables
> ----------------------------------
> Each differential scan discovers all pages that have been referenced
> since the last scan. Specifically, it walks the mm_struct list
> associated with an lruvec to scan page tables of processes that have
> been scheduled since the last scan. The cost of each differential scan
> is roughly proportional to the number of referenced pages it
> discovers. Unless address spaces are extremely sparse, page tables
> usually have better memory locality than the rmap. The end result is
> generally a significant reduction in CPU usage, for workloads using a
> large amount of anon memory.
> 
> Our real-world benchmark that browses popular websites in multiple
> Chrome tabs demonstrates 51% less CPU usage from kswapd and 52% (full)
> less PSI on v5.11. With this patchset, kswapd profile looks like:
>   49.36%  lzo1x_1_do_compress
>    4.54%  page_vma_mapped_walk
>    4.45%  memset_erms
>    3.47%  walk_pte_range
>    2.88%  zram_bvec_rw
> 
> In addition, direct reclaim latency is reduced by 22% at 99th
> percentile and the number of refaults is reduced by 7%. Both metrics
> are important to phones and laptops as they are correlated to user
> experience.
> 
> Framework
> =========
> For each lruvec, evictable pages are divided into multiple
> generations. The youngest generation number is stored in
> lruvec->evictable.max_seq for both anon and file types as they are
> aged on an equal footing. The oldest generation numbers are stored in
> lruvec->evictable.min_seq[2] separately for anon and file types as
> clean file pages can be evicted regardless of may_swap or
> may_writepage. Generation numbers are truncated into
> order_base_2(MAX_NR_GENS+1) bits in order to fit into page->flags. The
> sliding window technique is used to prevent truncated generation
> numbers from overlapping. Each truncated generation number is an inde
> to lruvec->evictable.lists[MAX_NR_GENS][ANON_AND_FILE][MAX_NR_ZONES].
> Evictable pages are added to the per-zone lists indexed by max_seq or
> min_seq[2] (modulo MAX_NR_GENS), depending on whether they are being
> faulted in.
> 
> Each generation is then divided into multiple tiers. Tiers represent
> levels of usage from file descriptors only. Pages accessed N times via
> file descriptors belong to tier order_base_2(N). In contrast to moving
> across generations which requires the lru lock, moving across tiers
> only involves an atomic operation on page->flags and therefore has a
> lower cost. A feedback loop modeled after the well-known PID
> controller monitors the refault rates across all tiers and decides
> when to activate pages from which tiers on the reclaim path.
> 
> The framework comprises two conceptually independent components: the
> aging and the eviction, which can be invoked separately from user
> space.
> 
> Aging
> -----
> The aging produces young generations. Given an lruvec, the aging scans
> page tables for referenced pages of this lruvec. Upon finding one, the
> aging updates its generation number to max_seq. After each round of
> scan, the aging increments max_seq.
> 
> The aging maintains either a system-wide mm_struct list or per-memcg
> mm_struct lists and tracks whether an mm_struct is being used or has
> been used since the last scan. Multiple threads can concurrently work
> on the same mm_struct list, and each of them will be given a different
> mm_struct belonging to a process that has been scheduled since the
> last scan.
> 
> The aging is due when both of min_seq[2] reaches max_seq-1, assuming
> both anon and file types are reclaimable.
> 
> Eviction
> --------
> The eviction consumes old generations. Given an lruvec, the eviction
> scans the pages on the per-zone lists indexed by either of min_seq[2].
> It first tries to select a type based on the values of min_seq[2].
> When anon and file types are both available from the same generation,
> it selects the one that has a lower refault rate.
> 
> During a scan, the eviction sorts pages according to their generation
> numbers, if the aging has found them referenced. It also moves pages
> from the tiers that have higher refault rates than tier 0 to the next
> generation.
> 
> When it finds all the per-zone lists of a selected type are empty, the
> eviction increments min_seq[2] indexed by this selected type.
> 
> Use cases
> =========
> On Android, our most advanced simulation that generates memory
> pressure from realistic user behavior shows 18% fewer low-memory
> kills, which in turn reduces cold starts by 16%.
> 
> On Borg, a similar approach enables us to identify jobs that
> underutilize their memory and downsize them considerably without
> compromising any of our service level indicators.
> 
> On Chrome OS, our field telemetry reports 96% fewer low-memory tab
> discards and 59% fewer OOM kills from fully-utilized devices and no
> regressions in monitored user experience from underutilized devices.
> 
> Working set estimation
> ----------------------
> User space can invoke the aging by writing "+ memcg_id node_id gen
> [swappiness]" to /sys/kernel/debug/lru_gen. This debugfs interface
> also provides the birth time and the size of each generation.
> 
> Proactive reclaim
> -----------------
> User space can invoke the eviction by writing "- memcg_id node_id gen
> [swappiness] [nr_to_reclaim]" to /sys/kernel/debug/lru_gen. Multiple
> command lines are supported, so does concatenation with delimiters.
> 
> Intensive buffered I/O
> ----------------------
> Tiers are specifically designed to improve the performance of
> intensive buffered I/O under memory pressure. The fio/io_uring
> benchmark shows 14% improvement in IOPS when randomly accessing
> Samsung PM981a in buffered I/O mode.
> 
> For far memory tiering and NUMA-aware job scheduling, please refer to
> the reference section.
> 
> FAQ
> ===
> Why not try to improve the existing code?
> -----------------------------------------
> We have tried but concluded the aforementioned problems are
> fundamental, and therefore changes made on top of them will not result
> in substantial gains.
> 
> What particular workloads does it help?
> ---------------------------------------
> This framework is designed to improve the performance of the page
> reclaim under any types of workloads.
> 
> How would it benefit the community?
> -----------------------------------
> Google is committed to promoting sustainable development of the
> community. We hope successful adoptions of this framework will
> steadily climb over time. To that end, we would be happy to learn your
> workloads and work with you case by case, and we will do our best to
> keep the repo fully maintained. For those whose workloads rely on the
> existing code, we will make sure you will not be affected in any way.
> 
> References
> ==========
> 1. Long-term SLOs for reclaimed cloud computing resources
>    https://research.google/pubs/pub43017/
> 2. Profiling a warehouse-scale computer
>    https://research.google/pubs/pub44271/
> 3. Evaluation of NUMA-Aware Scheduling in Warehouse-Scale Clusters
>    https://research.google/pubs/pub48329/
> 4. Software-defined far memory in warehouse-scale computers
>    https://research.google/pubs/pub48551/
> 5. Borg: the Next Generation
>    https://research.google/pubs/pub49065/
> 
> Yu Zhao (16):
>   include/linux/memcontrol.h: do not warn in page_memcg_rcu() if
>     !CONFIG_MEMCG
>   include/linux/nodemask.h: define next_memory_node() if !CONFIG_NUMA
>   include/linux/huge_mm.h: define is_huge_zero_pmd() if
>     !CONFIG_TRANSPARENT_HUGEPAGE
>   include/linux/cgroup.h: export cgroup_mutex
>   mm/swap.c: export activate_page()
>   mm, x86: support the access bit on non-leaf PMD entries
>   mm/vmscan.c: refactor shrink_node()
>   mm: multigenerational lru: groundwork
>   mm: multigenerational lru: activation
>   mm: multigenerational lru: mm_struct list
>   mm: multigenerational lru: aging
>   mm: multigenerational lru: eviction
>   mm: multigenerational lru: page reclaim
>   mm: multigenerational lru: user interface
>   mm: multigenerational lru: Kconfig
>   mm: multigenerational lru: documentation
> 
>  Documentation/vm/index.rst        |    1 +
>  Documentation/vm/multigen_lru.rst |  192 +++
>  arch/Kconfig                      |    9 +
>  arch/x86/Kconfig                  |    1 +
>  arch/x86/include/asm/pgtable.h    |    2 +-
>  arch/x86/mm/pgtable.c             |    5 +-
>  fs/exec.c                         |    2 +
>  fs/fuse/dev.c                     |    3 +-
>  fs/proc/task_mmu.c                |    3 +-
>  include/linux/cgroup.h            |   15 +-
>  include/linux/huge_mm.h           |    5 +
>  include/linux/memcontrol.h        |    7 +-
>  include/linux/mm.h                |    2 +
>  include/linux/mm_inline.h         |  294 ++++
>  include/linux/mm_types.h          |  117 ++
>  include/linux/mmzone.h            |  118 +-
>  include/linux/nodemask.h          |    1 +
>  include/linux/page-flags-layout.h |   20 +-
>  include/linux/page-flags.h        |    4 +-
>  include/linux/pgtable.h           |    4 +-
>  include/linux/swap.h              |    5 +-
>  kernel/bounds.c                   |    6 +
>  kernel/events/uprobes.c           |    2 +-
>  kernel/exit.c                     |    1 +
>  kernel/fork.c                     |   10 +
>  kernel/kthread.c                  |    1 +
>  kernel/sched/core.c               |    2 +
>  mm/Kconfig                        |   55 +
>  mm/huge_memory.c                  |    5 +-
>  mm/khugepaged.c                   |    2 +-
>  mm/memcontrol.c                   |   28 +
>  mm/memory.c                       |   14 +-
>  mm/migrate.c                      |    2 +-
>  mm/mm_init.c                      |   16 +-
>  mm/mmzone.c                       |    2 +
>  mm/rmap.c                         |    6 +
>  mm/swap.c                         |   54 +-
>  mm/swapfile.c                     |    6 +-
>  mm/userfaultfd.c                  |    2 +-
>  mm/vmscan.c                       | 2580 ++++++++++++++++++++++++++++-
>  mm/workingset.c                   |  179 +-
>  41 files changed, 3603 insertions(+), 180 deletions(-)
>  create mode 100644 Documentation/vm/multigen_lru.rst
> 



From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <SRS0=wbcb=J2=kvack.org=owner-linux-mm@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-5.5 required=3.0 tests=BAYES_00,DKIM_INVALID,
	DKIM_SIGNED,FREEMAIL_FORGED_FROMDOMAIN,FREEMAIL_FROM,
	HEADER_FROM_DIFFERENT_DOMAINS,INCLUDES_CR_TRAILER,MAILING_LIST_MULTI,
	SPF_HELO_NONE,SPF_PASS,URIBL_BLOCKED autolearn=no autolearn_force=no
	version=3.4.0
Received: from mail.kernel.org (mail.kernel.org [198.145.29.99])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 887EBC433ED
	for <linux-mm@archiver.kernel.org>; Thu, 29 Apr 2021 23:47:03 +0000 (UTC)
Received: from kanga.kvack.org (kanga.kvack.org [205.233.56.17])
	by mail.kernel.org (Postfix) with ESMTP id 01B3961289
	for <linux-mm@archiver.kernel.org>; Thu, 29 Apr 2021 23:47:02 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mail.kernel.org 01B3961289
Authentication-Results: mail.kernel.org; dmarc=fail (p=none dis=none) header.from=yandex.ru
Authentication-Results: mail.kernel.org; spf=pass smtp.mailfrom=owner-linux-mm@kvack.org
Received: by kanga.kvack.org (Postfix)
	id 8D40A6B006C; Thu, 29 Apr 2021 19:47:02 -0400 (EDT)
Received: by kanga.kvack.org (Postfix, from userid 40)
	id 884CB6B006E; Thu, 29 Apr 2021 19:47:02 -0400 (EDT)
X-Delivered-To: int-list-linux-mm@kvack.org
Received: by kanga.kvack.org (Postfix, from userid 63042)
	id 6D5DD6B0070; Thu, 29 Apr 2021 19:47:02 -0400 (EDT)
X-Delivered-To: linux-mm@kvack.org
Received: from forelay.hostedemail.com (smtprelay0204.hostedemail.com [216.40.44.204])
	by kanga.kvack.org (Postfix) with ESMTP id 45BDE6B006C
	for <linux-mm@kvack.org>; Thu, 29 Apr 2021 19:47:02 -0400 (EDT)
Received: from smtpin04.hostedemail.com (10.5.19.251.rfc1918.com [10.5.19.251])
	by forelay02.hostedemail.com (Postfix) with ESMTP id EEA0A3644
	for <linux-mm@kvack.org>; Thu, 29 Apr 2021 23:47:01 +0000 (UTC)
X-FDA: 78087042642.04.7AE5977
Received: from forward102o.mail.yandex.net (forward102o.mail.yandex.net [37.140.190.182])
	by imf14.hostedemail.com (Postfix) with ESMTP id 6D309C0007CE
	for <linux-mm@kvack.org>; Thu, 29 Apr 2021 23:46:44 +0000 (UTC)
Received: from sas2-a12590589674.qloud-c.yandex.net (sas2-a12590589674.qloud-c.yandex.net [IPv6:2a02:6b8:c08:b7a3:0:640:a125:9058])
	by forward102o.mail.yandex.net (Yandex) with ESMTP id ABC226680F48;
	Fri, 30 Apr 2021 02:46:58 +0300 (MSK)
Received: from sas8-b61c542d7279.qloud-c.yandex.net (sas8-b61c542d7279.qloud-c.yandex.net [2a02:6b8:c1b:2912:0:640:b61c:542d])
	by sas2-a12590589674.qloud-c.yandex.net (mxback/Yandex) with ESMTP id 3mpKp0g9Fg-kuIK1IWX;
	Fri, 30 Apr 2021 02:46:58 +0300
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yandex.ru; s=mail; t=1619740018;
	bh=XnBunxh2JzsdX0ViieYhmfFAtho+UllNfVKoQZrf04M=;
	h=In-Reply-To:Cc:To:From:Subject:Message-ID:References:Date;
	b=l8jw9ranooGaIXEbltI5acYo0amJT+QZokdP/gGaO0vIJ+v8jUj1ciY4YKOcf0bzY
	 kx408rRb1Rwgs9RZapBOC/RY86QiAzsXdhYWKNjuX292iabUwEerD01w8sQkvknk2l
	 FVJEYX6xntO+8IUL1imV2R2BwH1/Kth/ohJUpaTw=
Received: by sas8-b61c542d7279.qloud-c.yandex.net (smtp/Yandex) with ESMTPSA id fgDCQ3HtFG-krMCAUJY;
	Fri, 30 Apr 2021 02:46:54 +0300
	(using TLSv1.3 with cipher TLS_AES_256_GCM_SHA384 (256/256 bits))
	(Client certificate not present)
Message-ID: <140226722f2032c86301fbd326d91baefe3d7d23.camel@yandex.ru>
Subject: Re: [PATCH v2 00/16] Multigenerational LRU Framework
From: Konstantin Kharlamov <hi-angel@yandex.ru>
To: Yu Zhao <yuzhao@google.com>, linux-mm@kvack.org
Cc: Alex Shi <alexs@kernel.org>, Andi Kleen <ak@linux.intel.com>, Andrew
 Morton <akpm@linux-foundation.org>, Benjamin Manes <ben.manes@gmail.com>,
 Dave Chinner <david@fromorbit.com>, Dave Hansen
 <dave.hansen@linux.intel.com>, Hillf Danton <hdanton@sina.com>, Jens Axboe
 <axboe@kernel.dk>, Johannes Weiner <hannes@cmpxchg.org>, Jonathan Corbet
 <corbet@lwn.net>, Joonsoo Kim <iamjoonsoo.kim@lge.com>, Matthew Wilcox
 <willy@infradead.org>, Mel Gorman <mgorman@suse.de>, Miaohe Lin
 <linmiaohe@huawei.com>, Michael Larabel <michael@michaellarabel.com>,
 Michal Hocko <mhocko@suse.com>, Michel Lespinasse <michel@lespinasse.org>,
 Rik van Riel <riel@surriel.com>, Roman Gushchin <guro@fb.com>, Rong Chen
 <rong.a.chen@intel.com>, SeongJae Park <sjpark@amazon.de>,  Tim Chen
 <tim.c.chen@linux.intel.com>, Vlastimil Babka <vbabka@suse.cz>, Yang Shi
 <shy828301@gmail.com>, Ying Huang <ying.huang@intel.com>, Zi Yan
 <ziy@nvidia.com>,  linux-kernel@vger.kernel.org, lkp@lists.01.org,
 page-reclaim@google.com
Date: Fri, 30 Apr 2021 02:46:53 +0300
In-Reply-To: <20210413065633.2782273-1-yuzhao@google.com>
References: <20210413065633.2782273-1-yuzhao@google.com>
Content-Type: text/plain; charset="UTF-8"
User-Agent: Evolution 3.40.0 
MIME-Version: 1.0
Authentication-Results: imf14.hostedemail.com;
	dkim=pass header.d=yandex.ru header.s=mail header.b=l8jw9ran;
	dmarc=pass (policy=none) header.from=yandex.ru;
	spf=pass (imf14.hostedemail.com: domain of hi-angel@yandex.ru designates 37.140.190.182 as permitted sender) smtp.mailfrom=hi-angel@yandex.ru
X-Stat-Signature: wior3i93io9frpdm7fwnp4i5pqo1coqc
X-Rspamd-Queue-Id: 6D309C0007CE
X-Rspamd-Server: rspam01
Received-SPF: none (yandex.ru>: No applicable sender policy available) receiver=imf14; identity=mailfrom; envelope-from="<hi-angel@yandex.ru>"; helo=forward102o.mail.yandex.net; client-ip=37.140.190.182
X-HE-DKIM-Result: pass/pass
X-HE-Tag: 1619740004-117525
Content-Transfer-Encoding: quoted-printable
X-Bogosity: Ham, tests=bogofilter, spamicity=0.000000, version=1.2.4
Sender: owner-linux-mm@kvack.org
Precedence: bulk
X-Loop: owner-majordomo@kvack.org
List-ID: <linux-mm.kvack.org>

In case you need it yet, this series is:

Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>

My success story: I have Archlinux with 8G RAM + zswap + swap. While deve=
loping,
I have lots of apps opened such as multiple LSP-servers for different lan=
gs,
chats, two browsers, etc=E2=80=A6 Usually, my system gets quickly to a po=
int of SWAP-
storms, where I have to kill LSP-servers, restart browsers to free memory=
, etc,
otherwise the system lags heavily and is barely usable.

1.5 day ago I migrated from 5.11.15 kernel to 5.12 + the LRU patchset, an=
d I
started up by opening lots of apps to create memory pressure, and worked =
for a
day like this. Till now I had *not a single SWAP-storm*, and mind you I g=
ot 3.4G
in SWAP. I was never getting to the point of 3G in SWAP before without a =
single
SWAP-storm.

Right now my gf on Fedora 33 also suffers from SWAP-storms on her old Mac=
book
2013 with 4G RAM + zswap + swap, I think the next week I'll build for her=
 5.12 +
LRU patchset as well. Will see how it goes, I expect it will improve her
experience by a lot too.

P.S.: upon replying please keep me CCed, I'm not subscribed to the list

On Tue, 2021-04-13 at 00:56 -0600, Yu Zhao wrote:
> What's new in v2
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
> Special thanks to Jens Axboe for reporting a regression in buffered
> I/O and helping test the fix.
>=20
> This version includes the support of tiers, which represent levels of
> usage from file descriptors only. Pages accessed N times via file
> descriptors belong to tier order_base_2(N). Each generation contains
> at most MAX_NR_TIERS tiers, and they require additional MAX_NR_TIERS-2
> bits in page->flags. In contrast to moving across generations which
> requires the lru lock, moving across tiers only involves an atomic
> operation on page->flags and therefore has a negligible cost. A
> feedback loop modeled after the well-known PID controller monitors the
> refault rates across all tiers and decides when to activate pages from
> which tiers, on the reclaim path.
>=20
> This feedback model has a few advantages over the current feedforward
> model:
> 1) It has a negligible overhead in the buffered I/O access path
> =C2=A0=C2=A0 because activations are done in the reclaim path.
> 2) It takes mapped pages into account and avoids overprotecting pages
> =C2=A0=C2=A0 accessed multiple times via file descriptors.
> 3) More tiers offer better protection to pages accessed more than
> =C2=A0=C2=A0 twice when buffered-I/O-intensive workloads are under memo=
ry
> =C2=A0=C2=A0 pressure.
>=20
> The fio/io_uring benchmark shows 14% improvement in IOPS when randomly
> accessing Samsung PM981a in the buffered I/O mode.
>=20
> Highlights from the discussions on v1
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
> Thanks to Ying Huang and Dave Hansen for the comments and suggestions
> on page table scanning.
>=20
> A simple worst-case scenario test did not find page table scanning
> underperforms the rmap because of the following optimizations:
> 1) It will not scan page tables from processes that have been sleeping
> =C2=A0=C2=A0 since the last scan.
> 2) It will not scan PTE tables under non-leaf PMD entries that do not
> =C2=A0=C2=A0 have the accessed bit set, when
> =C2=A0=C2=A0 CONFIG_HAVE_ARCH_PARENT_PMD_YOUNG=3Dy.
> 3) It will not zigzag between the PGD table and the same PMD or PTE
> =C2=A0=C2=A0 table spanning multiple VMAs. In other words, it finishes =
all the
> =C2=A0=C2=A0 VMAs with the range of the same PMD or PTE table before it=
 returns
> =C2=A0=C2=A0 to the PGD table. This optimizes workloads that have large=
 numbers
> =C2=A0=C2=A0 of tiny VMAs, especially when CONFIG_PGTABLE_LEVELS=3D5.
>=20
> TLDR
> =3D=3D=3D=3D
> The current page reclaim is too expensive in terms of CPU usage and
> often making poor choices about what to evict. We would like to offer
> an alternative framework that is performant, versatile and
> straightforward.
>=20
> Repo
> =3D=3D=3D=3D
> git fetch https://linux-mm.googlesource.com/page-reclaim=C2=A0refs/chan=
ges/73/1173/1
>=20
> Gerrit https://linux-mm-review.googlesource.com/c/page-reclaim/+/1173
>=20
> Background
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
> DRAM is a major factor in total cost of ownership, and improving
> memory overcommit brings a high return on investment. Over the past
> decade of research and experimentation in memory overcommit, we
> observed a distinct trend across millions of servers and clients: the
> size of page cache has been decreasing because of the growing
> popularity of cloud storage. Nowadays anon pages account for more than
> 90% of our memory consumption and page cache contains mostly
> executable pages.
>=20
> Problems
> =3D=3D=3D=3D=3D=3D=3D=3D
> Notion of active/inactive
> -------------------------
> For servers equipped with hundreds of gigabytes of memory, the
> granularity of the active/inactive is too coarse to be useful for job
> scheduling. False active/inactive rates are relatively high, and thus
> the assumed savings may not materialize.
>=20
> For phones and laptops, executable pages are frequently evicted
> despite the fact that there are many less recently used anon pages.
> Major faults on executable pages cause "janks" (slow UI renderings)
> and negatively impact user experience.
>=20
> For lruvecs from different memcgs or nodes, comparisons are impossible
> due to the lack of a common frame of reference.
>=20
> Incremental scans via rmap
> --------------------------
> Each incremental scan picks up at where the last scan left off and
> stops after it has found a handful of unreferenced pages. For
> workloads using a large amount of anon memory, incremental scans lose
> the advantage under sustained memory pressure due to high ratios of
> the number of scanned pages to the number of reclaimed pages. In our
> case, the average ratio of pgscan to pgsteal is above 7.
>=20
> On top of that, the rmap has poor memory locality due to its complex
> data structures. The combined effects typically result in a high
> amount of CPU usage in the reclaim path. For example, with zram, a
> typical kswapd profile on v5.11 looks like:
> =C2=A0 31.03%=C2=A0 page_vma_mapped_walk
> =C2=A0 25.59%=C2=A0 lzo1x_1_do_compress
> =C2=A0=C2=A0 4.63%=C2=A0 do_raw_spin_lock
> =C2=A0=C2=A0 3.89%=C2=A0 vma_interval_tree_iter_next
> =C2=A0=C2=A0 3.33%=C2=A0 vma_interval_tree_subtree_search
>=20
> And with real swap, it looks like:
> =C2=A0 45.16%=C2=A0 page_vma_mapped_walk
> =C2=A0=C2=A0 7.61%=C2=A0 do_raw_spin_lock
> =C2=A0=C2=A0 5.69%=C2=A0 vma_interval_tree_iter_next
> =C2=A0=C2=A0 4.91%=C2=A0 vma_interval_tree_subtree_search
> =C2=A0=C2=A0 3.71%=C2=A0 page_referenced_one
>=20
> Solutions
> =3D=3D=3D=3D=3D=3D=3D=3D=3D
> Notion of generation numbers
> ----------------------------
> The notion of generation numbers introduces a quantitative approach to
> memory overcommit. A larger number of pages can be spread out across
> a configurable number of generations, and each generation includes all
> pages that have been referenced since the last generation. This
> improved granularity yields relatively low false active/inactive
> rates.
>=20
> Given an lruvec, scans of anon and file types and selections between
> them are all based on direct comparisons of generation numbers, which
> are simple and yet effective. For different lruvecs, comparisons are
> still possible based on birth times of generations.
>=20
> Differential scans via page tables
> ----------------------------------
> Each differential scan discovers all pages that have been referenced
> since the last scan. Specifically, it walks the mm_struct list
> associated with an lruvec to scan page tables of processes that have
> been scheduled since the last scan. The cost of each differential scan
> is roughly proportional to the number of referenced pages it
> discovers. Unless address spaces are extremely sparse, page tables
> usually have better memory locality than the rmap. The end result is
> generally a significant reduction in CPU usage, for workloads using a
> large amount of anon memory.
>=20
> Our real-world benchmark that browses popular websites in multiple
> Chrome tabs demonstrates 51% less CPU usage from kswapd and 52% (full)
> less PSI on v5.11. With this patchset, kswapd profile looks like:
> =C2=A0 49.36%=C2=A0 lzo1x_1_do_compress
> =C2=A0=C2=A0 4.54%=C2=A0 page_vma_mapped_walk
> =C2=A0=C2=A0 4.45%=C2=A0 memset_erms
> =C2=A0=C2=A0 3.47%=C2=A0 walk_pte_range
> =C2=A0=C2=A0 2.88%=C2=A0 zram_bvec_rw
>=20
> In addition, direct reclaim latency is reduced by 22% at 99th
> percentile and the number of refaults is reduced by 7%. Both metrics
> are important to phones and laptops as they are correlated to user
> experience.
>=20
> Framework
> =3D=3D=3D=3D=3D=3D=3D=3D=3D
> For each lruvec, evictable pages are divided into multiple
> generations. The youngest generation number is stored in
> lruvec->evictable.max_seq for both anon and file types as they are
> aged on an equal footing. The oldest generation numbers are stored in
> lruvec->evictable.min_seq[2] separately for anon and file types as
> clean file pages can be evicted regardless of may_swap or
> may_writepage. Generation numbers are truncated into
> order_base_2(MAX_NR_GENS+1) bits in order to fit into page->flags. The
> sliding window technique is used to prevent truncated generation
> numbers from overlapping. Each truncated generation number is an inde
> to lruvec->evictable.lists[MAX_NR_GENS][ANON_AND_FILE][MAX_NR_ZONES].
> Evictable pages are added to the per-zone lists indexed by max_seq or
> min_seq[2] (modulo MAX_NR_GENS), depending on whether they are being
> faulted in.
>=20
> Each generation is then divided into multiple tiers. Tiers represent
> levels of usage from file descriptors only. Pages accessed N times via
> file descriptors belong to tier order_base_2(N). In contrast to moving
> across generations which requires the lru lock, moving across tiers
> only involves an atomic operation on page->flags and therefore has a
> lower cost. A feedback loop modeled after the well-known PID
> controller monitors the refault rates across all tiers and decides
> when to activate pages from which tiers on the reclaim path.
>=20
> The framework comprises two conceptually independent components: the
> aging and the eviction, which can be invoked separately from user
> space.
>=20
> Aging
> -----
> The aging produces young generations. Given an lruvec, the aging scans
> page tables for referenced pages of this lruvec. Upon finding one, the
> aging updates its generation number to max_seq. After each round of
> scan, the aging increments max_seq.
>=20
> The aging maintains either a system-wide mm_struct list or per-memcg
> mm_struct lists and tracks whether an mm_struct is being used or has
> been used since the last scan. Multiple threads can concurrently work
> on the same mm_struct list, and each of them will be given a different
> mm_struct belonging to a process that has been scheduled since the
> last scan.
>=20
> The aging is due when both of min_seq[2] reaches max_seq-1, assuming
> both anon and file types are reclaimable.
>=20
> Eviction
> --------
> The eviction consumes old generations. Given an lruvec, the eviction
> scans the pages on the per-zone lists indexed by either of min_seq[2].
> It first tries to select a type based on the values of min_seq[2].
> When anon and file types are both available from the same generation,
> it selects the one that has a lower refault rate.
>=20
> During a scan, the eviction sorts pages according to their generation
> numbers, if the aging has found them referenced. It also moves pages
> from the tiers that have higher refault rates than tier 0 to the next
> generation.
>=20
> When it finds all the per-zone lists of a selected type are empty, the
> eviction increments min_seq[2] indexed by this selected type.
>=20
> Use cases
> =3D=3D=3D=3D=3D=3D=3D=3D=3D
> On Android, our most advanced simulation that generates memory
> pressure from realistic user behavior shows 18% fewer low-memory
> kills, which in turn reduces cold starts by 16%.
>=20
> On Borg, a similar approach enables us to identify jobs that
> underutilize their memory and downsize them considerably without
> compromising any of our service level indicators.
>=20
> On Chrome OS, our field telemetry reports 96% fewer low-memory tab
> discards and 59% fewer OOM kills from fully-utilized devices and no
> regressions in monitored user experience from underutilized devices.
>=20
> Working set estimation
> ----------------------
> User space can invoke the aging by writing "+ memcg_id node_id gen
> [swappiness]" to /sys/kernel/debug/lru_gen. This debugfs interface
> also provides the birth time and the size of each generation.
>=20
> Proactive reclaim
> -----------------
> User space can invoke the eviction by writing "- memcg_id node_id gen
> [swappiness] [nr_to_reclaim]" to /sys/kernel/debug/lru_gen. Multiple
> command lines are supported, so does concatenation with delimiters.
>=20
> Intensive buffered I/O
> ----------------------
> Tiers are specifically designed to improve the performance of
> intensive buffered I/O under memory pressure. The fio/io_uring
> benchmark shows 14% improvement in IOPS when randomly accessing
> Samsung PM981a in buffered I/O mode.
>=20
> For far memory tiering and NUMA-aware job scheduling, please refer to
> the reference section.
>=20
> FAQ
> =3D=3D=3D
> Why not try to improve the existing code?
> -----------------------------------------
> We have tried but concluded the aforementioned problems are
> fundamental, and therefore changes made on top of them will not result
> in substantial gains.
>=20
> What particular workloads does it help?
> ---------------------------------------
> This framework is designed to improve the performance of the page
> reclaim under any types of workloads.
>=20
> How would it benefit the community?
> -----------------------------------
> Google is committed to promoting sustainable development of the
> community. We hope successful adoptions of this framework will
> steadily climb over time. To that end, we would be happy to learn your
> workloads and work with you case by case, and we will do our best to
> keep the repo fully maintained. For those whose workloads rely on the
> existing code, we will make sure you will not be affected in any way.
>=20
> References
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
> 1. Long-term SLOs for reclaimed cloud computing resources
> =C2=A0=C2=A0 https://research.google/pubs/pub43017/
> 2. Profiling a warehouse-scale computer
> =C2=A0=C2=A0 https://research.google/pubs/pub44271/
> 3. Evaluation of NUMA-Aware Scheduling in Warehouse-Scale Clusters
> =C2=A0=C2=A0 https://research.google/pubs/pub48329/
> 4. Software-defined far memory in warehouse-scale computers
> =C2=A0=C2=A0 https://research.google/pubs/pub48551/
> 5. Borg: the Next Generation
> =C2=A0=C2=A0 https://research.google/pubs/pub49065/
>=20
> Yu Zhao (16):
> =C2=A0 include/linux/memcontrol.h: do not warn in page_memcg_rcu() if
> =C2=A0=C2=A0=C2=A0 !CONFIG_MEMCG
> =C2=A0 include/linux/nodemask.h: define next_memory_node() if !CONFIG_N=
UMA
> =C2=A0 include/linux/huge_mm.h: define is_huge_zero_pmd() if
> =C2=A0=C2=A0=C2=A0 !CONFIG_TRANSPARENT_HUGEPAGE
> =C2=A0 include/linux/cgroup.h: export cgroup_mutex
> =C2=A0 mm/swap.c: export activate_page()
> =C2=A0 mm, x86: support the access bit on non-leaf PMD entries
> =C2=A0 mm/vmscan.c: refactor shrink_node()
> =C2=A0 mm: multigenerational lru: groundwork
> =C2=A0 mm: multigenerational lru: activation
> =C2=A0 mm: multigenerational lru: mm_struct list
> =C2=A0 mm: multigenerational lru: aging
> =C2=A0 mm: multigenerational lru: eviction
> =C2=A0 mm: multigenerational lru: page reclaim
> =C2=A0 mm: multigenerational lru: user interface
> =C2=A0 mm: multigenerational lru: Kconfig
> =C2=A0 mm: multigenerational lru: documentation
>=20
> =C2=A0Documentation/vm/index.rst=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 |=C2=A0=C2=A0=C2=A0 1 +
> =C2=A0Documentation/vm/multigen_lru.rst |=C2=A0 192 +++
> =C2=A0arch/Kconfig=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =
|=C2=A0=C2=A0=C2=A0 9 +
> =C2=A0arch/x86/Kconfig=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=A0 1=
 +
> =C2=A0arch/x86/include/asm/pgtable.h=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=
=A0 2 +-
> =C2=A0arch/x86/mm/pgtable.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=A0 5 +-
> =C2=A0fs/exec.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=A0 2 +
> =C2=A0fs/fuse/dev.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=
=C2=A0=C2=A0 3 +-
> =C2=A0fs/proc/task_mmu.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=A0 3 +-
> =C2=A0include/linux/cgroup.h=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0 15 +-
> =C2=A0include/linux/huge_mm.h=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=A0 5 +
> =C2=A0include/linux/memcontrol.h=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 |=C2=A0=C2=A0=C2=A0 7 +-
> =C2=A0include/linux/mm.h=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=A0 2 +
> =C2=A0include/linux/mm_inline.h=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 |=C2=A0 294 ++++
> =C2=A0include/linux/mm_types.h=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 |=C2=A0 117 ++
> =C2=A0include/linux/mmzone.h=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 |=C2=A0 118 +-
> =C2=A0include/linux/nodemask.h=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=A0 1 +
> =C2=A0include/linux/page-flags-layout.h |=C2=A0=C2=A0 20 +-
> =C2=A0include/linux/page-flags.h=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 |=C2=A0=C2=A0=C2=A0 4 +-
> =C2=A0include/linux/pgtable.h=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=A0 4 +-
> =C2=A0include/linux/swap.h=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=A0 5 +-
> =C2=A0kernel/bounds.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=
=A0 6 +
> =C2=A0kernel/events/uprobes.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=A0 2 +-
> =C2=A0kernel/exit.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=
=C2=A0=C2=A0 1 +
> =C2=A0kernel/fork.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=
=C2=A0 10 +
> =C2=A0kernel/kthread.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=A0 1=
 +
> =C2=A0kernel/sched/core.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=A0 2 +
> =C2=A0mm/Kconfig=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 |=C2=A0=C2=A0 55 +
> =C2=A0mm/huge_memory.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=A0 5=
 +-
> =C2=A0mm/khugepaged.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=
=A0 2 +-
> =C2=A0mm/memcontrol.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0 2=
8 +
> =C2=A0mm/memory.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 |=C2=A0=C2=A0 14 +-
> =C2=A0mm/migrate.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =
|=C2=A0=C2=A0=C2=A0 2 +-
> =C2=A0mm/mm_init.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =
|=C2=A0=C2=A0 16 +-
> =C2=A0mm/mmzone.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 |=C2=A0=C2=A0=C2=A0 2 +
> =C2=A0mm/rmap.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=A0 6 +
> =C2=A0mm/swap.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 |=C2=A0=C2=A0 54 +-
> =C2=A0mm/swapfile.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=
=C2=A0=C2=A0 6 +-
> =C2=A0mm/userfaultfd.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0=C2=A0=C2=A0 2=
 +-
> =C2=A0mm/vmscan.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 | 2580 ++++++++++++++++++++++++++++-
> =C2=A0mm/workingset.c=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 |=C2=A0 179 +-
> =C2=A041 files changed, 3603 insertions(+), 180 deletions(-)
> =C2=A0create mode 100644 Documentation/vm/multigen_lru.rst
>=20

From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 7366CC433F5
	for <linux-kernel@archiver.kernel.org>; Thu,  3 Mar 2022 06:08:40 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S229775AbiCCGJW (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Thu, 3 Mar 2022 01:09:22 -0500
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:46950 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S229468AbiCCGJV (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 3 Mar 2022 01:09:21 -0500
Received: from mx0a-001b2d01.pphosted.com (mx0a-001b2d01.pphosted.com [148.163.156.1])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id E5926158E87;
        Wed,  2 Mar 2022 22:08:34 -0800 (PST)
Received: from pps.filterd (m0098394.ppops.net [127.0.0.1])
        by mx0a-001b2d01.pphosted.com (8.16.1.2/8.16.1.2) with SMTP id 2235Mj7l027039;
        Thu, 3 Mar 2022 06:07:17 GMT
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=ibm.com; h=from : to : cc : subject
 : in-reply-to : references : date : message-id : content-type :
 content-transfer-encoding : mime-version; s=pp1;
 bh=tAFo2nAVBHlJm3WcjxdMbljcwPYV8sMq6b92Cb6CE4k=;
 b=jnVcU8lBfvLwuVJpmTgJ5mT1SAtlcxDgfBsNv1Un6nzwd/gFRfOdIMh7db1SA1JJmPzd
 ynzEWS5zJeUKiHC6lKpfGBpkoEbQiV0MGQG+p3oblydwSzvoICLzGMAL7JilWdr1Rwvm
 ljYC3Ljar2SxIZu/vSJUZRvgXq3CHlmUAFVElqKr+KddJ4R4sLqwbfTE37ZYYBaz/0Ll
 X1ePODYgLIoL4iNLLYB8nl8Sdovf0mTKfuaNh5Hjj9Ud0Gy1vvpmV/wrzaGUEu9vV0Qm
 p3KxVqiHptc4TLOq8cNCrtfpNN3EENs0yK6X00RPmKkydImIl2ZvKviQ7C9KVUsPfG7U Cg== 
Received: from pps.reinject (localhost [127.0.0.1])
        by mx0a-001b2d01.pphosted.com with ESMTP id 3ejqfr0pjt-1
        (version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256 verify=NOT);
        Thu, 03 Mar 2022 06:07:17 +0000
Received: from m0098394.ppops.net (m0098394.ppops.net [127.0.0.1])
        by pps.reinject (8.16.0.43/8.16.0.43) with SMTP id 2235NL2D029716;
        Thu, 3 Mar 2022 06:07:16 GMT
Received: from ppma03ams.nl.ibm.com (62.31.33a9.ip4.static.sl-reverse.com [169.51.49.98])
        by mx0a-001b2d01.pphosted.com with ESMTP id 3ejqfr0phy-1
        (version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256 verify=NOT);
        Thu, 03 Mar 2022 06:07:16 +0000
Received: from pps.filterd (ppma03ams.nl.ibm.com [127.0.0.1])
        by ppma03ams.nl.ibm.com (8.16.1.2/8.16.1.2) with SMTP id 22364tR9020914;
        Thu, 3 Mar 2022 06:07:14 GMT
Received: from b06cxnps4075.portsmouth.uk.ibm.com (d06relay12.portsmouth.uk.ibm.com [9.149.109.197])
        by ppma03ams.nl.ibm.com with ESMTP id 3efbu9gthq-1
        (version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256 verify=NOT);
        Thu, 03 Mar 2022 06:07:13 +0000
Received: from d06av22.portsmouth.uk.ibm.com (d06av22.portsmouth.uk.ibm.com [9.149.105.58])
        by b06cxnps4075.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id 22367B5D34603464
        (version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256 verify=OK);
        Thu, 3 Mar 2022 06:07:11 GMT
Received: from d06av22.portsmouth.uk.ibm.com (unknown [127.0.0.1])
        by IMSVA (Postfix) with ESMTP id 4AD6E4C040;
        Thu,  3 Mar 2022 06:07:11 +0000 (GMT)
Received: from d06av22.portsmouth.uk.ibm.com (unknown [127.0.0.1])
        by IMSVA (Postfix) with ESMTP id CFC274C046;
        Thu,  3 Mar 2022 06:06:54 +0000 (GMT)
Received: from vajain21.in.ibm.com (unknown [9.163.20.88])
        by d06av22.portsmouth.uk.ibm.com (Postfix) with SMTP;
        Thu,  3 Mar 2022 06:06:54 +0000 (GMT)
Received: by vajain21.in.ibm.com (sSMTP sendmail emulation); Thu, 03 Mar 2022 11:36:51 +0530
From:   Vaibhav Jain <vaibhav@linux.ibm.com>
To:     Yu Zhao <yuzhao@google.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Johannes Weiner <hannes@cmpxchg.org>,
        Mel Gorman <mgorman@suse.de>, Michal Hocko <mhocko@kernel.org>
Cc:     Andi Kleen <ak@linux.intel.com>,
        Aneesh Kumar <aneesh.kumar@linux.ibm.com>,
        Barry Song <21cnbao@gmail.com>,
        Catalin Marinas <catalin.marinas@arm.com>,
        Dave Hansen <dave.hansen@linux.intel.com>,
        Hillf Danton <hdanton@sina.com>, Jens Axboe <axboe@kernel.dk>,
        Jesse Barnes <jsbarnes@google.com>,
        Jonathan Corbet <corbet@lwn.net>,
        Linus Torvalds <torvalds@linux-foundation.org>,
        Matthew Wilcox <willy@infradead.org>,
        Michael Larabel <Michael@michaellarabel.com>,
        Mike Rapoport <rppt@kernel.org>,
        Rik van Riel <riel@surriel.com>,
        Vlastimil Babka <vbabka@suse.cz>,
        Will Deacon <will@kernel.org>,
        Ying Huang <ying.huang@intel.com>,
        linux-arm-kernel@lists.infradead.org, linux-doc@vger.kernel.org,
        linux-kernel@vger.kernel.org, linux-mm@kvack.org,
        page-reclaim@google.com, x86@kernel.org,
        Yu Zhao <yuzhao@google.com>
Subject: Re: [PATCH v7 00/12] Multigenerational LRU Framework
In-Reply-To: <20220208081902.3550911-1-yuzhao@google.com>
References: <20220208081902.3550911-1-yuzhao@google.com>
Date:   Thu, 03 Mar 2022 11:36:51 +0530
Message-ID: <87czj3mux0.fsf@vajain21.in.ibm.com>
Content-Type: text/plain; charset=utf-8
X-TM-AS-GCONF: 00
X-Proofpoint-GUID: FeXKMhmdtlwCLyty8s-Fq5RgMLRv5zGR
X-Proofpoint-ORIG-GUID: H1XbNMky1SKwfWuu_TXkq8ftQ8J3pmpO
Content-Transfer-Encoding: quoted-printable
X-Proofpoint-UnRewURL: 0 URL was un-rewritten
MIME-Version: 1.0
X-Proofpoint-Virus-Version: vendor=baseguard
 engine=ICAP:2.0.205,Aquarius:18.0.816,Hydra:6.0.425,FMLib:17.11.64.514
 definitions=2022-03-03_01,2022-02-26_01,2022-02-23_01
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0 impostorscore=0 mlxscore=0
 suspectscore=0 mlxlogscore=999 spamscore=0 malwarescore=0 adultscore=0
 phishscore=0 lowpriorityscore=0 priorityscore=1501 clxscore=1011
 bulkscore=0 classifier=spam adjust=0 reason=mlx scancount=1
 engine=8.12.0-2201110000 definitions=main-2203030027
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org


In a synthetic MongoDB Benchmark (YCSB) seeing an average of ~19% throughput
improvement on POWER10(Radix MMU + 64K Page Size) with MGLRU patches on
top of v5.16 kernel for MongoDB + YCSB bench across three different
request distriburions namely Exponential,Uniform and Zipfan

Test-Results
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D

Average YCSB reported throughput (95% Confidence Interval):
|---------------------+---------------------+---------------------+--------=
-------------|
| Kernel-Type         | Exponential         | Uniform             | Zipfan =
             |
|---------------------+---------------------+---------------------+--------=
-------------|
| Base Kernel (v5.16) | 27324.701 =C2=B1 759.652 | 20671.590 =C2=B1 412.974=
 | 37713.761 =C2=B1 621.213 |
| v5.16 + MGLRU       | 32702.231 =C2=B1 287.957 | 24916.239 =C2=B1 217.977=
 | 44308.839 =C2=B1 701.829 |
|---------------------+---------------------+---------------------+--------=
-------------|
| Speedup             | 19.68% =C2=B1 4.03%      | 20.11% =C2=B1 2.95%     =
 | 17.49% =C2=B1 2.82%      |
|---------------------+---------------------+---------------------+--------=
-------------|

n =3D 11 Samples x 3 (Distributions) x 2 (Kernels) =3D 66 Observations

Test Environment
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
Cpu: POWER10 (architected), altivec supported
platform: pSeries
CPUs: 32
MMU: Radix
Page-Size: 64K
Total-Memory: 64G

Distro
-------
# cat /etc/os-release
NAME=3D"Red Hat Enterprise Linux"
VERSION=3D"8.4 (Ootpa)"
ID=3D"rhel"
ID_LIKE=3D"fedora"
VERSION_ID=3D"8.4"
PLATFORM_ID=3D"platform:el8"
PRETTY_NAME=3D"Red Hat Enterprise Linux 8.4 (Ootpa)"
ANSI_COLOR=3D"0;31"
CPE_NAME=3D"cpe:/o:redhat:enterprise_linux:8.4:GA"
HOME_URL=3D"https://www.redhat.com/"
DOCUMENTATION_URL=3D"https://access.redhat.com/documentation/red_hat_enterp=
rise_linux/8/"
BUG_REPORT_URL=3D"https://bugzilla.redhat.com/"

REDHAT_BUGZILLA_PRODUCT=3D"Red Hat Enterprise Linux 8"
REDHAT_BUGZILLA_PRODUCT_VERSION=3D8.4
REDHAT_SUPPORT_PRODUCT=3D"Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION=3D"8.4"

System-config
-------------
# cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]

# cat /proc/swaps=20
Filename                                Type            Size            Use=
d            Priority
/dev/dm-5                               partition       10485696        940=
864          -2

# cat /proc/sys/vm/overcommit_memory
0

#cat /proc/cmdline
<existing parameters> systemd.unified_cgroup_hierarchy=3D1 transparent_huge=
page=3Dnever

MongoDB data partition
----------------------
lsblk /dev/sdb
NAME MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sdb    8:16   0  128G  0 disk <home>/data/mongodb

mount | grep /dev/sdb
/dev/sdb on /root/vajain21/mglru/data/mongodb type ext4 (rw,relatime)

Testing Artifacts
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D

MongoDB-configuration
---------------------
MongoDB Commounity Server built from https://github.com/mongodb/mongo relea=
se v5.0.6

# mongod --version
db version v5.0.6
Build Info: {
      "version": "5.0.6",
      "gitVersion": "212a8dbb47f07427dae194a9c75baec1d81d9259",
      "openSSLVersion": "OpenSSL 1.1.1g FIPS  21 Apr 2020",
      "modules": [],
      "allocator": "tcmalloc",
      "environment": {
      "distarch": "ppc64le",
      "target_arch": "ppc64le"
      }
}

# cat /etc/mongod.conf=20
storage:
  dbPath: <home-path>/data/mongodb
  journal:
     enabled: true
  engine: wiredTiger
  wiredTiger:
    engineConfig:
    cacheSizeGB: 50
  net:
    bindIp: 127.0.0.1
    unixDomainSocket:
    enabled: true
    pathPrefix: /run/mongodb
setParameter:
    enableLocalhostAuthBypass: true

YCSB (https://github.com/vaibhav92/YCSB/tree/mongodb-domain-sockets)
--------------------------------------------------------------------

YCSB forked from https://github.com/brianfrankcooper/YCSB.git. This fixes a
problem with YCSB when trying to connect to MongoDB on a unix domain socket=
. PR
raised to the project at https://github.com/brianfrankcooper/YCSB/pull/1587

Head Commit: fb2555a77005ae70c26e4adc46c945caf4daa2f9(" [core] Generate
classpath from all dependencies rather than just compile scoped")

Kernel-Config
-------------

Base-Kernel: https://github.com/torvalds/linux/ v5.16
Base-Kernel-Config:
https://github.com/vaibhav92/mglru-benchmark/blob/auto_build/config-non-mgl=
ru

Test-Kernel: https://linux-mm.googlesource.com/page-reclaim refs/changes/49=
/1549/1
Test-Kernel-Config:
https://github.com/vaibhav92/mglru-benchmark/blob/auto_build/config-mglru

CONFIG_LRU_GEN=3Dy
CONFIG_LRU_GEN_ENABLED=3Dy
CONFIG_NR_LRU_GENS=3D4
CONFIG_TIERS_PER_GEN=3D4

YCSB:
recordcount=3D80000000
operationcount=3D80000000
readproportion=3D0.8
updateproportion=3D0.2
workload=3Dsite.ycsb.workloads.CoreWorkload
threads=3D64
requestdistributions=3D{uniform, exponential, zipfian}

Test-Bench
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
Source: https://github.com/vaibhav92/mglru-benchmark/tree/auto_build

Invoked via following command that will *destroy* contents of /dev/sdd
and use it as data disk for MongoDB:

$ export MONGODB_DISK=3D/dev/sdd; curl \
https://raw.githubusercontent.com/vaibhav92/mglru-benchmark/auto_build/buil=
d.sh
\ | sudo bash -s

Test-Methodology
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D

Setup
-----
1. Pull & Build testing artifact v5.16 Base Kernel, MGLRU Kernel,
MongoDB, YCSB & Qemu for qemu-img tools
2. Format and mount provided MongoDB Data disk with ext4.
3. Generate Systemd service/slice files for MongoDB and place them into /et=
c/systemd/system/
4. Generate MongoDB configration pointing to the data disk mount.
5. Start the built MongoDB instance.
6. Ensure that MongoDB is running.

Load Test Data
---------------
1. Ensure that MongoDB instance is stopped.
2. Unmount the data disk and reformat it with ext4.
3. Restart MongoDB.
4. Spin off YCSB to load data into the Mongo instance.
5. Stop MongoDB + Unmount data Disk
6. Create a qcow2 image of the data disk and store it with test data.
7. Kexec into base kernel.

Test Phase (Happens at each boot)
---------------------------------
1. Select the distribution to be used for YCSB from
{"Uniform","Exponential","Zipfan"}
2. Restore the MongoDB qcow2 data disk Image to the disk
3. Mount the data disk and restart MongoDB daemon.
4. Start YCSB to generate the workload on MongoDB.
5. Once finished collect results.
6. Kexec into next-kernel which keeps switching between Base-Kernel &
MGLRU-Kernel when all three distriutions have been tested.

Setup and Load Test Data stages can be accomplished by following command:
#export MONGODB_DISK=3D/dev/sdd; \
curl https://raw.githubusercontent.com/vaibhav92/mglru-benchmark/auto_build=
/build.sh | bash -s

Once completed successfully it will kexec into the base kernel and start the
Test phase on boot via systemd service named 'mglru-benchmark'

Based on above results,
Tested-by: Vaibhav Jain<vaibhav@linux.ibm.com>

Yu Zhao <yuzhao@google.com> writes:

> What's new
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
> 1) Addressed all the comments received on the mailing list and in the
>    meeting with the stakeholders (will note on individual patches).
> 2) Measured the performance improvements for each patch between 5-8
>    (reported in the commit messages).
>
> TLDR
> =3D=3D=3D=3D
> The current page reclaim is too expensive in terms of CPU usage and it
> often makes poor choices about what to evict. This patchset offers an
> alternative solution that is performant, versatile and straightforward.
>
> Patchset overview
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
> The design and implementation overview was moved to patch 12 so that
> people can finish reading this cover letter.
>
> 1. mm: x86, arm64: add arch_has_hw_pte_young()
> 2. mm: x86: add CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG
> Using hardware optimizations when trying to clear the accessed bit in
> many PTEs.
>
> 3. mm/vmscan.c: refactor shrink_node()
> A minor refactor.
>
> 4. mm: multigenerational LRU: groundwork
> Adding the basic data structure and the functions that insert/remove
> pages to/from the multigenerational LRU (MGLRU) lists.
>
> 5. mm: multigenerational LRU: minimal implementation
> A minimal (functional) implementation without any optimizations.
>
> 6. mm: multigenerational LRU: exploit locality in rmap
> Improving the efficiency when using the rmap.
>
> 7. mm: multigenerational LRU: support page table walks
> Adding the (optional) page table scanning.
>
> 8. mm: multigenerational LRU: optimize multiple memcgs
> Optimizing the overall performance for multiple memcgs running mixed
> types of workloads.
>
> 9. mm: multigenerational LRU: runtime switch
> Adding a runtime switch to enable or disable MGLRU.
>
> 10. mm: multigenerational LRU: thrashing prevention
> 11. mm: multigenerational LRU: debugfs interface
> Providing userspace with additional features like thrashing prevention,
> working set estimation and proactive reclaim.
>
> 12. mm: multigenerational LRU: documentation
> Adding a design doc and an admin guide.
>
> Benchmark results
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
> Independent lab results
> -----------------------
> Based on the popularity of searches [01] and the memory usage in
> Google's public cloud, the most popular open-source memory-hungry
> applications, in alphabetical order, are:
>       Apache Cassandra      Memcached
>       Apache Hadoop         MongoDB
>       Apache Spark          PostgreSQL
>       MariaDB (MySQL)       Redis
>
> An independent lab evaluated MGLRU with the most widely used benchmark
> suites for the above applications. They posted 960 data points along
> with kernel metrics and perf profiles collected over more than 500
> hours of total benchmark time. Their final reports show that, with 95%
> confidence intervals (CIs), the above applications all performed
> significantly better for at least part of their benchmark matrices.
>
> On 5.14:
> 1. Apache Spark [02] took 95% CIs [9.28, 11.19]% and [12.20, 14.93]%
>    less wall time to sort three billion random integers, respectively,
>    under the medium- and the high-concurrency conditions, when
>    overcommitting memory. There were no statistically significant
>    changes in wall time for the rest of the benchmark matrix.
> 2. MariaDB [03] achieved 95% CIs [5.24, 10.71]% and [20.22, 25.97]%
>    more transactions per minute (TPM), respectively, under the medium-
>    and the high-concurrency conditions, when overcommitting memory.
>    There were no statistically significant changes in TPM for the rest
>    of the benchmark matrix.
> 3. Memcached [04] achieved 95% CIs [23.54, 32.25]%, [20.76, 41.61]%
>    and [21.59, 30.02]% more operations per second (OPS), respectively,
>    for sequential access, random access and Gaussian (distribution)
>    access, when THP=3Dalways; 95% CIs [13.85, 15.97]% and
>    [23.94, 29.92]% more OPS, respectively, for random access and
>    Gaussian access, when THP=3Dnever. There were no statistically
>    significant changes in OPS for the rest of the benchmark matrix.
> 4. MongoDB [05] achieved 95% CIs [2.23, 3.44]%, [6.97, 9.73]% and
>    [2.16, 3.55]% more operations per second (OPS), respectively, for
>    exponential (distribution) access, random access and Zipfian
>    (distribution) access, when underutilizing memory; 95% CIs
>    [8.83, 10.03]%, [21.12, 23.14]% and [5.53, 6.46]% more OPS,
>    respectively, for exponential access, random access and Zipfian
>    access, when overcommitting memory.
>
> On 5.15:
> 5. Apache Cassandra [06] achieved 95% CIs [1.06, 4.10]%, [1.94, 5.43]%
>    and [4.11, 7.50]% more operations per second (OPS), respectively,
>    for exponential (distribution) access, random access and Zipfian
>    (distribution) access, when swap was off; 95% CIs [0.50, 2.60]%,
>    [6.51, 8.77]% and [3.29, 6.75]% more OPS, respectively, for
>    exponential access, random access and Zipfian access, when swap was
>    on.
> 6. Apache Hadoop [07] took 95% CIs [5.31, 9.69]% and [2.02, 7.86]%
>    less average wall time to finish twelve parallel TeraSort jobs,
>    respectively, under the medium- and the high-concurrency
>    conditions, when swap was on. There were no statistically
>    significant changes in average wall time for the rest of the
>    benchmark matrix.
> 7. PostgreSQL [08] achieved 95% CI [1.75, 6.42]% more transactions per
>    minute (TPM) under the high-concurrency condition, when swap was
>    off; 95% CIs [12.82, 18.69]% and [22.70, 46.86]% more TPM,
>    respectively, under the medium- and the high-concurrency
>    conditions, when swap was on. There were no statistically
>    significant changes in TPM for the rest of the benchmark matrix.
> 8. Redis [09] achieved 95% CIs [0.58, 5.94]%, [6.55, 14.58]% and
>    [11.47, 19.36]% more total operations per second (OPS),
>    respectively, for sequential access, random access and Gaussian
>    (distribution) access, when THP=3Dalways; 95% CIs [1.27, 3.54]%,
>    [10.11, 14.81]% and [8.75, 13.64]% more total OPS, respectively,
>    for sequential access, random access and Gaussian access, when
>    THP=3Dnever.
>
> Our lab results
> ---------------
> To supplement the above results, we ran the following benchmark suites
> on 5.16-rc7 and found no regressions [10]. (These synthetic benchmarks
> are popular among MM developers, but we prefer large-scale A/B
> experiments to validate improvements.)
>       fs_fio_bench_hdd_mq      pft
>       fs_lmbench               pgsql-hammerdb
>       fs_parallelio            redis
>       fs_postmark              stream
>       hackbench                sysbenchthread
>       kernbench                tpcc_spark
>       memcached                unixbench
>       multichase               vm-scalability
>       mutilate                 will-it-scale
>       nginx
>
> [01] https://trends.google.com
> [02] https://lore.kernel.org/lkml/20211102002002.92051-1-bot@edi.works/
> [03] https://lore.kernel.org/lkml/20211009054315.47073-1-bot@edi.works/
> [04] https://lore.kernel.org/lkml/20211021194103.65648-1-bot@edi.works/
> [05] https://lore.kernel.org/lkml/20211109021346.50266-1-bot@edi.works/
> [06] https://lore.kernel.org/lkml/20211202062806.80365-1-bot@edi.works/
> [07] https://lore.kernel.org/lkml/20211209072416.33606-1-bot@edi.works/
> [08] https://lore.kernel.org/lkml/20211218071041.24077-1-bot@edi.works/
> [09] https://lore.kernel.org/lkml/20211122053248.57311-1-bot@edi.works/
> [10] https://lore.kernel.org/lkml/20220104202247.2903702-1-yuzhao@google.=
com/
>
> Read-world applications
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
> Third-party testimonials
> ------------------------
> Konstantin wrote [11]:
>    I have Archlinux with 8G RAM + zswap + swap. While developing, I
>    have lots of apps opened such as multiple LSP-servers for different
>    langs, chats, two browsers, etc... Usually, my system gets quickly
>    to a point of SWAP-storms, where I have to kill LSP-servers,
>    restart browsers to free memory, etc, otherwise the system lags
>    heavily and is barely usable.
>=20=20=20=20
>    1.5 day ago I migrated from 5.11.15 kernel to 5.12 + the LRU
>    patchset, and I started up by opening lots of apps to create memory
>    pressure, and worked for a day like this. Till now I had *not a
>    single SWAP-storm*, and mind you I got 3.4G in SWAP. I was never
>    getting to the point of 3G in SWAP before without a single
>    SWAP-storm.
>
> An anonymous user wrote [12]:
>    Using that v5 for some time and confirm that difference under heavy
>    load and memory pressure is significant.
>
> Shuang wrote [13]:
>    With the MGLRU, fio achieved 95% CIs [38.95, 40.26]%, [4.12, 6.64]%
>    and [9.26, 10.36]% higher throughput, respectively, for random
>    access, Zipfian (distribution) access and Gaussian (distribution)
>    access, when the average number of jobs per CPU is 1; 95% CIs
>    [42.32, 49.15]%, [9.44, 9.89]% and [20.99, 22.86]% higher throughput,
>    respectively, for random access, Zipfian access and Gaussian access,
>    when the average number of jobs per CPU is 2.
>
> Daniel wrote [14]:
>    With memcached allocating ~100GB of byte-addressable Optante,
>    performance improvement in terms of throughput (measured as queries
>    per second) was about 10% for a series of workloads.
>
> Large-scale deployments
> -----------------------
> The downstream kernels that have been using MGLRU include:
> 1. Android ARCVM [15]
> 2. Arch Linux Zen [16]
> 3. Chrome OS [17]
> 4. Liquorix [18]
> 5. post-factum [19]
> 6. XanMod [20]
>
> We've rolled out MGLRU to tens of millions of Chrome OS users and
> about a million Android users. Google's fleetwide profiling [21] shows
> an overall 40% decrease in kswapd CPU usage, in addition to
> improvements in other UX metrics, e.g., an 85% decrease in the number
> of low-memory kills at the 75th percentile and an 18% decrease in
> rendering latency at the 50th percentile.
>
> [11] https://lore.kernel.org/lkml/140226722f2032c86301fbd326d91baefe3d7d2=
3.camel@yandex.ru/
> [12] https://phoronix.com/forums/forum/software/general-linux-open-source=
/1301258-mglru-is-a-very-enticing-enhancement-for-linux-in-2022?p=3D1301275=
#post1301275
> [13] https://lore.kernel.org/lkml/20220105024423.26409-1-szhai2@cs.roches=
ter.edu/
> [14] https://lore.kernel.org/linux-mm/CA+4-3vksGvKd18FgRinxhqHetBS1hQekJE=
2gwco8Ja-bJWKtFw@mail.gmail.com/
> [15] https://chromium.googlesource.com/chromiumos/third_party/kernel
> [16] https://archlinux.org
> [17] https://chromium.org
> [18] https://liquorix.net
> [19] https://gitlab.com/post-factum/pf-kernel
> [20] https://xanmod.org
> [21] https://research.google/pubs/pub44271/
>
> Summery
> =3D=3D=3D=3D=3D=3D=3D
> The facts are:
> 1. The independent lab results and the real-world applications
>    indicate substantial improvements; there are no known regressions.
> 2. Thrashing prevention, working set estimation and proactive reclaim
>    work out of the box; there are no equivalent solutions.
> 3. There is a lot of new code; nobody has demonstrated smaller changes
>    with similar effects.
>
> Our options, accordingly, are:
> 1. Given the amount of evidence, the reported improvements will likely
>    materialize for a wide range of workloads.
> 2. Gauging the interest from the past discussions [22][23][24], the
>    new features will likely be put to use for both personal computers
>    and data centers.
> 3. Based on Google's track record, the new code will likely be well
>    maintained in the long term. It'd be more difficult if not
>    impossible to achieve similar effects on top of the existing
>    design.
>
> [22] https://lore.kernel.org/lkml/20201005081313.732745-1-andrea.righi@ca=
nonical.com/
> [23] https://lore.kernel.org/lkml/20210716081449.22187-1-sj38.park@gmail.=
com/
> [24] https://lore.kernel.org/lkml/20211130201652.2218636d@mail.inbox.lv/
>
> Yu Zhao (12):
>   mm: x86, arm64: add arch_has_hw_pte_young()
>   mm: x86: add CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG
>   mm/vmscan.c: refactor shrink_node()
>   mm: multigenerational LRU: groundwork
>   mm: multigenerational LRU: minimal implementation
>   mm: multigenerational LRU: exploit locality in rmap
>   mm: multigenerational LRU: support page table walks
>   mm: multigenerational LRU: optimize multiple memcgs
>   mm: multigenerational LRU: runtime switch
>   mm: multigenerational LRU: thrashing prevention
>   mm: multigenerational LRU: debugfs interface
>   mm: multigenerational LRU: documentation
>
>  Documentation/admin-guide/mm/index.rst        |    1 +
>  Documentation/admin-guide/mm/multigen_lru.rst |  121 +
>  Documentation/vm/index.rst                    |    1 +
>  Documentation/vm/multigen_lru.rst             |  152 +
>  arch/Kconfig                                  |    9 +
>  arch/arm64/include/asm/pgtable.h              |   14 +-
>  arch/x86/Kconfig                              |    1 +
>  arch/x86/include/asm/pgtable.h                |    9 +-
>  arch/x86/mm/pgtable.c                         |    5 +-
>  fs/exec.c                                     |    2 +
>  fs/fuse/dev.c                                 |    3 +-
>  include/linux/cgroup.h                        |   15 +-
>  include/linux/memcontrol.h                    |   36 +
>  include/linux/mm.h                            |    8 +
>  include/linux/mm_inline.h                     |  214 ++
>  include/linux/mm_types.h                      |   78 +
>  include/linux/mmzone.h                        |  182 ++
>  include/linux/nodemask.h                      |    1 +
>  include/linux/page-flags-layout.h             |   19 +-
>  include/linux/page-flags.h                    |    4 +-
>  include/linux/pgtable.h                       |   17 +-
>  include/linux/sched.h                         |    4 +
>  include/linux/swap.h                          |    5 +
>  kernel/bounds.c                               |    3 +
>  kernel/cgroup/cgroup-internal.h               |    1 -
>  kernel/exit.c                                 |    1 +
>  kernel/fork.c                                 |    9 +
>  kernel/sched/core.c                           |    1 +
>  mm/Kconfig                                    |   50 +
>  mm/huge_memory.c                              |    3 +-
>  mm/memcontrol.c                               |   27 +
>  mm/memory.c                                   |   39 +-
>  mm/mm_init.c                                  |    6 +-
>  mm/page_alloc.c                               |    1 +
>  mm/rmap.c                                     |    7 +
>  mm/swap.c                                     |   55 +-
>  mm/vmscan.c                                   | 2831 ++++++++++++++++-
>  mm/workingset.c                               |  119 +-
>  38 files changed, 3908 insertions(+), 146 deletions(-)
>  create mode 100644 Documentation/admin-guide/mm/multigen_lru.rst
>  create mode 100644 Documentation/vm/multigen_lru.rst
>
> --=20
> 2.35.0.263.gb82422642f-goog
>
>

--=20
Cheers
~ Vaibhav

From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-arm-kernel-bounces+linux-arm-kernel=archiver.kernel.org@lists.infradead.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from bombadil.infradead.org (bombadil.infradead.org [198.137.202.133])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by smtp.lore.kernel.org (Postfix) with ESMTPS id 26BC5C433F5
	for <linux-arm-kernel@archiver.kernel.org>; Thu,  3 Mar 2022 06:09:27 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/relaxed;
	d=lists.infradead.org; s=bombadil.20210309; h=Sender:
	Content-Transfer-Encoding:Content-Type:List-Subscribe:List-Help:List-Post:
	List-Archive:List-Unsubscribe:List-Id:MIME-Version:Message-ID:Date:References
	:In-Reply-To:Subject:Cc:To:From:Reply-To:Content-ID:Content-Description:
	Resent-Date:Resent-From:Resent-Sender:Resent-To:Resent-Cc:Resent-Message-ID:
	List-Owner; bh=LS+9qOqhm9+ujSkr6wYG2QBxHotdGBIGhFrGIPmpvg0=; b=SqQ6YDlpuuY7vW
	jfNdqG0+n/3p+n5mZSZfJlFJ/AKe5XxksN5ruDrLGBVuWdbzsrvI5u3mDZpToF4utgW3w2qSUweAw
	fjEDrcOrymolOC0o3DeBdu/v0OEpaB2/yYmMtKQqhgjAihXd6OgUnPLQrYIyQpMmVtuld5AKRtU/7
	BupmkxknmX89T2OuZLtpsfniLmNPM6YI2hBW1A4KoVDekZXcdb50CBEzHS3OUd62RahUsjCyvemfh
	+Chv9XxdP27EkK+S8U0ev19JKzTQesjQYC27f1JNWnp63rtd4bw9q78uYRdA4B/lrtqcOHt0pCVHx
	3h/O+R8Fy2C5FvZ6nSkQ==;
Received: from localhost ([::1] helo=bombadil.infradead.org)
	by bombadil.infradead.org with esmtp (Exim 4.94.2 #2 (Red Hat Linux))
	id 1nPecv-005I91-9M; Thu, 03 Mar 2022 06:07:49 +0000
Received: from mx0a-001b2d01.pphosted.com ([148.163.156.1])
 by bombadil.infradead.org with esmtps (Exim 4.94.2 #2 (Red Hat Linux))
 id 1nPecp-005I84-FE
 for linux-arm-kernel@lists.infradead.org; Thu, 03 Mar 2022 06:07:46 +0000
Received: from pps.filterd (m0098394.ppops.net [127.0.0.1])
 by mx0a-001b2d01.pphosted.com (8.16.1.2/8.16.1.2) with SMTP id 2235Mj7l027039; 
 Thu, 3 Mar 2022 06:07:17 GMT
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=ibm.com;
 h=from : to : cc : subject
 : in-reply-to : references : date : message-id : content-type :
 content-transfer-encoding : mime-version; s=pp1;
 bh=tAFo2nAVBHlJm3WcjxdMbljcwPYV8sMq6b92Cb6CE4k=;
 b=jnVcU8lBfvLwuVJpmTgJ5mT1SAtlcxDgfBsNv1Un6nzwd/gFRfOdIMh7db1SA1JJmPzd
 ynzEWS5zJeUKiHC6lKpfGBpkoEbQiV0MGQG+p3oblydwSzvoICLzGMAL7JilWdr1Rwvm
 ljYC3Ljar2SxIZu/vSJUZRvgXq3CHlmUAFVElqKr+KddJ4R4sLqwbfTE37ZYYBaz/0Ll
 X1ePODYgLIoL4iNLLYB8nl8Sdovf0mTKfuaNh5Hjj9Ud0Gy1vvpmV/wrzaGUEu9vV0Qm
 p3KxVqiHptc4TLOq8cNCrtfpNN3EENs0yK6X00RPmKkydImIl2ZvKviQ7C9KVUsPfG7U Cg== 
Received: from pps.reinject (localhost [127.0.0.1])
 by mx0a-001b2d01.pphosted.com with ESMTP id 3ejqfr0pjt-1
 (version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256 verify=NOT);
 Thu, 03 Mar 2022 06:07:17 +0000
Received: from m0098394.ppops.net (m0098394.ppops.net [127.0.0.1])
 by pps.reinject (8.16.0.43/8.16.0.43) with SMTP id 2235NL2D029716;
 Thu, 3 Mar 2022 06:07:16 GMT
Received: from ppma03ams.nl.ibm.com (62.31.33a9.ip4.static.sl-reverse.com
 [169.51.49.98])
 by mx0a-001b2d01.pphosted.com with ESMTP id 3ejqfr0phy-1
 (version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256 verify=NOT);
 Thu, 03 Mar 2022 06:07:16 +0000
Received: from pps.filterd (ppma03ams.nl.ibm.com [127.0.0.1])
 by ppma03ams.nl.ibm.com (8.16.1.2/8.16.1.2) with SMTP id 22364tR9020914;
 Thu, 3 Mar 2022 06:07:14 GMT
Received: from b06cxnps4075.portsmouth.uk.ibm.com
 (d06relay12.portsmouth.uk.ibm.com [9.149.109.197])
 by ppma03ams.nl.ibm.com with ESMTP id 3efbu9gthq-1
 (version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256 verify=NOT);
 Thu, 03 Mar 2022 06:07:13 +0000
Received: from d06av22.portsmouth.uk.ibm.com (d06av22.portsmouth.uk.ibm.com
 [9.149.105.58])
 by b06cxnps4075.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id
 22367B5D34603464
 (version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256 verify=OK);
 Thu, 3 Mar 2022 06:07:11 GMT
Received: from d06av22.portsmouth.uk.ibm.com (unknown [127.0.0.1])
 by IMSVA (Postfix) with ESMTP id 4AD6E4C040;
 Thu,  3 Mar 2022 06:07:11 +0000 (GMT)
Received: from d06av22.portsmouth.uk.ibm.com (unknown [127.0.0.1])
 by IMSVA (Postfix) with ESMTP id CFC274C046;
 Thu,  3 Mar 2022 06:06:54 +0000 (GMT)
Received: from vajain21.in.ibm.com (unknown [9.163.20.88])
 by d06av22.portsmouth.uk.ibm.com (Postfix) with SMTP;
 Thu,  3 Mar 2022 06:06:54 +0000 (GMT)
Received: by vajain21.in.ibm.com (sSMTP sendmail emulation);
 Thu, 03 Mar 2022 11:36:51 +0530
From: Vaibhav Jain <vaibhav@linux.ibm.com>
To: Yu Zhao <yuzhao@google.com>, Andrew Morton <akpm@linux-foundation.org>,
 Johannes Weiner <hannes@cmpxchg.org>, Mel Gorman <mgorman@suse.de>, Michal
 Hocko <mhocko@kernel.org>
Cc: Andi Kleen <ak@linux.intel.com>, Aneesh Kumar <aneesh.kumar@linux.ibm.com>,
 Barry Song <21cnbao@gmail.com>, Catalin Marinas <catalin.marinas@arm.com>,
 Dave Hansen <dave.hansen@linux.intel.com>,
 Hillf Danton <hdanton@sina.com>, Jens Axboe <axboe@kernel.dk>,
 Jesse Barnes <jsbarnes@google.com>, Jonathan Corbet <corbet@lwn.net>,
 Linus Torvalds <torvalds@linux-foundation.org>, Matthew
 Wilcox <willy@infradead.org>, Michael Larabel <Michael@michaellarabel.com>,
 Mike Rapoport <rppt@kernel.org>, Rik van Riel <riel@surriel.com>,
 Vlastimil Babka <vbabka@suse.cz>, Will Deacon <will@kernel.org>,
 Ying Huang <ying.huang@intel.com>,
 linux-arm-kernel@lists.infradead.org, linux-doc@vger.kernel.org,
 linux-kernel@vger.kernel.org, linux-mm@kvack.org,
 page-reclaim@google.com, x86@kernel.org, Yu Zhao <yuzhao@google.com>
Subject: Re: [PATCH v7 00/12] Multigenerational LRU Framework
In-Reply-To: <20220208081902.3550911-1-yuzhao@google.com>
References: <20220208081902.3550911-1-yuzhao@google.com>
Date: Thu, 03 Mar 2022 11:36:51 +0530
Message-ID: <87czj3mux0.fsf@vajain21.in.ibm.com>
X-TM-AS-GCONF: 00
X-Proofpoint-GUID: FeXKMhmdtlwCLyty8s-Fq5RgMLRv5zGR
X-Proofpoint-ORIG-GUID: H1XbNMky1SKwfWuu_TXkq8ftQ8J3pmpO
X-Proofpoint-UnRewURL: 0 URL was un-rewritten
MIME-Version: 1.0
X-Proofpoint-Virus-Version: vendor=baseguard
 engine=ICAP:2.0.205,Aquarius:18.0.816,Hydra:6.0.425,FMLib:17.11.64.514
 definitions=2022-03-03_01,2022-02-26_01,2022-02-23_01
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
 impostorscore=0 mlxscore=0
 suspectscore=0 mlxlogscore=999 spamscore=0 malwarescore=0 adultscore=0
 phishscore=0 lowpriorityscore=0 priorityscore=1501 clxscore=1011
 bulkscore=0 classifier=spam adjust=0 reason=mlx scancount=1
 engine=8.12.0-2201110000 definitions=main-2203030027
X-CRM114-Version: 20100106-BlameMichelson ( TRE 0.8.0 (BSD) ) MR-646709E3 
X-CRM114-CacheID: sfid-20220302_220743_537130_36307225 
X-CRM114-Status: GOOD (  36.78  )
X-BeenThere: linux-arm-kernel@lists.infradead.org
X-Mailman-Version: 2.1.34
Precedence: list
List-Id: <linux-arm-kernel.lists.infradead.org>
List-Unsubscribe: <http://lists.infradead.org/mailman/options/linux-arm-kernel>, 
 <mailto:linux-arm-kernel-request@lists.infradead.org?subject=unsubscribe>
List-Archive: <http://lists.infradead.org/pipermail/linux-arm-kernel/>
List-Post: <mailto:linux-arm-kernel@lists.infradead.org>
List-Help: <mailto:linux-arm-kernel-request@lists.infradead.org?subject=help>
List-Subscribe: <http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>, 
 <mailto:linux-arm-kernel-request@lists.infradead.org?subject=subscribe>
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
Sender: "linux-arm-kernel" <linux-arm-kernel-bounces@lists.infradead.org>
Errors-To: linux-arm-kernel-bounces+linux-arm-kernel=archiver.kernel.org@lists.infradead.org
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From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-kernel-owner@kernel.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from vger.kernel.org (vger.kernel.org [23.128.96.18])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 96E74C433F5
	for <linux-kernel@archiver.kernel.org>; Wed,  5 Jan 2022 03:34:41 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S237224AbiAEDel (ORCPT <rfc822;linux-kernel@archiver.kernel.org>);
        Tue, 4 Jan 2022 22:34:41 -0500
Received: from slate.cs.rochester.edu ([128.151.167.14]:55642 "EHLO
        slate.cs.rochester.edu" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S230020AbiAEDeg (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Tue, 4 Jan 2022 22:34:36 -0500
X-Greylist: delayed 2874 seconds by postgrey-1.27 at vger.kernel.org; Tue, 04 Jan 2022 22:34:32 EST
Received: from node1x10a.cs.rochester.edu (node1x10a.cs.rochester.edu [192.5.53.74])
        by slate.cs.rochester.edu (8.14.7/8.14.7) with ESMTP id 2052ikd2018776
        (version=TLSv1/SSLv3 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256 verify=NO);
        Tue, 4 Jan 2022 21:44:46 -0500
Received: from node1x10a.cs.rochester.edu (localhost [127.0.0.1])
        by node1x10a.cs.rochester.edu (8.15.2/8.15.1) with ESMTP id 2052ikc5026449;
        Tue, 4 Jan 2022 21:44:46 -0500
Received: (from szhai2@localhost)
        by node1x10a.cs.rochester.edu (8.15.2/8.15.1/Submit) id 2052ieJE026448;
        Tue, 4 Jan 2022 21:44:40 -0500
From:   Shuang Zhai <szhai2@cs.rochester.edu>
To:     yuzhao@google.com
Cc:     Michael@michaellarabel.com, ak@linux.intel.com,
        akpm@linux-foundation.org, axboe@kernel.dk,
        catalin.marinas@arm.com, corbet@lwn.net,
        dave.hansen@linux.intel.com, hannes@cmpxchg.org, hdanton@sina.com,
        jsbarnes@google.com, linux-arm-kernel@lists.infradead.org,
        linux-doc@vger.kernel.org, linux-kernel@vger.kernel.org,
        linux-mm@kvack.org, mgorman@suse.de, mhocko@kernel.org,
        page-reclaim@google.com, riel@surriel.com,
        torvalds@linux-foundation.org, vbabka@suse.cz, will@kernel.org,
        willy@infradead.org, x86@kernel.org, ying.huang@intel.com
Subject: Re: [PATCH v6 0/9] Multigenerational LRU Framework
Date:   Tue,  4 Jan 2022 21:44:23 -0500
Message-Id: <20220105024423.26409-1-szhai2@cs.rochester.edu>
X-Mailer: git-send-email 2.21.3
In-Reply-To: <20220104202227.2903605-1-yuzhao@google.com>
References: <20220104202227.2903605-1-yuzhao@google.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Fio / pmem benchmark with MGLRU

TLDR
====
With the MGLRU, fio achieved 95% CIs [38.95, 40.26]%, [4.12, 6.64]%
and [9.26, 10.36]% higher throughput, respectively, for random
access, Zipfian (distribution) access and Gaussian (distribution)
access, when the average number of jobs per CPU is 1; 95% CIs
[42.32, 49.15]%, [9.44, 9.89]% and [20.99, 22.86]% higher throughput,
respectively, for random access, Zipfian access and Gaussian access,
when the average number of jobs per CPU is 2.

Background
==========
Many applications running on warehouse-scale computers heavily use
POSIX read(2)/write(2) and page cache, e.g., Apache Kafka, a
distributed streaming application used by "more than 80% of all
Fortune 100 companies" [1] and PostgreSQL, "the world's most advanced
open source relational database" [2].

Intel DC Persistent Memory, as an affordable alternative to DRAM, can
deliver large capacity and data persistence. Specifically, the device
used in this benchmark can achieve up to 36 GiB/s and 15 GiB/s
throughput, respectively, for sequential and random read access.

Our research group at the University of Rochester focuses on the
intersection of computer architecture and system software. My current
research interest is memory management on tiered memory systems.

Matrix
======
Kernels: version [+ patchset]
* Baseline: 5.15
* Patched: 5.15 + MGLRU

Access patterns (4KB read):
* Random (uniform)
* Zipfian (theta 0.8; the recommended range is 0-2)
* Gaussian (deviation 40; the possible range is 0-100)

Concurrency conditions (the average number of jobs per CPU):
* 1
* 2

Total file size (GB): 400 (~2x memory capacity)
Total configurations: 12
Data points per configuration: 10
Total run duration (minutes) per data point: ~30

Notes
-----
1. All files were stored on pmem. Each job had the exclusive access to
   a single file.
2. Due to the hardware limitation when accessing remote pmem [3],
   numactl was used to bind the fio processes to the local pmem. Only
   one of the two NUMA nodes was used during the benchmark.
3. During dry runs, we observed that the throughput doesn't improve
   beyond 2 jobs per CPU for random access. Moreover, the patched
   kernel showed consistent improvements over the baseline kernel
   when using 3 or 4 jobs per CPU.
4. We wanted to simulate the real-world scenarios and therefore used
   default swap configuration (on). Moreover, we didn't observe any
   negative impact on performance with dry runs that disabled swap.

Procedure
=========
<for each kernel>
    grub2-reboot <baseline, patched>
    <for each concurrency condition>
        <generate test files>
        <for each access pattern>
            <for each data point>
                <reboot>
                <run fio>

Hardware
--------
Memory (GiB per socket): 192
CPU (# per socket): 40
Pmem (GiB per socket): 768

Fio
---
$ fio -version
fio-3.28

$ numactl --cpubind=0 --membind=0 fio --name=randread \
  --directory=/mnt/pmem/ --size={10G, 5G} --io_size=1000TB \
  --time_based --numjobs={40, 80} --ioengine=io_uring \
  --ramp_time=20m --runtime=10m --iodepth=128 \
  --iodepth_batch_submit=32 --iodepth_batch_complete=32 \
  --rw=randread --random_distribution={random, zipf:0.8, normal:40} \
  --direct=0 --norandommap --group_reporting

Results
=======
Throughput
----------
The patched kernel achieved substantially higher throughput for all
three access patterns and two concurrency conditions. Specifically,
comparing the patched with the baseline kernel, fio achieved 95% CIs
[38.95, 40.26]%, [4.12, 6.64]% and [9.26, 10.36]% higher throughput,
respectively, for random access, Zipfian access, and Gaussian access,
when the average number of jobs per CPU is 1; 95% CIs [42.32, 49.15]%,
[9.44, 9.89]% and [20.99, 22.86]% higher throughput, respectively, for
random access, Zipfian access and Gaussian access, when the average
number of jobs per CPU is 2.

+---------------------+---------------+---------------+
| Mean MiB/s [95% CI] | 1 job / CPU   | 2 jobs / CPU  |
+---------------------+---------------+---------------+
| Random access       | 8411 / 11742  | 8417 / 12267  |
|                     | [3275, 3387]  | [3562, 4137]  |
+---------------------+---------------+---------------+
| Zipfian access      | 14576 / 15360 | 12932 / 14181 |
|                     | [600, 967]    | [1220, 1279]  |
+---------------------+---------------+---------------+
| Gaussian access     | 14564 / 15993 | 11513 / 14037 |
|                     | [1348, 1508]  | [2417, 2631]  |
+---------------------+---------------+---------------+
Table 1. Throughput comparison between the baseline and the patched
         kernels

The patched kernel exhibited less degradation in throughput when
running more concurrent jobs. Comparing 2 jobs per CPU with 1 job per
CPU, fio achieved 95% CIs [-11.54, -11.02]%, [-16.91, -12.01]% and
[-21.61, -20.30]% higher throughput, respectively, for random access,
Zipfian access and Gaussian access, when using the baseline kernel;
95% CIs [2.04, 6.92]%, [-8.86, -6.48]% and [-12.83, -11.64]% higher
throughput, respectively, for random access, Zipfian access and
Gaussian access, when using the patched kernel. There were no
statistically significant changes in throughput for the rest of the
test matrix.

+---------------------+-----------------+----------------+
| Mean MiB/s [95% CI] | Baseline kernel | Patched kernel |
+---------------------+-----------------+----------------+
| Random access       | 8411 / 8417     | 11741 / 12267  |
|                     | [-55, 69]       | [239, 812]     |
+---------------------+-----------------+----------------+
| Zipfian access      | 14576 / 12932   | 15360/ 14181   |
|                     | [-1682, -1607]  | [-1361, -996]  |
+---------------------+-----------------+----------------+
| Gaussian access     | 14565 / 11513   | 15993 / 14037  |
|                     | [-3147, -2957]  | [-2051, -1861] |
+---------------------+-----------------+----------------+
Table 2. Throughput comparison between 1 job per CPU and 2 jobs per
         CPU

Tail Latency
------------
Comparing the patched with the baseline kernel, fio experienced 95%
CIs [-41.77, -40.35]% and [6.64, 13.95]% higher latency at the 99th
percentile, respectively, for random access and Gaussian access, when
the average number of jobs per CPU is 1; 95% CIs [-41.97, -40.59]%,
[-47.74, -47.04]% and [-51.32, -50.27]% higher latency at the 99th
percentile, respectively, for random access, Zipfian access and
Gaussian access, when the average number of jobs per CPU is 2. There
were no statistically significant changes in latency at the 99th
percentile for the rest of the test matrix.

+------------------------------+----------------+------------------+
| 99th percentile latency (us) | 1 job / CPU    | 2 jobs / CPU     |
+------------------------------+----------------+------------------+
| Random access                | 12466 / 7347   | 25560 / 15008    |
|                              | [-5207, -5030] | [-10729, -10375] |
+------------------------------+----------------+------------------+
| Zipfian access               | 3395 / 3382    | 14563 / 7661     |
|                              | [-131, 105]    | [-6953,-6850]    |
+------------------------------+----------------+------------------+
| Gaussian access              | 3280 / 3618    | 15611 / 7681     |
|                              | [217, 457]     | [-8012, -7848]   |
+------------------------------+----------------+------------------+
Table 3. Comparison of the 99th percentile latency between the
         baseline and the patched kernels (lower is better)

Metrics collected during each run are available at:
https://github.com/zhaishuang1/MglruPerf/tree/master

A peek at 5.16-rc6
------------------
We also ran the benchmark on 5.16-rc6 with swap off. However, we
haven't collected enough data points to establish a 95% CI. Here are
a few numbers we've collected:

+----------------+------------+----------+----------------+----------+
| Access pattern | Jobs / CPU | 5.16-rc6 | 5.16-rc6-mglru | % change |
+----------------+------------+----------+----------------+----------+
| Random access  | 1          | 7467     | 10440          | 39.8%    |
+----------------+------------+----------+----------------+----------+
| Random access  | 2          | 7504     | 13417          | 78.8%    |
+----------------+------------+----------+----------------+----------+
| Random access  | 3          | 7511     | 13954          | 85.8%    |
+----------------+------------+----------+----------------+----------+
| Random access  | 4          | 7542     | 13925          | 84.6%    |
+----------------+------------+----------+----------------+----------+

Reference
=========
[1] https://kafka.apache.org/documentation/#design_filesystem
[2] https://www.postgresql.org/docs/11/runtime-config-resource.html#RUNTIME-CONFIG-RESOURCE-MEMORY
[3] System Evaluation of the Intel Optane byte-addressable NVM, MEMSYS 2019.

Appendix
========
Throughput
----------
$ cat raw_data_fio.r
v <- c(
    # baseline 40 procs random
    8467.89, 8428.34, 8383.32, 8253.12, 8464.65, 8307.42, 8424.78, 8434.44, 8474.88, 8468.26,
    # baseline 40 procs zipf
    14570.44, 14598.03, 14550.74, 14640.29, 14591.4, 14573.35, 14503.18, 14613.39, 14598.61, 14522.27,
    # baseline 40 procs gaussian
    14504.95, 14427.23, 14652.19, 14519.47, 14557.97, 14617.92, 14555.87, 14446.94, 14678.12, 14688.33,
    # baseline 80 procs random
    8427.51, 8267.23, 8437.48, 8432.37, 8441.4, 8454.26, 8413.13, 8412.44, 8444.36, 8444.32,
    # baseline 80 procs zipf
    12980.12, 12946.43, 12911.95, 12925.83, 12952.75, 12841.44, 12920.35, 12924.19, 12944.38, 12967.72,
    # baseline 80 procs gaussian
    11666.29, 11624.72, 11454.82, 11482.36, 11462.24, 11379.46, 11691.5, 11471.19, 11402.08, 11494.13,
    # patched 40 procs random
    11706.69, 11778.1, 11774.07, 11750.07, 11744.97, 11766.65, 11727.79, 11708.41, 11745.3, 11716.45,
    # patched 40 procs zipf
    15498.31, 14647.94, 15423.35, 15467.32, 15467.05, 15342.49, 15511.34, 15414.06, 15401.1, 15431.57,
    # patched 40 procs gaussian
    15957.86, 15957.13, 16022.69, 16035.85, 16150.2, 15904.5, 15943.36, 16036.78, 16025.95, 15900.56,
    # patched 80 procs random
    12568.51, 11772.25, 11622.15, 12057.66, 11971.72, 12693.36, 12399.71, 12553.23, 12242.74, 12793.34,
    # patched 80 procs zipf
    14194.78, 14213.61, 14148.66, 14182.35, 14183.91, 14192.23, 14163.2, 14179.7, 14162.12, 14196.34,
    # patched 80 procs gaussian
    14084.86, 13706.34, 14089.42, 14058.4, 14096.74, 14108.06, 14043.41, 14072.15, 14088.44, 14024.51
)

a <- array(v, dim = c(10, 3, 2, 2))

# baseline vs patched
for (concurr in 1:2) {
    for (dist in 1:3) {
        r <- t.test(a[, dist, concurr, 1], a[, dist, concurr, 2])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("concurr%d dist%d: no significance", concurr, dist)
        } else {
            s <- sprintf("concurr%d dist%d: [%.2f, %.2f]%%", concurr, dist, -p[2], -p[1])
        }
        print(s)
    }
}

# low concurr vs high concurr
for (kern in 1:2) {
    for (dist in 1:3) {
        r <- t.test(a[, dist, 1, kern], a[, dist, 2, kern])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("kern%d dist%d: no significance", kern, dist)
        } else {
            s <- sprintf("kern%d dist%d: [%.2f, %.2f]%%", kern, dist, -p[2], -p[1])
        }
        print(s)
    }
}

$ R -q -s -f raw_data_fio.r

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = -132.15, df = 11.177, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -3386.514 -3275.766
sample estimates:
mean of x mean of y
  8410.71  11741.85

[1] "concurr1 dist1: [38.95, 40.26]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = -9.5917, df = 9.4797, p-value = 3.463e-06
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -967.8353 -600.7307
sample estimates:
mean of x mean of y
 14576.17  15360.45

[1] "concurr1 dist2: [4.12, 6.64]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = -37.744, df = 17.33, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -1508.328 -1348.850
sample estimates:
mean of x mean of y
 14564.90  15993.49

[1] "concurr1 dist3: [9.26, 10.36]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = -30.144, df = 9.3334, p-value = 1.281e-10
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -4137.381 -3562.653
sample estimates:
mean of x mean of y
  8417.45  12267.47

[1] "concurr2 dist1: [42.32, 49.15]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = -92.164, df = 13.276, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -1279.417 -1220.931
sample estimates:
mean of x mean of y
 12931.52  14181.69

[1] "concurr2 dist2: [9.44, 9.89]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = -49.453, df = 17.863, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2631.656 -2417.052
sample estimates:
mean of x mean of y
 11512.88  14037.23

[1] "concurr2 dist3: [20.99, 22.86]%"

	Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = -0.22947, df = 16.403, p-value = 0.8213
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -68.88155  55.40155
sample estimates:
mean of x mean of y
  8410.71   8417.45

[1] "kern1 dist1: no significance"

	Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 91.86, df = 17.875, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 1607.021 1682.287
sample estimates:
mean of x mean of y
 14576.17  12931.52

[1] "kern1 dist2: [-11.54, -11.02]%"

	Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 67.477, df = 17.539, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 2956.815 3147.225
sample estimates:
mean of x mean of y
 14564.90  11512.88

[1] "kern1 dist3: [-21.61, -20.30]%"

	Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = -4.1443, df = 9.0781, p-value = 0.002459
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -812.1507 -239.0833
sample estimates:
mean of x mean of y
 11741.85  12267.47

[1] "kern2 dist1: [2.04, 6.92]%"

	Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 14.566, df = 9.1026, p-value = 1.291e-07
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  996.0064 1361.5196
sample estimates:
mean of x mean of y
 15360.45  14181.69

[1] "kern2 dist2: [-8.86, -6.48]%"

	Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 43.826, df = 15.275, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 1861.263 2051.247
sample estimates:
mean of x mean of y
 15993.49  14037.23

[1] "kern2 dist3: [-12.83, -11.64]%"

99th Percentile Latency
-----------------------
$ cat raw_data_fio_lat.r
v <- c(
    # baseline 40 procs random
    12649, 12387, 12518, 12518, 12518, 12387, 12518, 12518, 12387, 12256,
    # baseline 40 procs zipf
    3458, 3294, 3425, 3294, 3294, 3359, 3752, 3326, 3294, 3458,
    # baseline 40 procs gaussian
    3326, 3458, 3195, 3392, 3326, 3228, 3228, 3326, 3130, 3195,
    # baseline 80 procs random
    25560, 26084, 25560, 25560, 25297, 25297, 25822, 25560, 25560, 25297,
    # baseline 80 procs zipf
    14484, 14615, 14615, 14484, 14484, 14615, 14615, 14615, 14615, 14484,
    # baseline 80 procs gaussian
    15664, 15664, 15533, 15533, 15533, 15664, 15795, 15533, 15664, 15533,
    # patched 40 procs random
    7439, 7242, 7373, 7373, 7373, 7439, 7242, 7308, 7308, 7373,
    # patched 40 procs zipf
    3261, 3425, 3392, 3294, 3359, 3556, 3228, 3490, 3458, 3359,
    # patched 40 procs gaussian
    3687, 3523, 3556, 3523, 3752, 3654, 3884, 3490, 3392, 3720,
    # patched 80 procs random
    15008, 15008, 15008, 15008, 15008, 15008, 15008, 15008, 15008, 15008,
    # patched 80 procs zipf
    7701, 7635, 7701, 7701, 7635, 7635, 7701, 7635, 7635, 7635,
    # patched 80 procs gaussian
    7635, 7898, 7701, 7635, 7635, 7635, 7635, 7635, 7701, 7701
)

a <- array(v, dim = c(10, 3, 2, 2))

# baseline vs patched
for (concurr in 1:2) {
    for (dist in 1:3) {
        r <- t.test(a[, dist, concurr, 1], a[, dist, concurr, 2])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("concurr%d dist%d: no significance", concurr, dist)
        } else {
            s <- sprintf("concurr%d dist%d: [%.2f, %.2f]%%", concurr, dist, -p[2], -p[1])
        }
        print(s)
    }
}

$ R -q -s -f raw_data_fio_lat.r

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = 123.52, df = 15.287, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 5030.417 5206.783
sample estimates:
mean of x mean of y
  12465.6    7347.0

[1] "concurr1 dist1: [-41.77, -40.35]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = 0.23667, df = 16.437, p-value = 0.8158
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -104.7812  131.1812
sample estimates:
mean of x mean of y
   3395.4    3382.2

[1] "concurr1 dist2: no significance"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = -5.9754, df = 16.001, p-value = 1.94e-05
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -457.5065 -217.8935
sample estimates:
mean of x mean of y
   3280.4    3618.1

[1] "concurr1 dist3: [6.64, 13.95]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = 134.89, df = 9, p-value = 3.437e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 10374.74 10728.66
sample estimates:
mean of x mean of y
  25559.7   15008.0

[1] "concurr2 dist1: [-41.97, -40.59]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = 288.1, df = 13.292, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 6849.566 6952.834
sample estimates:
mean of x mean of y
  14562.6    7661.4

[1] "concurr2 dist2: [-47.74, -47.04]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = 203.64, df = 17.798, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 7848.616 8012.384
sample estimates:
mean of x mean of y
  15611.6    7681.1

[1] "concurr2 dist3: [-51.32, -50.27]%"

From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <linux-arm-kernel-bounces+linux-arm-kernel=archiver.kernel.org@lists.infradead.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from bombadil.infradead.org (bombadil.infradead.org [198.137.202.133])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by smtp.lore.kernel.org (Postfix) with ESMTPS id 01C11C433EF
	for <linux-arm-kernel@archiver.kernel.org>; Wed,  5 Jan 2022 02:47:13 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/relaxed;
	d=lists.infradead.org; s=bombadil.20210309; h=Sender:
	Content-Transfer-Encoding:Content-Type:List-Subscribe:List-Help:List-Post:
	List-Archive:List-Unsubscribe:List-Id:MIME-Version:References:In-Reply-To:
	Message-Id:Date:Subject:Cc:To:From:Reply-To:Content-ID:Content-Description:
	Resent-Date:Resent-From:Resent-Sender:Resent-To:Resent-Cc:Resent-Message-ID:
	List-Owner; bh=Tc0XOZg56Y299qLp+/1chJog3z80DUgdGs7+DIJ+c9I=; b=G/NxpUfx35U0p7
	oG8YhrLmNIKMI6P7JVn9AVjkyEnG2NXKlySuvl0rVPqlvxj+0wkFVZsCRNTF1oXTclrxFxU4YlFCE
	9HPg/eXU4cRpM9psXu9YCUMva8/u6Ve624b1wvzkVyURxHrI0rkvvGzLGUaL4HO6WQZT1JUuLhR2q
	SWuH6idsI2iPYw+b+/5lza5PZ+jBzAgcn4ypBSmX2jK69q28W59eyLuxbnWkL0PuFkZGfwajG/GWW
	s4bXOobj1oY2hwxZ1GTntqliDj92nAylzcrSsaL6iYRU31kFiK+wmrbtuFdPbTBz3X05L2jkvPY1l
	rsQZeRHCL5nG7MecTRRQ==;
Received: from localhost ([::1] helo=bombadil.infradead.org)
	by bombadil.infradead.org with esmtp (Exim 4.94.2 #2 (Red Hat Linux))
	id 1n4wJ5-00DNLh-2u; Wed, 05 Jan 2022 02:45:43 +0000
Received: from slate.cs.rochester.edu ([128.151.167.14])
 by bombadil.infradead.org with esmtps (Exim 4.94.2 #2 (Red Hat Linux))
 id 1n4wIy-00DNKx-SQ
 for linux-arm-kernel@lists.infradead.org; Wed, 05 Jan 2022 02:45:40 +0000
Received: from node1x10a.cs.rochester.edu (node1x10a.cs.rochester.edu
 [192.5.53.74])
 by slate.cs.rochester.edu (8.14.7/8.14.7) with ESMTP id 2052ikd2018776
 (version=TLSv1/SSLv3 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256 verify=NO);
 Tue, 4 Jan 2022 21:44:46 -0500
Received: from node1x10a.cs.rochester.edu (localhost [127.0.0.1])
 by node1x10a.cs.rochester.edu (8.15.2/8.15.1) with ESMTP id 2052ikc5026449;
 Tue, 4 Jan 2022 21:44:46 -0500
Received: (from szhai2@localhost)
 by node1x10a.cs.rochester.edu (8.15.2/8.15.1/Submit) id 2052ieJE026448;
 Tue, 4 Jan 2022 21:44:40 -0500
From: Shuang Zhai <szhai2@cs.rochester.edu>
To: yuzhao@google.com
Cc: Michael@michaellarabel.com, ak@linux.intel.com, akpm@linux-foundation.org, 
 axboe@kernel.dk, catalin.marinas@arm.com, corbet@lwn.net,
 dave.hansen@linux.intel.com, hannes@cmpxchg.org, hdanton@sina.com,
 jsbarnes@google.com, linux-arm-kernel@lists.infradead.org,
 linux-doc@vger.kernel.org, linux-kernel@vger.kernel.org,
 linux-mm@kvack.org, mgorman@suse.de, mhocko@kernel.org,
 page-reclaim@google.com, riel@surriel.com,
 torvalds@linux-foundation.org, vbabka@suse.cz, will@kernel.org,
 willy@infradead.org, x86@kernel.org, ying.huang@intel.com
Subject: Re: [PATCH v6 0/9] Multigenerational LRU Framework
Date: Tue,  4 Jan 2022 21:44:23 -0500
Message-Id: <20220105024423.26409-1-szhai2@cs.rochester.edu>
X-Mailer: git-send-email 2.21.3
In-Reply-To: <20220104202227.2903605-1-yuzhao@google.com>
References: <20220104202227.2903605-1-yuzhao@google.com>
MIME-Version: 1.0
X-CRM114-Version: 20100106-BlameMichelson ( TRE 0.8.0 (BSD) ) MR-646709E3 
X-CRM114-CacheID: sfid-20220104_184537_079774_CD3889D6 
X-CRM114-Status: GOOD (  13.08  )
X-BeenThere: linux-arm-kernel@lists.infradead.org
X-Mailman-Version: 2.1.34
Precedence: list
List-Id: <linux-arm-kernel.lists.infradead.org>
List-Unsubscribe: <http://lists.infradead.org/mailman/options/linux-arm-kernel>, 
 <mailto:linux-arm-kernel-request@lists.infradead.org?subject=unsubscribe>
List-Archive: <http://lists.infradead.org/pipermail/linux-arm-kernel/>
List-Post: <mailto:linux-arm-kernel@lists.infradead.org>
List-Help: <mailto:linux-arm-kernel-request@lists.infradead.org?subject=help>
List-Subscribe: <http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>, 
 <mailto:linux-arm-kernel-request@lists.infradead.org?subject=subscribe>
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit
Sender: "linux-arm-kernel" <linux-arm-kernel-bounces@lists.infradead.org>
Errors-To: linux-arm-kernel-bounces+linux-arm-kernel=archiver.kernel.org@lists.infradead.org

Fio / pmem benchmark with MGLRU

TLDR
====
With the MGLRU, fio achieved 95% CIs [38.95, 40.26]%, [4.12, 6.64]%
and [9.26, 10.36]% higher throughput, respectively, for random
access, Zipfian (distribution) access and Gaussian (distribution)
access, when the average number of jobs per CPU is 1; 95% CIs
[42.32, 49.15]%, [9.44, 9.89]% and [20.99, 22.86]% higher throughput,
respectively, for random access, Zipfian access and Gaussian access,
when the average number of jobs per CPU is 2.

Background
==========
Many applications running on warehouse-scale computers heavily use
POSIX read(2)/write(2) and page cache, e.g., Apache Kafka, a
distributed streaming application used by "more than 80% of all
Fortune 100 companies" [1] and PostgreSQL, "the world's most advanced
open source relational database" [2].

Intel DC Persistent Memory, as an affordable alternative to DRAM, can
deliver large capacity and data persistence. Specifically, the device
used in this benchmark can achieve up to 36 GiB/s and 15 GiB/s
throughput, respectively, for sequential and random read access.

Our research group at the University of Rochester focuses on the
intersection of computer architecture and system software. My current
research interest is memory management on tiered memory systems.

Matrix
======
Kernels: version [+ patchset]
* Baseline: 5.15
* Patched: 5.15 + MGLRU

Access patterns (4KB read):
* Random (uniform)
* Zipfian (theta 0.8; the recommended range is 0-2)
* Gaussian (deviation 40; the possible range is 0-100)

Concurrency conditions (the average number of jobs per CPU):
* 1
* 2

Total file size (GB): 400 (~2x memory capacity)
Total configurations: 12
Data points per configuration: 10
Total run duration (minutes) per data point: ~30

Notes
-----
1. All files were stored on pmem. Each job had the exclusive access to
   a single file.
2. Due to the hardware limitation when accessing remote pmem [3],
   numactl was used to bind the fio processes to the local pmem. Only
   one of the two NUMA nodes was used during the benchmark.
3. During dry runs, we observed that the throughput doesn't improve
   beyond 2 jobs per CPU for random access. Moreover, the patched
   kernel showed consistent improvements over the baseline kernel
   when using 3 or 4 jobs per CPU.
4. We wanted to simulate the real-world scenarios and therefore used
   default swap configuration (on). Moreover, we didn't observe any
   negative impact on performance with dry runs that disabled swap.

Procedure
=========
<for each kernel>
    grub2-reboot <baseline, patched>
    <for each concurrency condition>
        <generate test files>
        <for each access pattern>
            <for each data point>
                <reboot>
                <run fio>

Hardware
--------
Memory (GiB per socket): 192
CPU (# per socket): 40
Pmem (GiB per socket): 768

Fio
---
$ fio -version
fio-3.28

$ numactl --cpubind=0 --membind=0 fio --name=randread \
  --directory=/mnt/pmem/ --size={10G, 5G} --io_size=1000TB \
  --time_based --numjobs={40, 80} --ioengine=io_uring \
  --ramp_time=20m --runtime=10m --iodepth=128 \
  --iodepth_batch_submit=32 --iodepth_batch_complete=32 \
  --rw=randread --random_distribution={random, zipf:0.8, normal:40} \
  --direct=0 --norandommap --group_reporting

Results
=======
Throughput
----------
The patched kernel achieved substantially higher throughput for all
three access patterns and two concurrency conditions. Specifically,
comparing the patched with the baseline kernel, fio achieved 95% CIs
[38.95, 40.26]%, [4.12, 6.64]% and [9.26, 10.36]% higher throughput,
respectively, for random access, Zipfian access, and Gaussian access,
when the average number of jobs per CPU is 1; 95% CIs [42.32, 49.15]%,
[9.44, 9.89]% and [20.99, 22.86]% higher throughput, respectively, for
random access, Zipfian access and Gaussian access, when the average
number of jobs per CPU is 2.

+---------------------+---------------+---------------+
| Mean MiB/s [95% CI] | 1 job / CPU   | 2 jobs / CPU  |
+---------------------+---------------+---------------+
| Random access       | 8411 / 11742  | 8417 / 12267  |
|                     | [3275, 3387]  | [3562, 4137]  |
+---------------------+---------------+---------------+
| Zipfian access      | 14576 / 15360 | 12932 / 14181 |
|                     | [600, 967]    | [1220, 1279]  |
+---------------------+---------------+---------------+
| Gaussian access     | 14564 / 15993 | 11513 / 14037 |
|                     | [1348, 1508]  | [2417, 2631]  |
+---------------------+---------------+---------------+
Table 1. Throughput comparison between the baseline and the patched
         kernels

The patched kernel exhibited less degradation in throughput when
running more concurrent jobs. Comparing 2 jobs per CPU with 1 job per
CPU, fio achieved 95% CIs [-11.54, -11.02]%, [-16.91, -12.01]% and
[-21.61, -20.30]% higher throughput, respectively, for random access,
Zipfian access and Gaussian access, when using the baseline kernel;
95% CIs [2.04, 6.92]%, [-8.86, -6.48]% and [-12.83, -11.64]% higher
throughput, respectively, for random access, Zipfian access and
Gaussian access, when using the patched kernel. There were no
statistically significant changes in throughput for the rest of the
test matrix.

+---------------------+-----------------+----------------+
| Mean MiB/s [95% CI] | Baseline kernel | Patched kernel |
+---------------------+-----------------+----------------+
| Random access       | 8411 / 8417     | 11741 / 12267  |
|                     | [-55, 69]       | [239, 812]     |
+---------------------+-----------------+----------------+
| Zipfian access      | 14576 / 12932   | 15360/ 14181   |
|                     | [-1682, -1607]  | [-1361, -996]  |
+---------------------+-----------------+----------------+
| Gaussian access     | 14565 / 11513   | 15993 / 14037  |
|                     | [-3147, -2957]  | [-2051, -1861] |
+---------------------+-----------------+----------------+
Table 2. Throughput comparison between 1 job per CPU and 2 jobs per
         CPU

Tail Latency
------------
Comparing the patched with the baseline kernel, fio experienced 95%
CIs [-41.77, -40.35]% and [6.64, 13.95]% higher latency at the 99th
percentile, respectively, for random access and Gaussian access, when
the average number of jobs per CPU is 1; 95% CIs [-41.97, -40.59]%,
[-47.74, -47.04]% and [-51.32, -50.27]% higher latency at the 99th
percentile, respectively, for random access, Zipfian access and
Gaussian access, when the average number of jobs per CPU is 2. There
were no statistically significant changes in latency at the 99th
percentile for the rest of the test matrix.

+------------------------------+----------------+------------------+
| 99th percentile latency (us) | 1 job / CPU    | 2 jobs / CPU     |
+------------------------------+----------------+------------------+
| Random access                | 12466 / 7347   | 25560 / 15008    |
|                              | [-5207, -5030] | [-10729, -10375] |
+------------------------------+----------------+------------------+
| Zipfian access               | 3395 / 3382    | 14563 / 7661     |
|                              | [-131, 105]    | [-6953,-6850]    |
+------------------------------+----------------+------------------+
| Gaussian access              | 3280 / 3618    | 15611 / 7681     |
|                              | [217, 457]     | [-8012, -7848]   |
+------------------------------+----------------+------------------+
Table 3. Comparison of the 99th percentile latency between the
         baseline and the patched kernels (lower is better)

Metrics collected during each run are available at:
https://github.com/zhaishuang1/MglruPerf/tree/master

A peek at 5.16-rc6
------------------
We also ran the benchmark on 5.16-rc6 with swap off. However, we
haven't collected enough data points to establish a 95% CI. Here are
a few numbers we've collected:

+----------------+------------+----------+----------------+----------+
| Access pattern | Jobs / CPU | 5.16-rc6 | 5.16-rc6-mglru | % change |
+----------------+------------+----------+----------------+----------+
| Random access  | 1          | 7467     | 10440          | 39.8%    |
+----------------+------------+----------+----------------+----------+
| Random access  | 2          | 7504     | 13417          | 78.8%    |
+----------------+------------+----------+----------------+----------+
| Random access  | 3          | 7511     | 13954          | 85.8%    |
+----------------+------------+----------+----------------+----------+
| Random access  | 4          | 7542     | 13925          | 84.6%    |
+----------------+------------+----------+----------------+----------+

Reference
=========
[1] https://kafka.apache.org/documentation/#design_filesystem
[2] https://www.postgresql.org/docs/11/runtime-config-resource.html#RUNTIME-CONFIG-RESOURCE-MEMORY
[3] System Evaluation of the Intel Optane byte-addressable NVM, MEMSYS 2019.

Appendix
========
Throughput
----------
$ cat raw_data_fio.r
v <- c(
    # baseline 40 procs random
    8467.89, 8428.34, 8383.32, 8253.12, 8464.65, 8307.42, 8424.78, 8434.44, 8474.88, 8468.26,
    # baseline 40 procs zipf
    14570.44, 14598.03, 14550.74, 14640.29, 14591.4, 14573.35, 14503.18, 14613.39, 14598.61, 14522.27,
    # baseline 40 procs gaussian
    14504.95, 14427.23, 14652.19, 14519.47, 14557.97, 14617.92, 14555.87, 14446.94, 14678.12, 14688.33,
    # baseline 80 procs random
    8427.51, 8267.23, 8437.48, 8432.37, 8441.4, 8454.26, 8413.13, 8412.44, 8444.36, 8444.32,
    # baseline 80 procs zipf
    12980.12, 12946.43, 12911.95, 12925.83, 12952.75, 12841.44, 12920.35, 12924.19, 12944.38, 12967.72,
    # baseline 80 procs gaussian
    11666.29, 11624.72, 11454.82, 11482.36, 11462.24, 11379.46, 11691.5, 11471.19, 11402.08, 11494.13,
    # patched 40 procs random
    11706.69, 11778.1, 11774.07, 11750.07, 11744.97, 11766.65, 11727.79, 11708.41, 11745.3, 11716.45,
    # patched 40 procs zipf
    15498.31, 14647.94, 15423.35, 15467.32, 15467.05, 15342.49, 15511.34, 15414.06, 15401.1, 15431.57,
    # patched 40 procs gaussian
    15957.86, 15957.13, 16022.69, 16035.85, 16150.2, 15904.5, 15943.36, 16036.78, 16025.95, 15900.56,
    # patched 80 procs random
    12568.51, 11772.25, 11622.15, 12057.66, 11971.72, 12693.36, 12399.71, 12553.23, 12242.74, 12793.34,
    # patched 80 procs zipf
    14194.78, 14213.61, 14148.66, 14182.35, 14183.91, 14192.23, 14163.2, 14179.7, 14162.12, 14196.34,
    # patched 80 procs gaussian
    14084.86, 13706.34, 14089.42, 14058.4, 14096.74, 14108.06, 14043.41, 14072.15, 14088.44, 14024.51
)

a <- array(v, dim = c(10, 3, 2, 2))

# baseline vs patched
for (concurr in 1:2) {
    for (dist in 1:3) {
        r <- t.test(a[, dist, concurr, 1], a[, dist, concurr, 2])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("concurr%d dist%d: no significance", concurr, dist)
        } else {
            s <- sprintf("concurr%d dist%d: [%.2f, %.2f]%%", concurr, dist, -p[2], -p[1])
        }
        print(s)
    }
}

# low concurr vs high concurr
for (kern in 1:2) {
    for (dist in 1:3) {
        r <- t.test(a[, dist, 1, kern], a[, dist, 2, kern])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("kern%d dist%d: no significance", kern, dist)
        } else {
            s <- sprintf("kern%d dist%d: [%.2f, %.2f]%%", kern, dist, -p[2], -p[1])
        }
        print(s)
    }
}

$ R -q -s -f raw_data_fio.r

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = -132.15, df = 11.177, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -3386.514 -3275.766
sample estimates:
mean of x mean of y
  8410.71  11741.85

[1] "concurr1 dist1: [38.95, 40.26]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = -9.5917, df = 9.4797, p-value = 3.463e-06
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -967.8353 -600.7307
sample estimates:
mean of x mean of y
 14576.17  15360.45

[1] "concurr1 dist2: [4.12, 6.64]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = -37.744, df = 17.33, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -1508.328 -1348.850
sample estimates:
mean of x mean of y
 14564.90  15993.49

[1] "concurr1 dist3: [9.26, 10.36]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = -30.144, df = 9.3334, p-value = 1.281e-10
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -4137.381 -3562.653
sample estimates:
mean of x mean of y
  8417.45  12267.47

[1] "concurr2 dist1: [42.32, 49.15]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = -92.164, df = 13.276, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -1279.417 -1220.931
sample estimates:
mean of x mean of y
 12931.52  14181.69

[1] "concurr2 dist2: [9.44, 9.89]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = -49.453, df = 17.863, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2631.656 -2417.052
sample estimates:
mean of x mean of y
 11512.88  14037.23

[1] "concurr2 dist3: [20.99, 22.86]%"

	Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = -0.22947, df = 16.403, p-value = 0.8213
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -68.88155  55.40155
sample estimates:
mean of x mean of y
  8410.71   8417.45

[1] "kern1 dist1: no significance"

	Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 91.86, df = 17.875, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 1607.021 1682.287
sample estimates:
mean of x mean of y
 14576.17  12931.52

[1] "kern1 dist2: [-11.54, -11.02]%"

	Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 67.477, df = 17.539, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 2956.815 3147.225
sample estimates:
mean of x mean of y
 14564.90  11512.88

[1] "kern1 dist3: [-21.61, -20.30]%"

	Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = -4.1443, df = 9.0781, p-value = 0.002459
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -812.1507 -239.0833
sample estimates:
mean of x mean of y
 11741.85  12267.47

[1] "kern2 dist1: [2.04, 6.92]%"

	Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 14.566, df = 9.1026, p-value = 1.291e-07
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  996.0064 1361.5196
sample estimates:
mean of x mean of y
 15360.45  14181.69

[1] "kern2 dist2: [-8.86, -6.48]%"

	Welch Two Sample t-test

data:  a[, dist, 1, kern] and a[, dist, 2, kern]
t = 43.826, df = 15.275, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 1861.263 2051.247
sample estimates:
mean of x mean of y
 15993.49  14037.23

[1] "kern2 dist3: [-12.83, -11.64]%"

99th Percentile Latency
-----------------------
$ cat raw_data_fio_lat.r
v <- c(
    # baseline 40 procs random
    12649, 12387, 12518, 12518, 12518, 12387, 12518, 12518, 12387, 12256,
    # baseline 40 procs zipf
    3458, 3294, 3425, 3294, 3294, 3359, 3752, 3326, 3294, 3458,
    # baseline 40 procs gaussian
    3326, 3458, 3195, 3392, 3326, 3228, 3228, 3326, 3130, 3195,
    # baseline 80 procs random
    25560, 26084, 25560, 25560, 25297, 25297, 25822, 25560, 25560, 25297,
    # baseline 80 procs zipf
    14484, 14615, 14615, 14484, 14484, 14615, 14615, 14615, 14615, 14484,
    # baseline 80 procs gaussian
    15664, 15664, 15533, 15533, 15533, 15664, 15795, 15533, 15664, 15533,
    # patched 40 procs random
    7439, 7242, 7373, 7373, 7373, 7439, 7242, 7308, 7308, 7373,
    # patched 40 procs zipf
    3261, 3425, 3392, 3294, 3359, 3556, 3228, 3490, 3458, 3359,
    # patched 40 procs gaussian
    3687, 3523, 3556, 3523, 3752, 3654, 3884, 3490, 3392, 3720,
    # patched 80 procs random
    15008, 15008, 15008, 15008, 15008, 15008, 15008, 15008, 15008, 15008,
    # patched 80 procs zipf
    7701, 7635, 7701, 7701, 7635, 7635, 7701, 7635, 7635, 7635,
    # patched 80 procs gaussian
    7635, 7898, 7701, 7635, 7635, 7635, 7635, 7635, 7701, 7701
)

a <- array(v, dim = c(10, 3, 2, 2))

# baseline vs patched
for (concurr in 1:2) {
    for (dist in 1:3) {
        r <- t.test(a[, dist, concurr, 1], a[, dist, concurr, 2])
        print(r)

        p <- r$conf.int * 100 / r$estimate[1]
        if ((p[1] > 0 && p[2] < 0) || (p[1] < 0 && p[2] > 0)) {
            s <- sprintf("concurr%d dist%d: no significance", concurr, dist)
        } else {
            s <- sprintf("concurr%d dist%d: [%.2f, %.2f]%%", concurr, dist, -p[2], -p[1])
        }
        print(s)
    }
}

$ R -q -s -f raw_data_fio_lat.r

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = 123.52, df = 15.287, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 5030.417 5206.783
sample estimates:
mean of x mean of y
  12465.6    7347.0

[1] "concurr1 dist1: [-41.77, -40.35]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = 0.23667, df = 16.437, p-value = 0.8158
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -104.7812  131.1812
sample estimates:
mean of x mean of y
   3395.4    3382.2

[1] "concurr1 dist2: no significance"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = -5.9754, df = 16.001, p-value = 1.94e-05
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -457.5065 -217.8935
sample estimates:
mean of x mean of y
   3280.4    3618.1

[1] "concurr1 dist3: [6.64, 13.95]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = 134.89, df = 9, p-value = 3.437e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 10374.74 10728.66
sample estimates:
mean of x mean of y
  25559.7   15008.0

[1] "concurr2 dist1: [-41.97, -40.59]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = 288.1, df = 13.292, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 6849.566 6952.834
sample estimates:
mean of x mean of y
  14562.6    7661.4

[1] "concurr2 dist2: [-47.74, -47.04]%"

	Welch Two Sample t-test

data:  a[, dist, concurr, 1] and a[, dist, concurr, 2]
t = 203.64, df = 17.798, p-value < 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 7848.616 8012.384
sample estimates:
mean of x mean of y
  15611.6    7681.1

[1] "concurr2 dist3: [-51.32, -50.27]%"

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel


From mboxrd@z Thu Jan  1 00:00:00 1970
Return-Path: <owner-linux-mm@kvack.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from kanga.kvack.org (kanga.kvack.org [205.233.56.17])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 7D555C433F5
	for <linux-mm@archiver.kernel.org>; Wed, 12 Jan 2022 16:18:05 +0000 (UTC)
Received: by kanga.kvack.org (Postfix)
	id 13D7E6B0195; Wed, 12 Jan 2022 11:18:05 -0500 (EST)
Received: by kanga.kvack.org (Postfix, from userid 40)
	id 0EDC16B0196; Wed, 12 Jan 2022 11:18:05 -0500 (EST)
X-Delivered-To: int-list-linux-mm@kvack.org
Received: by kanga.kvack.org (Postfix, from userid 63042)
	id EA8566B0197; Wed, 12 Jan 2022 11:18:04 -0500 (EST)
X-Delivered-To: linux-mm@kvack.org
Received: from forelay.hostedemail.com (smtprelay0067.hostedemail.com [216.40.44.67])
	by kanga.kvack.org (Postfix) with ESMTP id C05576B0195
	for <linux-mm@kvack.org>; Wed, 12 Jan 2022 11:18:04 -0500 (EST)
Received: from smtpin09.hostedemail.com (10.5.19.251.rfc1918.com [10.5.19.251])
	by forelay04.hostedemail.com (Postfix) with ESMTP id 58E7795197
	for <linux-mm@kvack.org>; Wed, 12 Jan 2022 16:18:04 +0000 (UTC)
X-FDA: 79022141688.09.0F98A9D
Received: from mail-io1-f47.google.com (mail-io1-f47.google.com [209.85.166.47])
	by imf16.hostedemail.com (Postfix) with ESMTP id D5646180019
	for <linux-mm@kvack.org>; Wed, 12 Jan 2022 16:18:03 +0000 (UTC)
Received: by mail-io1-f47.google.com with SMTP id o7so4337598ioo.9
        for <linux-mm@kvack.org>; Wed, 12 Jan 2022 08:18:03 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=mtu.edu; s=google;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to
         :cc;
        bh=uX4CuZrEDPoEI3mLrH+dV7WoOJjqJH28sJzHNN8i4Kw=;
        b=Fcxt+ZbN0RQY4bhHKsko98bBv13yh8QaWOnpLDvCNErkioyxd2Yhm9g+Vnu9MOjXYq
         WNx6gLSttZxsyQNQ8ir1/WTQNoG79jJagEXS5VqD0mmgqCMEuOhHw4VBfPb+D7fM0aBM
         KKUUuK8GIXnPCdXqpOd8ulhcbLkSTLKFdWaH/18RuXucF2Xg9hc3F3jJ3sGIA+SqnfwD
         ons50klJDWoxxQTFVOOK5U2o/6HmsC7oTF6+1Qh1Xx8jc5lHc2TKHb9tKmahMTfDkZvl
         svowg8tMGv+noca2q2cS8zObQQyE8LASFHUkHl0I3mdom/IsIJbSWATPBrcVhn3u38/D
         iYrA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:mime-version:references:in-reply-to:from:date
         :message-id:subject:to:cc;
        bh=uX4CuZrEDPoEI3mLrH+dV7WoOJjqJH28sJzHNN8i4Kw=;
        b=MOPlu1+MQEW9Bizi1jUY/bJIGwdYPcAPLa25evYEVkOGnDYrzV//EwP+A3L8cwFz+J
         SqjI42sSf0bMM1scSzLQJikSTm+QBvqwjYZbar53uGGplkCiVgRqTUuJ7vF2NGAyuqsm
         EjZ7H5uGaMT+ndwh9T/L/fBnZFItYXyvDrjeZv5BFG/6MQMVh4radoHXKTiB59LyvLtp
         lR+2c4vn+iiyrlV/N+L7CKyu7JKAPR5FDoEAjcn2iouDpspCaTPQof6J3RX5O6NMGcI7
         7DGFkVRa48hhWNTXbpn6cnMj2oQmaerotKE/9DHtdT5qcI8LDKbZXnL9bbe+98erd2u0
         Lqag==
X-Gm-Message-State: AOAM530tdK1OmG1wrekvxfYPErzKmzbAbXn9R/N5RrVE2d8+hrENS0mD
	i1h9SQd9dOhfvUvP6vnCoWXR5pJDKgRRnxYdplg=
X-Google-Smtp-Source: ABdhPJzcRCaOOrl3B/HFpMuy7suaqGfN1oqRrHdxFE0vp3nmID3F3zPLkZ61+I9MoSntOM5aocU95V1q4wzn1yblyW4=
X-Received: by 2002:a05:6638:72e:: with SMTP id j14mr224858jad.246.1642004282861;
 Wed, 12 Jan 2022 08:18:02 -0800 (PST)
MIME-Version: 1.0
References: <20220104202227.2903605-1-yuzhao@google.com> <YdSuSHa/Vjl6bPkg@google.com>
 <Yd1Css8+jsspeZHh@google.com> <CAMwLHrZi0ZGFsuc74Yj3DErT1zyG2DEudS3ZgPEsexj0XfZJyA@mail.gmail.com>
In-Reply-To: <CAMwLHrZi0ZGFsuc74Yj3DErT1zyG2DEudS3ZgPEsexj0XfZJyA@mail.gmail.com>
From: Daniel Byrne <djbyrne@mtu.edu>
Date: Wed, 12 Jan 2022 11:17:50 -0500
Message-ID: <CA+4-3vksGvKd18FgRinxhqHetBS1hQekJE2gwco8Ja-bJWKtFw@mail.gmail.com>
Subject: Re: [PATCH v6 0/9] Multigenerational LRU Framework
To: Sofia Trinh <sofia.trinh@edi.works>
Cc: Yu Zhao <yuzhao@google.com>, Alexandre Frade <kernel@xanmod.org>, 
	Brian Geffon <bgeffon@google.com>, Daniel Byrne <djbyrne@mtu.edu>, 
	=?UTF-8?Q?Holger_Hoffst=C3=A4tte?= <holger@applied-asynchrony.com>, 
	Jan Alexander Steffens <heftig@archlinux.org>, Shuang Zhai <szhai2@cs.rochester.edu>, 
	Steven Barrett <steven@liquorix.net>, Suleiman Souhlal <suleiman@google.com>, 
	Andi Kleen <ak@linux.intel.com>, Catalin Marinas <catalin.marinas@arm.com>, 
	Dave Hansen <dave.hansen@linux.intel.com>, Hillf Danton <hdanton@sina.com>, 
	Jens Axboe <axboe@kernel.dk>, Jesse Barnes <jsbarnes@google.com>, 
	Johannes Weiner <hannes@cmpxchg.org>, Jonathan Corbet <corbet@lwn.net>, 
	Matthew Wilcox <willy@infradead.org>, Mel Gorman <mgorman@suse.de>, 
	Michael Larabel <Michael@michaellarabel.com>, Michal Hocko <mhocko@kernel.org>, 
	Rik van Riel <riel@surriel.com>, Vlastimil Babka <vbabka@suse.cz>, Will Deacon <will@kernel.org>, 
	Ying Huang <ying.huang@intel.com>, linux-arm-kernel@lists.infradead.org, 
	linux-doc@vger.kernel.org, linux-kernel@vger.kernel.org, linux-mm@kvack.org, 
	page-reclaim@google.com, x86@kernel.org, 
	Andrew Morton <akpm@linux-foundation.org>, Linus Torvalds <torvalds@linux-foundation.org>
Content-Type: multipart/alternative; boundary="000000000000ffecb705d564e999"
X-Rspamd-Server: rspam04
X-Rspamd-Queue-Id: D5646180019
X-Stat-Signature: pn5fbnrb8phswi7r6nwjhakk8o3wcton
Authentication-Results: imf16.hostedemail.com;
	dkim=pass header.d=mtu.edu header.s=google header.b=Fcxt+ZbN;
	spf=pass (imf16.hostedemail.com: domain of byrnedj12@gmail.com designates 209.85.166.47 as permitted sender) smtp.mailfrom=byrnedj12@gmail.com;
	dmarc=pass (policy=none) header.from=mtu.edu
X-HE-Tag: 1642004283-529290
X-Bogosity: Ham, tests=bogofilter, spamicity=0.000000, version=1.2.4
Sender: owner-linux-mm@kvack.org
Precedence: bulk
X-Loop: owner-majordomo@kvack.org
List-ID: <linux-mm.kvack.org>

--000000000000ffecb705d564e999
Content-Type: text/plain; charset="UTF-8"

Tested-by: Daniel Byrne <djbyrne@mtu.edu> with memcached allocating ~100GB
of byte-addressable Optante, performance improvement in terms of throughput
(measured as queries per second) was about 10% for a series of workloads.

On Wed, Jan 12, 2022 at 1:07 AM Sofia Trinh <sofia.trinh@edi.works> wrote:

> On Tue, Jan 11, 2022 at 12:41 AM Yu Zhao <yuzhao@google.com> wrote:
> >
> > On Tue, Jan 04, 2022 at 01:30:00PM -0700, Yu Zhao wrote:
> > > On Tue, Jan 04, 2022 at 01:22:19PM -0700, Yu Zhao wrote:
> > > > TLDR
> > > > ====
> > > > The current page reclaim is too expensive in terms of CPU usage and
> it
> > > > often makes poor choices about what to evict. This patchset offers an
> > > > alternative solution that is performant, versatile and
> > > > straightforward.
> > >
> > > <snipped>
> > >
> > > > Summery
> > > > =======
> > > > The facts are:
> > > > 1. The independent lab results and the real-world applications
> > > >    indicate substantial improvements; there are no known regressions.
> > > > 2. Thrashing prevention, working set estimation and proactive reclaim
> > > >    work out of the box; there are no equivalent solutions.
> > > > 3. There is a lot of new code; nobody has demonstrated smaller
> changes
> > > >    with similar effects.
> > > >
> > > > Our options, accordingly, are:
> > > > 1. Given the amount of evidence, the reported improvements will
> likely
> > > >    materialize for a wide range of workloads.
> > > > 2. Gauging the interest from the past discussions [14][15][16], the
> > > >    new features will likely be put to use for both personal computers
> > > >    and data centers.
> > > > 3. Based on Google's track record, the new code will likely be well
> > > >    maintained in the long term. It'd be more difficult if not
> > > >    impossible to achieve similar effects on top of the existing
> > > >    design.
> > >
> > > Hi Andrew, Linus,
> > >
> > > Can you please take a look at this patchset and let me know if it's
> > > 5.17 material?
> > >
> > > My goal is to get it merged asap so that users can reap the benefits
> > > and I can push the sequels. Please examine the data provided -- I
> > > think the unprecedented coverage and the magnitude of the improvements
> > > warrant a green light.
> >
> > Downstream kernel maintainers who have been carrying MGLRU for more than
> > 3 versions, can you please provide your Acked-by tags?
> >
> > Having this patchset in the mainline will make your job easier :)
> >
> >    Alexandre - the XanMod Kernel maintainer
> >                https://xanmod.org
> >
> >    Brian     - the Chrome OS kernel memory maintainer
> >                https://www.chromium.org
> >
> >    Jan       - the Arch Linux Zen kernel maintainer
> >                https://archlinux.org
> >
> >    Steven    - the Liquorix kernel maintainer
> >                https://liquorix.net
> >
> >    Suleiman  - the ARCVM (Android downstream) kernel memory maintainer
> >
> https://chromium.googlesource.com/chromiumos/third_party/kernel
> >
> > Also my gratitude to those who have helped test MGLRU:
> >
> >    Daniel - researcher at Michigan Tech
> >             benchmarked memcached
> >
> >    Holger - who has been testing/patching/contributing to various
> >             subsystems since ~2008
> >
> >    Shuang - researcher at University of Rochester
> >             benchmarked fio and provided a report
> >
> >    Sofia  - EDI https://www.edi.works
> >             benchmarked the top eight memory hogs and provided reports
>
> Tested-by: Sofia Trinh <sofia.trinh@edi.works>
>


-- 
Daniel Byrne

--000000000000ffecb705d564e999
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div class=3D"gmail_default" style=3D"font-family:arial,he=
lvetica,sans-serif">Tested-by: Daniel Byrne &lt;<a href=3D"mailto:djbyrne@m=
tu.edu">djbyrne@mtu.edu</a>&gt; with memcached allocating ~100GB of byte-ad=
dressable Optante, performance improvement in terms of throughput (measured=
 as queries per second) was about 10% for a series of workloads. <br></div>=
</div><br><div class=3D"gmail_quote"><div dir=3D"ltr" class=3D"gmail_attr">=
On Wed, Jan 12, 2022 at 1:07 AM Sofia Trinh &lt;sofia.trinh@edi.works&gt; w=
rote:<br></div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0p=
x 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex">On Tue, Ja=
n 11, 2022 at 12:41 AM Yu Zhao &lt;<a href=3D"mailto:yuzhao@google.com" tar=
get=3D"_blank">yuzhao@google.com</a>&gt; wrote:<br>
&gt;<br>
&gt; On Tue, Jan 04, 2022 at 01:30:00PM -0700, Yu Zhao wrote:<br>
&gt; &gt; On Tue, Jan 04, 2022 at 01:22:19PM -0700, Yu Zhao wrote:<br>
&gt; &gt; &gt; TLDR<br>
&gt; &gt; &gt; =3D=3D=3D=3D<br>
&gt; &gt; &gt; The current page reclaim is too expensive in terms of CPU us=
age and it<br>
&gt; &gt; &gt; often makes poor choices about what to evict. This patchset =
offers an<br>
&gt; &gt; &gt; alternative solution that is performant, versatile and<br>
&gt; &gt; &gt; straightforward.<br>
&gt; &gt;<br>
&gt; &gt; &lt;snipped&gt;<br>
&gt; &gt;<br>
&gt; &gt; &gt; Summery<br>
&gt; &gt; &gt; =3D=3D=3D=3D=3D=3D=3D<br>
&gt; &gt; &gt; The facts are:<br>
&gt; &gt; &gt; 1. The independent lab results and the real-world applicatio=
ns<br>
&gt; &gt; &gt;=C2=A0 =C2=A0 indicate substantial improvements; there are no=
 known regressions.<br>
&gt; &gt; &gt; 2. Thrashing prevention, working set estimation and proactiv=
e reclaim<br>
&gt; &gt; &gt;=C2=A0 =C2=A0 work out of the box; there are no equivalent so=
lutions.<br>
&gt; &gt; &gt; 3. There is a lot of new code; nobody has demonstrated small=
er changes<br>
&gt; &gt; &gt;=C2=A0 =C2=A0 with similar effects.<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; Our options, accordingly, are:<br>
&gt; &gt; &gt; 1. Given the amount of evidence, the reported improvements w=
ill likely<br>
&gt; &gt; &gt;=C2=A0 =C2=A0 materialize for a wide range of workloads.<br>
&gt; &gt; &gt; 2. Gauging the interest from the past discussions [14][15][1=
6], the<br>
&gt; &gt; &gt;=C2=A0 =C2=A0 new features will likely be put to use for both=
 personal computers<br>
&gt; &gt; &gt;=C2=A0 =C2=A0 and data centers.<br>
&gt; &gt; &gt; 3. Based on Google&#39;s track record, the new code will lik=
ely be well<br>
&gt; &gt; &gt;=C2=A0 =C2=A0 maintained in the long term. It&#39;d be more d=
ifficult if not<br>
&gt; &gt; &gt;=C2=A0 =C2=A0 impossible to achieve similar effects on top of=
 the existing<br>
&gt; &gt; &gt;=C2=A0 =C2=A0 design.<br>
&gt; &gt;<br>
&gt; &gt; Hi Andrew, Linus,<br>
&gt; &gt;<br>
&gt; &gt; Can you please take a look at this patchset and let me know if it=
&#39;s<br>
&gt; &gt; 5.17 material?<br>
&gt; &gt;<br>
&gt; &gt; My goal is to get it merged asap so that users can reap the benef=
its<br>
&gt; &gt; and I can push the sequels. Please examine the data provided -- I=
<br>
&gt; &gt; think the unprecedented coverage and the magnitude of the improve=
ments<br>
&gt; &gt; warrant a green light.<br>
&gt;<br>
&gt; Downstream kernel maintainers who have been carrying MGLRU for more th=
an<br>
&gt; 3 versions, can you please provide your Acked-by tags?<br>
&gt;<br>
&gt; Having this patchset in the mainline will make your job easier :)<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 Alexandre - the XanMod Kernel maintainer<br>
&gt;=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 <a href=3D"http=
s://xanmod.org" rel=3D"noreferrer" target=3D"_blank">https://xanmod.org</a>=
<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 Brian=C2=A0 =C2=A0 =C2=A0- the Chrome OS kernel memory ma=
intainer<br>
&gt;=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 <a href=3D"http=
s://www.chromium.org" rel=3D"noreferrer" target=3D"_blank">https://www.chro=
mium.org</a><br>
&gt;<br>
&gt;=C2=A0 =C2=A0 Jan=C2=A0 =C2=A0 =C2=A0 =C2=A0- the Arch Linux Zen kernel=
 maintainer<br>
&gt;=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 <a href=3D"http=
s://archlinux.org" rel=3D"noreferrer" target=3D"_blank">https://archlinux.o=
rg</a><br>
&gt;<br>
&gt;=C2=A0 =C2=A0 Steven=C2=A0 =C2=A0 - the Liquorix kernel maintainer<br>
&gt;=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 <a href=3D"http=
s://liquorix.net" rel=3D"noreferrer" target=3D"_blank">https://liquorix.net=
</a><br>
&gt;<br>
&gt;=C2=A0 =C2=A0 Suleiman=C2=A0 - the ARCVM (Android downstream) kernel me=
mory maintainer<br>
&gt;=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 <a href=3D"http=
s://chromium.googlesource.com/chromiumos/third_party/kernel" rel=3D"norefer=
rer" target=3D"_blank">https://chromium.googlesource.com/chromiumos/third_p=
arty/kernel</a><br>
&gt;<br>
&gt; Also my gratitude to those who have helped test MGLRU:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 Daniel - researcher at Michigan Tech<br>
&gt;=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0benchmarked memcached<b=
r>
&gt;<br>
&gt;=C2=A0 =C2=A0 Holger - who has been testing/patching/contributing to va=
rious<br>
&gt;=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0subsystems since ~2008<=
br>
&gt;<br>
&gt;=C2=A0 =C2=A0 Shuang - researcher at University of Rochester<br>
&gt;=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0benchmarked fio and pro=
vided a report<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 Sofia=C2=A0 - EDI <a href=3D"https://www.edi.works" rel=
=3D"noreferrer" target=3D"_blank">https://www.edi.works</a><br>
&gt;=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0benchmarked the top eig=
ht memory hogs and provided reports<br>
<br>
Tested-by: Sofia Trinh &lt;sofia.trinh@edi.works&gt;<br>
</blockquote></div><br clear=3D"all"><br>-- <br><div dir=3D"ltr" class=3D"g=
mail_signature"><div dir=3D"ltr"><div><div dir=3D"ltr"><div style=3D"backgr=
ound-color:rgb(255,255,255)"><div><font face=3D"arial, sans-serif">Daniel B=
yrne</font></div><div><font face=3D"arial, sans-serif"><br></font></div><br=
></div></div></div></div></div>

--000000000000ffecb705d564e999--


